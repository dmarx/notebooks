{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNy+0WddcJ39iIURE/iN/MG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmarx/notebooks/blob/main/AnimateDiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unofficial AnimateDiff Demo\n",
        "\n",
        "> \"[AnimateDiff](https://animatediff.github.io/): Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning\"  \n",
        "> by [Yuwei Guo](guoyuwei@pjlab.org.cn), [Ceyuan Yang](https://ceyuan.me/) [Anyi Rao](https://anyirao.com/), [Yaohui Wang](https://wyhsirius.github.io/) [Yu Qiao](https://wyhsirius.github.io/) [Dahua Lin](http://dahua.site/) [Bo Dai](https://daibo.info/)\n",
        "\n",
        "\n",
        "```\n",
        "@misc{guo2023animatediff,\n",
        "  title={AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning},\n",
        "  author={Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai},\n",
        "  booktitle={arXiv preprint arxiv:2307.04725},\n",
        "  year={2023},\n",
        "  archivePrefix={arXiv},\n",
        "  primaryClass={cs.CV}\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "[Notebook](https://colab.research.google.com/github/dmarx/notebooks/blob/main/AnimateDiff.ipynb) by [David Marx](https://twitter.com/DigThatData), notebook brought to you by [Stability AI](https://stability.ai/)\n",
        "\n",
        "Setup adapted from Camenduru\n",
        "\n",
        "https://github.com/guoyww/animatediff/"
      ],
      "metadata": {
        "id": "v04sBovMvKBy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5R8KaG34V7h"
      },
      "outputs": [],
      "source": [
        "# @title setup: install stuff\n",
        "\n",
        "!git clone -b dev https://github.com/camenduru/animatediff\n",
        "!pip install einops omegaconf safetensors diffusers[torch]==0.11.1 transformers\n",
        "!apt -y install -qq aria2\n",
        "!rm -rf /content/animatediff/models/StableDiffusion\n",
        "!git clone -b fp16 https://huggingface.co/runwayml/stable-diffusion-v1-5 /content/animatediff/models/StableDiffusion/\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AnimateDiff/resolve/main/mm_sd_v14.ckpt -d /content/animatediff/models/Motion_Module -o mm_sd_v14.ckpt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AnimateDiff/resolve/main/mm_sd_v15.ckpt -d /content/animatediff/models/Motion_Module -o mm_sd_v15.ckpt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AnimateDiff/resolve/main/toonyou_beta3.safetensors -d /content/animatediff/models/DreamBooth_LoRA -o toonyou_beta3.safetensors\n",
        "\n",
        "!pip install napm\n",
        "\n",
        "import napm\n",
        "napm.pseudoinstall_git_repo(\"https://github.com/guoyww/animatediff/\", package_name='animatediff', add_install_dir_to_path=True)\n",
        "\n",
        "# the old way\n",
        "#!cd /content/animatediff; python -m scripts.animate --config /content/animatediff/configs/prompts/1-ToonYou.yaml --pretrained_model_path /content/animatediff/models/StableDiffusion --L 16 --W 256 --H 256\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title setup: import stuff and load models\n",
        "\n",
        "#import animatediff.scripts.animate\n",
        "import napm\n",
        "import argparse\n",
        "import datetime\n",
        "import inspect\n",
        "import os\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "import torch\n",
        "\n",
        "import diffusers\n",
        "from diffusers import AutoencoderKL, DDIMScheduler\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from animatediff.models.unet import UNet3DConditionModel\n",
        "from animatediff.pipelines.pipeline_animation import AnimationPipeline\n",
        "from animatediff.utils.util import save_videos_grid\n",
        "from animatediff.utils.convert_from_ckpt import convert_ldm_unet_checkpoint, convert_ldm_clip_checkpoint, convert_ldm_vae_checkpoint\n",
        "from animatediff.utils.convert_lora_safetensor_to_diffusers import convert_lora\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "import csv, pdb, glob\n",
        "from safetensors import safe_open\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "from types import SimpleNamespace\n",
        "\n",
        "\n",
        "\n",
        "cfg = napm.config.NapmConfig().load()\n",
        "PKG_ROOT = Path(cfg['packages']['animatediff']['install_dir'])\n",
        "\n",
        "inference_config = OmegaConf.load(PKG_ROOT/\"configs/inference/inference.yaml\")\n",
        "inference_config"
      ],
      "metadata": {
        "id": "btEIWXWO8Wc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SETTINGS (crystal says i should fix this name later)\n",
        "\n",
        "# --pretrained_model_path /content/animatediff/models/StableDiffusion --L 16 --W 256 --H 256\n",
        "args = SimpleNamespace()\n",
        "\n",
        "args.pretrained_model_path = \"/content/animatediff/models/StableDiffusion\" # @param {type:\"string\"}\n",
        "args.L = 16 # @param {type:\"integer\"}\n",
        "args.W = 448 # @param {type:\"integer\"}\n",
        "args.H = 320 # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "model_config = SimpleNamespace()\n",
        "model_config.path = '/content/animatediff/models/DreamBooth_LoRA/toonyou_beta3.safetensors' # @param {type:\"string\"}\n",
        " #PKG_ROOT/\"models/Motion_Module/mm_sd_v15.ckpt\"\n",
        "model_config.motion_module = \"/content/animatediff/models/Motion_Module/mm_sd_v15.ckpt\" # @param {type:\"string\"}\n",
        "model_config.seed = 10788741199826055526 # @param {type:\"integer\"}\n",
        "model_config.steps = 25 # @param {type:\"integer\"}\n",
        "model_config.guidance_scale = 7.5 # @param {type:\"number\"}\n",
        "model_config.prompt = \"apollo 13, 'houston we have a problem', tom hanks playing an astronaut\" # @param {type:\"string\"}\n",
        "model_config.n_prompt = \"stationary, motionless, boring, watermark, trademark, copyright, text, shutterstock\" # @param {type:\"string\"}\n",
        "\n",
        "#unet_additional_kwargs = {}\n"
      ],
      "metadata": {
        "id": "CiqNXwq_Bfod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load Models\n",
        "\n",
        "tokenizer    = CLIPTokenizer.from_pretrained(args.pretrained_model_path, subfolder=\"tokenizer\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_path, subfolder=\"text_encoder\")\n",
        "vae          = AutoencoderKL.from_pretrained(args.pretrained_model_path, subfolder=\"vae\")\n",
        "unet         = UNet3DConditionModel.from_pretrained_2d(args.pretrained_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.get(\"unet_additional_kwargs\", {})))\n",
        "\n",
        "pipeline = AnimationPipeline(\n",
        "  vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n",
        "  scheduler=DDIMScheduler(**OmegaConf.to_container(inference_config.noise_scheduler_kwargs)),\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "# probably wanna change this\n",
        "func_args = SimpleNamespace\n",
        "\n",
        "motion_module_state_dict = torch.load(model_config.motion_module, map_location=\"cpu\")\n",
        "if \"global_step\" in motion_module_state_dict:\n",
        "  func_args.update({\"global_step\": motion_module_state_dict[\"global_step\"]})\n",
        "missing, unexpected = pipeline.unet.load_state_dict(motion_module_state_dict, strict=False)\n",
        "assert len(unexpected) == 0\n",
        "\n",
        "\n",
        "\n",
        " # 1.2 T2I\n",
        "if model_config.path != \"\":\n",
        "    if model_config.path.endswith(\".ckpt\"):\n",
        "        state_dict = torch.load(model_config.path)\n",
        "        pipeline.unet.load_state_dict(state_dict)\n",
        "\n",
        "    elif model_config.path.endswith(\".safetensors\"):\n",
        "        state_dict = {}\n",
        "        with safe_open(model_config.path, framework=\"pt\", device=\"cpu\") as f:\n",
        "            for key in f.keys():\n",
        "                state_dict[key] = f.get_tensor(key)\n",
        "\n",
        "        is_lora = all(\"lora\" in k for k in state_dict.keys())\n",
        "        if not is_lora:\n",
        "            base_state_dict = state_dict\n",
        "        else:\n",
        "            base_state_dict = {}\n",
        "            with safe_open(model_config.base, framework=\"pt\", device=\"cpu\") as f:\n",
        "                for key in f.keys():\n",
        "                    base_state_dict[key] = f.get_tensor(key)\n",
        "\n",
        "        # vae\n",
        "        converted_vae_checkpoint = convert_ldm_vae_checkpoint(base_state_dict, pipeline.vae.config)\n",
        "        pipeline.vae.load_state_dict(converted_vae_checkpoint)\n",
        "        # unet\n",
        "        converted_unet_checkpoint = convert_ldm_unet_checkpoint(base_state_dict, pipeline.unet.config)\n",
        "        pipeline.unet.load_state_dict(converted_unet_checkpoint, strict=False)\n",
        "        # text_model\n",
        "        pipeline.text_encoder = convert_ldm_clip_checkpoint(base_state_dict)\n",
        "\n",
        "        # import pdb\n",
        "        # pdb.set_trace()\n",
        "        if is_lora:\n",
        "            pipeline = convert_lora(pipeline, state_dict, alpha=model_config.lora_alpha)\n",
        "\n",
        "pipeline.to(\"cuda\")\n",
        "### <<< create validation pipeline <<< ###"
      ],
      "metadata": {
        "id": "XMbZhSyvB-9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = pipeline(\n",
        "    prompt=model_config.prompt,\n",
        "    negative_prompt     = model_config.n_prompt,\n",
        "    num_inference_steps = model_config.steps,\n",
        "    guidance_scale      = model_config.guidance_scale,\n",
        "    width               = args.W,\n",
        "    height              = args.H,\n",
        "    video_length        = args.L,\n",
        ").videos\n",
        "\n",
        "samples = torch.concat([sample])\n",
        "savedir=\".\"\n",
        "outpath = f\"{savedir}/sample.gif\"\n",
        "save_videos_grid(samples, outpath , n_rows=1)"
      ],
      "metadata": {
        "id": "g8gLhMqjGszy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Show me the gif\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "Image(outpath)\n"
      ],
      "metadata": {
        "id": "EfD97tv3HIc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}