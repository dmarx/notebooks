{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMeVN2rGvnIBNEq64X5EhYJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7faf6d9f34e3404a8c9b8425275efee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65f10c228b2a4ec191b63b4328692505",
              "IPY_MODEL_c8bdc42d9cd646ff8b7294d28b8fda21",
              "IPY_MODEL_5dcf8a70a8c14de59fce1dd428a91767"
            ],
            "layout": "IPY_MODEL_186887bf56804fffb573aca57065e87c"
          }
        },
        "65f10c228b2a4ec191b63b4328692505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b71c50b995714699a071c59b28109b65",
            "placeholder": "​",
            "style": "IPY_MODEL_7ac87bbf1d2d4b32b43008b3a60f59ab",
            "value": "100%"
          }
        },
        "c8bdc42d9cd646ff8b7294d28b8fda21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e6005bf96a646f688837d60bebad323",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00ddd2751f5a4c5195892cc401c78288",
            "value": 50
          }
        },
        "5dcf8a70a8c14de59fce1dd428a91767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d860c8a4665f49099b5cea89f6847b0f",
            "placeholder": "​",
            "style": "IPY_MODEL_5e4737d3ab034b95becd1301bbe29f27",
            "value": " 50/50 [00:06&lt;00:00,  7.33it/s]"
          }
        },
        "186887bf56804fffb573aca57065e87c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b71c50b995714699a071c59b28109b65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac87bbf1d2d4b32b43008b3a60f59ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e6005bf96a646f688837d60bebad323": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00ddd2751f5a4c5195892cc401c78288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d860c8a4665f49099b5cea89f6847b0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e4737d3ab034b95becd1301bbe29f27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmarx/notebooks/blob/text2video-modelscope/modelscope_text2video_w_hf_diffusers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title setup: installations\n",
        "!pip install transformers huggingface_hub accelerate\n",
        "!pip install git+https://github.com/huggingface/diffusers/ # need latest, currently unreleased "
      ],
      "metadata": {
        "id": "_6gmUNyxs2IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title setup: imports/definitions\n",
        "# https://github.com/huggingface/diffusers/pull/2738\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers.utils import export_to_video\n",
        "import random\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "pipe = pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "M4CQY6ghuTHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title setup: more imports/definitions\n",
        "\n",
        "# scavenged and refactored from https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py\n",
        "\n",
        "from typing import Any, Callable, Dict, List, Optional, Union\n",
        "import numpy as np\n",
        "\n",
        "@torch.no_grad()\n",
        "def tensor2vid(video: torch.Tensor, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) -> List[np.ndarray]:\n",
        "    # This code is copied from https://github.com/modelscope/modelscope/blob/1509fdb973e5871f37148a4b5e5964cafd43e64d/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py#L78\n",
        "    # reshape to ncfhw\n",
        "    mean = torch.tensor(mean, device=video.device).reshape(1, -1, 1, 1, 1)\n",
        "    std = torch.tensor(std, device=video.device).reshape(1, -1, 1, 1, 1)\n",
        "    # unnormalize back to [0,1]\n",
        "    video = video.mul_(std).add_(mean)\n",
        "    video.clamp_(0, 1)\n",
        "    # prepare the final outputs\n",
        "    i, c, f, h, w = video.shape\n",
        "    images = video.permute(2, 3, 0, 4, 1).reshape(\n",
        "        f, h, i * w, c\n",
        "    )  # 1st (frames, h, batch_size, w, c) 2nd (frames, h, batch_size * w, c)\n",
        "    images = images.unbind(dim=0)  # prepare a list of indvidual (consecutive frames)\n",
        "    images = [(image.cpu().numpy() * 255).astype(\"uint8\") for image in images]  # f h w c\n",
        "    return images\n",
        "\n",
        "@torch.no_grad()\n",
        "def prepare_pipeline_inputs(\n",
        "    pipe,\n",
        "    prompt, \n",
        "    height, \n",
        "    width, \n",
        "    callback_steps, \n",
        "    negative_prompt, \n",
        "    prompt_embeds, \n",
        "    negative_prompt_embeds,\n",
        "    guidance_scale,\n",
        "    num_inference_steps,\n",
        "    num_frames,\n",
        "    generator,\n",
        "    eta,\n",
        "    latents,\n",
        "):\n",
        "    self=pipe\n",
        "    # 0. Default height and width to unet\n",
        "    height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
        "    width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
        "\n",
        "    num_images_per_prompt = 1\n",
        "\n",
        "    # 1. Check inputs. Raise error if not correct\n",
        "    self.check_inputs(\n",
        "        prompt, height, width, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds\n",
        "    )\n",
        "\n",
        "    # 2. Define call parameters\n",
        "    if prompt is not None and isinstance(prompt, str):\n",
        "        batch_size = 1\n",
        "    elif prompt is not None and isinstance(prompt, list):\n",
        "        batch_size = len(prompt)\n",
        "    else:\n",
        "        batch_size = prompt_embeds.shape[0]\n",
        "\n",
        "    device = self._execution_device\n",
        "    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "    # corresponds to doing no classifier free guidance.\n",
        "    do_classifier_free_guidance = guidance_scale > 1.0\n",
        "\n",
        "    # 3. Encode input prompt\n",
        "    prompt_embeds = self._encode_prompt(\n",
        "        prompt,\n",
        "        device,\n",
        "        num_images_per_prompt,\n",
        "        do_classifier_free_guidance,\n",
        "        negative_prompt,\n",
        "        prompt_embeds=prompt_embeds,\n",
        "        negative_prompt_embeds=negative_prompt_embeds,\n",
        "    )\n",
        "\n",
        "    # 4. Prepare timesteps\n",
        "    self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "    timesteps = self.scheduler.timesteps\n",
        "\n",
        "    # 5. Prepare latent variables\n",
        "    num_channels_latents = self.unet.in_channels\n",
        "    latents = self.prepare_latents(\n",
        "        batch_size * num_images_per_prompt,\n",
        "        num_channels_latents,\n",
        "        num_frames,\n",
        "        height,\n",
        "        width,\n",
        "        prompt_embeds.dtype,\n",
        "        device,\n",
        "        generator,\n",
        "        latents,\n",
        "    )\n",
        "\n",
        "    # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
        "    extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
        "\n",
        "    return do_classifier_free_guidance, device, batch_size, prompt_embeds, timesteps, latents, extra_step_kwargs\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def call_pipeline(\n",
        "    pipe,\n",
        "    prompt: Union[str, List[str]] = None,\n",
        "    height: Optional[int] = None,\n",
        "    width: Optional[int] = None,\n",
        "    num_frames: int = 16,\n",
        "    num_inference_steps: int = 50,\n",
        "    guidance_scale: float = 9.0,\n",
        "    negative_prompt: Optional[Union[str, List[str]]] = None,\n",
        "    eta: float = 0.0,\n",
        "    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
        "    latents: Optional[torch.FloatTensor] = None,\n",
        "    prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
        "    output_type: Optional[str] = \"np\",\n",
        "    return_dict: bool = True,\n",
        "    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
        "    callback_steps: int = 1,\n",
        "    cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
        "):\n",
        "    r\"\"\"\n",
        "    Function invoked when calling the pipeline for generation.\n",
        "    Args:\n",
        "        prompt (`str` or `List[str]`, *optional*):\n",
        "            The prompt or prompts to guide the video generation. If not defined, one has to pass `prompt_embeds`.\n",
        "            instead.\n",
        "        height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
        "            The height in pixels of the generated video.\n",
        "        width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
        "            The width in pixels of the generated video.\n",
        "        num_frames (`int`, *optional*, defaults to 16):\n",
        "            The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds\n",
        "            amounts to 2 seconds of video.\n",
        "        num_inference_steps (`int`, *optional*, defaults to 50):\n",
        "            The number of denoising steps. More denoising steps usually lead to a higher quality videos at the\n",
        "            expense of slower inference.\n",
        "        guidance_scale (`float`, *optional*, defaults to 7.5):\n",
        "            Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
        "            `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
        "            Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
        "            1`. Higher guidance scale encourages to generate videos that are closely linked to the text `prompt`,\n",
        "            usually at the expense of lower video quality.\n",
        "        negative_prompt (`str` or `List[str]`, *optional*):\n",
        "            The prompt or prompts not to guide the video generation. If not defined, one has to pass\n",
        "            `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
        "            less than `1`).\n",
        "        eta (`float`, *optional*, defaults to 0.0):\n",
        "            Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
        "            [`schedulers.DDIMScheduler`], will be ignored for others.\n",
        "        generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
        "            One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
        "            to make generation deterministic.\n",
        "        latents (`torch.FloatTensor`, *optional*):\n",
        "            Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for video\n",
        "            generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
        "            tensor will ge generated by sampling using the supplied random `generator`. Latents should be of shape\n",
        "            `(batch_size, num_channel, num_frames, height, width)`.\n",
        "        prompt_embeds (`torch.FloatTensor`, *optional*):\n",
        "            Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
        "            provided, text embeddings will be generated from `prompt` input argument.\n",
        "        negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
        "            Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
        "            weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
        "            argument.\n",
        "        output_type (`str`, *optional*, defaults to `\"np\"`):\n",
        "            The output format of the generate video. Choose between `torch.FloatTensor` or `np.array`.\n",
        "        callback (`Callable`, *optional*):\n",
        "            A function that will be called every `callback_steps` steps during inference. The function will be\n",
        "            called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
        "        callback_steps (`int`, *optional*, defaults to 1):\n",
        "            The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
        "            called at every step.\n",
        "        cross_attention_kwargs (`dict`, *optional*):\n",
        "            A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n",
        "            `self.processor` in\n",
        "            [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n",
        "    Examples:\n",
        "    Returns:\n",
        "        [`~pipelines.stable_diffusion.TextToVideoSDPipelineOutput`] or `tuple`:\n",
        "        [`~pipelines.stable_diffusion.TextToVideoSDPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n",
        "        When returning a tuple, the first element is a list with the generated frames.\n",
        "    \"\"\"\n",
        "    self=pipe\n",
        "\n",
        "    (\n",
        "        do_classifier_free_guidance, \n",
        "        device, \n",
        "        batch_size, \n",
        "        prompt_embeds, \n",
        "        timesteps, \n",
        "        latents, \n",
        "        extra_step_kwargs,\n",
        "    ) = prepare_pipeline_inputs(\n",
        "        pipe,\n",
        "        prompt, \n",
        "        height, \n",
        "        width, \n",
        "        callback_steps, \n",
        "        negative_prompt, \n",
        "        prompt_embeds, \n",
        "        negative_prompt_embeds,\n",
        "        guidance_scale,\n",
        "        num_inference_steps,\n",
        "        num_frames,\n",
        "        generator,\n",
        "        eta,\n",
        "        latents,\n",
        "    )\n",
        "\n",
        "    # 7. Denoising loop\n",
        "    num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
        "    with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "        for i, t in enumerate(timesteps):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "            # predict the noise residual\n",
        "            noise_pred = self.unet(\n",
        "                latent_model_input,\n",
        "                t,\n",
        "                encoder_hidden_states=prompt_embeds,\n",
        "                cross_attention_kwargs=cross_attention_kwargs,\n",
        "            ).sample\n",
        "\n",
        "            # perform guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # reshape latents\n",
        "            bsz, channel, frames, width, height = latents.shape\n",
        "            latents = latents.permute(0, 2, 1, 3, 4).reshape(bsz * frames, channel, width, height)\n",
        "            noise_pred = noise_pred.permute(0, 2, 1, 3, 4).reshape(bsz * frames, channel, width, height)\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
        "\n",
        "            # reshape latents back\n",
        "            latents = latents[None, :].reshape(bsz, frames, channel, width, height).permute(0, 2, 1, 3, 4)\n",
        "\n",
        "            # call the callback, if provided\n",
        "            if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
        "                progress_bar.update()\n",
        "                if callback is not None and i % callback_steps == 0:\n",
        "                    callback(i, t, latents)\n",
        "    return latents\n",
        "\n",
        "@torch.no_grad()\n",
        "def latents2video(pipe, latents):    \n",
        "    video_tensor = pipe.decode_latents(latents)\n",
        "    return tensor2vid(video_tensor)\n"
      ],
      "metadata": {
        "id": "Bfv6--N31xVW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate Video\n",
        "prompt = \"Spiderman surfing on top of a bus\" # @param {'type':'string'}\n",
        "negative_prompt='stockphoto watermark text' # @param {'type':'string'}\n",
        "output_video_path = 'myvid.mp4' # @param {'type':'string'}\n",
        "seed = 0 # @param {'type':'integer'}\n",
        "\n",
        "######################################\n",
        "\n",
        "if seed < 0:\n",
        "    seed = random.randint(0, 2147483647)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "# outv = pipe(\n",
        "#     prompt=prompt,\n",
        "#     negative_prompt=negative_prompt,\n",
        "# )\n",
        "# outpath = export_to_video(outv.frames, output_video_path)\n",
        "\n",
        "latents = call_pipeline(\n",
        "    pipe=pipe,\n",
        "    prompt=prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        ")\n",
        "\n",
        "video_frames = latents2video(pipe, latents)\n",
        "outpath = export_to_video(video_frames, output_video_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7faf6d9f34e3404a8c9b8425275efee6",
            "65f10c228b2a4ec191b63b4328692505",
            "c8bdc42d9cd646ff8b7294d28b8fda21",
            "5dcf8a70a8c14de59fce1dd428a91767",
            "186887bf56804fffb573aca57065e87c",
            "b71c50b995714699a071c59b28109b65",
            "7ac87bbf1d2d4b32b43008b3a60f59ab",
            "0e6005bf96a646f688837d60bebad323",
            "00ddd2751f5a4c5195892cc401c78288",
            "d860c8a4665f49099b5cea89f6847b0f",
            "5e4737d3ab034b95becd1301bbe29f27"
          ]
        },
        "id": "LGZWVgt9v0oi",
        "outputId": "dc41c055-d085-450e-8b9f-de05d1a290f9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7faf6d9f34e3404a8c9b8425275efee6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video, display\n",
        "\n",
        "# TO DO: fix video encoding?\n",
        "display(Video('/content/myvid.mp4'))"
      ],
      "metadata": {
        "id": "n1JsncMZuw5X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}