{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15BNHICpOOXg"
      },
      "source": [
        "# Stable Diffusion KLMC2 Animation\n",
        "\n",
        "<div>\n",
        "<img src=\"https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/a432c21c-bb12-4f38-b5e2-1c12a3c403f6/Animated-Logo_1.gif\" width=\"150\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "Notebook by Katherine Crowson (https://twitter.com/RiversHaveWings)\n",
        "\n",
        "Sponsored by StabilityAI (https://twitter.com/stabilityai)\n",
        "\n",
        "Generate animations with [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release) 1.4, using the [KLMC2 discretization of underdamped Langevin dynamics](https://arxiv.org/abs/1807.09382). The notebook is largely inspired by Ajay Jain and Ben Poole's paper [Journey to the BAOAB-limit](https://www.ajayjain.net/journey)&mdash;thank you so much for it!\n",
        "\n",
        "Notebook by [Katherine Crowson](https://twitter.com/RiversHaveWings)\n",
        "\n",
        "Sponsored by [StabilityAI](https://twitter.com/stabilityai)\n",
        "\n",
        "Generate animations with [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release) 1.4, using the [KLMC2 discretization of underdamped Langevin dynamics](https://arxiv.org/abs/1807.09382). The notebook is largely inspired by [Ajay Jain](https://twitter.com/ajayj_) and [Ben Poole](https://twitter.com/poolio)'s paper [Journey to the BAOAB-limit](https://www.ajayjain.net/journey)&mdash;thank you so much for it!\n",
        "\n",
        "---\n",
        "\n",
        "## Modifications Provenance\n",
        "\n",
        "Original notebook URL - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1m8ovBpO2QilE2o4O-p2PONSwqGn4_x2G)\n",
        "\n",
        "Features and QOL Modifications by [David Marx](https://twitter.com/DigThatData) - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dmarx/notebooks/blob/main/Stable_Diffusion_KLMC2_Animation.ipynb)\n",
        "\n",
        "* Keyframed prompts and settings\n",
        "* Multiprompt conditioning w independent prompt schedules\n",
        "* Set seed for deterministic output\n",
        "* Mount Google Drive\n",
        "* Faster Setup\n",
        "* Set output filename\n",
        "* Fancy GPU info\n",
        "* Video embed optional\n",
        "* Cheaper default runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ty3IOeXbLzvc"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU\n",
        "#!nvidia-smi\n",
        "\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "\n",
        "def gpu_info():\n",
        "    outv = subprocess.run([\n",
        "        'nvidia-smi',\n",
        "            # these lines concatenate into a single query string\n",
        "            '--query-gpu='\n",
        "            'timestamp,'\n",
        "            'name,'\n",
        "            'utilization.gpu,'\n",
        "            'utilization.memory,'\n",
        "            'memory.used,'\n",
        "            'memory.free,'\n",
        "            ,\n",
        "        '--format=csv'\n",
        "        ],\n",
        "        stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "    header, rec = outv.split('\\n')[:-1]\n",
        "    return pd.DataFrame({' '.join(k.strip().split('.')).capitalize():v for k,v in zip(header.split(','), rec.split(','))}, index=[0]).T\n",
        "\n",
        "gpu_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kelHR9VM1-hg"
      },
      "outputs": [],
      "source": [
        "#@title Install Deps\n",
        "\n",
        "# @markdown Your runtime will automatically restart after running this cell.\n",
        "# @markdown You should only need to run this cell when setting up a new runtime. After future runtime restarts,\n",
        "# @markdown you should be able to skip this cell.\n",
        "\n",
        "!pip install ftfy einops braceexpand requests transformers clip open_clip_torch omegaconf pytorch-lightning kornia k-diffusion ninja\n",
        "#!pip install -U torch torchvision\n",
        "!pip install -U git+https://github.com/huggingface/huggingface_hub\n",
        "!pip install napm keyframed\n",
        "\n",
        "#####################\n",
        "# Install more Deps #\n",
        "#####################\n",
        "\n",
        "#!git clone https://github.com/Stability-AI/stablediffusion\n",
        "#!git clone https://github.com/CompVis/stable-diffusion\n",
        "#!git clone https://github.com/CompVis/taming-transformers\n",
        "#!git clone https://github.com/CompVis/latent-diffusion\n",
        "\n",
        "# !pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers\n",
        "\n",
        "exit() # oh is this a way to restart the runtime? clever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJZtXShcPXx5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown # Setup Workspace { display-mode: \"form\" }\n",
        "\n",
        "###################\n",
        "# Setup Workspace #\n",
        "###################\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "mount_gdrive = True # @param {type:'boolean'}\n",
        "\n",
        "# defaults\n",
        "outdir = Path('./frames')\n",
        "if not os.environ.get('XDG_CACHE_HOME'):\n",
        "    os.environ['XDG_CACHE_HOME'] = str(Path('~/.cache').expanduser())\n",
        "\n",
        "if mount_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    Path('/content/drive/MyDrive/AI/models/.cache/').mkdir(parents=True, exist_ok=True) \n",
        "    os.environ['XDG_CACHE_HOME']='/content/drive/MyDrive/AI/models/.cache'\n",
        "    outdir = Path('/content/drive/MyDrive/AI/klmc2/frames/')\n",
        "\n",
        "# make sure the paths we need exist\n",
        "outdir.mkdir(parents=True, exist_ok=True)\n",
        "os.environ['NAPM_PATH'] = str( Path(os.environ['XDG_CACHE_HOME']) / 'napm' )\n",
        "Path(os.environ['NAPM_PATH']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "import napm\n",
        "\n",
        "url = 'https://github.com/Stability-AI/stablediffusion'\n",
        "napm.pseudoinstall_git_repo(url, add_install_dir_to_path=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2jXKIf2ZkT8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown # Main Setup { display-mode: \"form\" }\n",
        "\n",
        "###########\n",
        "# imports #\n",
        "###########\n",
        "\n",
        "import napm\n",
        "\n",
        "from base64 import b64encode\n",
        "from collections import defaultdict\n",
        "from concurrent import futures\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import functorch\n",
        "from IPython import display\n",
        "import k_diffusion as K\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "#sys.path.extend(['./stablediffusion'])\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "from requests.exceptions import HTTPError\n",
        "import huggingface_hub\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "#########################\n",
        "# Define useful globals #\n",
        "#########################\n",
        "\n",
        "cpu = torch.device(\"cpu\")\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "############################\n",
        "\n",
        "model_dir_str=str(Path(os.environ['XDG_CACHE_HOME']))\n",
        "\n",
        "sdmodelid2hfrepo = {\n",
        "    \"sd-v1-4\":\"CompVis/stable-diffusion-v-1-4-original\",\n",
        "    \"sd-v1-5\":\"runwayml/stable-diffusion-v1-5\",\n",
        "}\n",
        "\n",
        "sdmodelid2hfckpt = {\n",
        "    \"sd-v1-4\":\"sd-v1-4.ckpt\",\n",
        "    \"sd-v1-5\":\"v1-5-pruned-emaonly.ckpt\",\n",
        "}\n",
        "\n",
        "sdmodelid2yamlurl = {\n",
        "    \"sd-v1-4\":\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml\",\n",
        "    \"sd-v1-5\":\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml\",\n",
        "}\n",
        "\n",
        "sdmodelid2ckptstyle = {\n",
        "    \"sd-v1-4\":\"compvis\",\n",
        "    \"sd-v1-5\":\"compvis\",\n",
        "}\n",
        "\n",
        "############################\n",
        "\n",
        "vaemodelid2hfrepo = {\n",
        "    \"vae-ft-mse-840k\":\"stabilityai/sd-vae-ft-mse-original\",\n",
        "    \"vae-ft-ema-560k\":\"stabilityai/sd-vae-ft-ema-original\",\n",
        "    #\"vae-orig\":,\n",
        "}\n",
        "\n",
        "vaemodelid2hfckpt = {\n",
        "    \"vae-ft-mse-840k\":\"vae-ft-mse-840000-ema-pruned.ckpt\",\n",
        "    \"vae-ft-ema-560k\":\"vae-ft-ema-560000-ema-pruned.ckpt\",\n",
        "    #\"vae-orig\":,\n",
        "}\n",
        "\n",
        "vaemodelid2yamlurl = {\n",
        "    \"vae-ft-mse-840k\":\"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/models/first_stage_models/kl-f8/config.yaml\",\n",
        "    \"vae-ft-ema-560k\":\"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/models/first_stage_models/kl-f8/config.yaml\",\n",
        "    #\"vae-orig\":\"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/models/first_stage_models/kl-f8/config.yaml\",\n",
        "}\n",
        "\n",
        "\n",
        "##############################\n",
        "# Define necessary functions #\n",
        "##############################\n",
        "\n",
        "\n",
        "class NormalizingCFGDenoiser(nn.Module):\n",
        "    def __init__(self, model, g):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "        self.g = g\n",
        "        self.eps_norms = defaultdict(lambda: (0, 0))\n",
        "\n",
        "    def mean_sq(self, x):\n",
        "        return x.pow(2).flatten(1).mean(1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_eps_norm(self, eps, sigma):\n",
        "        sigma = sigma[0].item()\n",
        "        eps_norm = self.mean_sq(eps).mean()\n",
        "        eps_norm_avg, count = self.eps_norms[sigma]\n",
        "        eps_norm_avg = eps_norm_avg * count / (count + 1) + eps_norm / (count + 1)\n",
        "        self.eps_norms[sigma] = (eps_norm_avg, count + 1)\n",
        "        return eps_norm_avg\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, g):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "\n",
        "        denoised = self.inner_model(x_in, sigma_in, cond=cond_in)\n",
        "        eps = K.sampling.to_d(x_in, sigma_in, denoised)\n",
        "        eps_uc, eps_c = eps.chunk(2)\n",
        "        eps_norm = self.update_eps_norm(eps, sigma).sqrt()\n",
        "        c = eps_c - eps_uc\n",
        "        cond_scale = g * eps_norm / self.mean_sq(c).sqrt()\n",
        "        eps_final = eps_uc + c * K.utils.append_dims(cond_scale, x.ndim)\n",
        "        return x - eps_final * K.utils.append_dims(sigma, eps.ndim)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_mcmc_klmc2(\n",
        "    model, x, \n",
        "    sigma_min, sigma, sigma_max, \n",
        "    n, \n",
        "    hvp_method='reverse', \n",
        "    callback=None, \n",
        "    disable=None, \n",
        "    prompts=None,\n",
        "    settings=None, # g, h, gamma, alpha, tau, prompt\n",
        "):\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    sigma = torch.tensor(sigma, device=x.device)\n",
        "    sigmas = K.sampling.get_sigmas_karras(6, sigma_min, sigma.item(), device=x.device)[:-1]\n",
        "\n",
        "    uc = sd_model.get_learned_conditioning([''])\n",
        "    extra_args = {'uncond': uc}\n",
        "    v = torch.randn_like(x) * sigma # ... I guess?\n",
        "\n",
        "    for i in trange(n, disable=disable):\n",
        "\n",
        "        h = settings[i]['h']\n",
        "        gamma = settings[i]['gamma']\n",
        "        alpha = settings[i]['alpha']\n",
        "        tau = settings[i]['tau']\n",
        "\n",
        "        h = torch.tensor(h, device=x.device)\n",
        "        gamma = torch.tensor(gamma, device=x.device)\n",
        "        alpha = torch.tensor(alpha, device=x.device)\n",
        "        tau = torch.tensor(tau, device=x.device)\n",
        "\n",
        "        # Model helper functions\n",
        "\n",
        "        def hvp_fn_forward_functorch(x, sigma, v, **extra_args):\n",
        "            def grad_fn(x, sigma):\n",
        "                denoised = model(x, sigma * s_in, **extra_args)\n",
        "                return (x - denoised) + alpha * x\n",
        "            jvp_fn = lambda v: functorch.jvp(grad_fn, (x, sigma), (v, torch.zeros_like(sigma)))\n",
        "            grad, jvp_out = functorch.vmap(jvp_fn)(v)\n",
        "            return grad[0], jvp_out\n",
        "\n",
        "        def hvp_fn_reverse(x, sigma, v, **extra_args):\n",
        "            def grad_fn(x, sigma):\n",
        "                denoised = model(x, sigma * s_in, **extra_args)\n",
        "                return (x - denoised) + alpha * x\n",
        "            vjps = []\n",
        "            with torch.enable_grad():\n",
        "                x_ = x.clone().requires_grad_()\n",
        "                grad = grad_fn(x_, sigma)\n",
        "                for k, item in enumerate(v):\n",
        "                    vjp_out = torch.autograd.grad(grad, x_, item, retain_graph=k < len(v) - 1)[0]\n",
        "                    vjps.append(vjp_out)\n",
        "            return grad, torch.stack(vjps)\n",
        "\n",
        "        def hvp_fn_zero(x, sigma, v, **extra_args):\n",
        "            def grad_fn(x, sigma):\n",
        "                denoised = model(x, sigma * s_in, **extra_args)\n",
        "                return (x - denoised) + alpha * x\n",
        "            return grad_fn(x, sigma), torch.zeros_like(v)\n",
        "\n",
        "        def hvp_fn_fake(x, sigma, v, **extra_args):\n",
        "            def grad_fn(x, sigma):\n",
        "                denoised = model(x, sigma * s_in, **extra_args)\n",
        "                return (x - denoised) + alpha * x\n",
        "            return grad_fn(x, sigma), (1 + alpha) * v\n",
        "\n",
        "        hvp_fns = {'forward-functorch': hvp_fn_forward_functorch,\n",
        "                  'reverse': hvp_fn_reverse,\n",
        "                  'zero': hvp_fn_zero,\n",
        "                  'fake': hvp_fn_fake}\n",
        "\n",
        "        hvp_fn = hvp_fns[hvp_method]\n",
        "\n",
        "        # KLMC2 helper functions\n",
        "        def psi_0(gamma, t):\n",
        "            return torch.exp(-gamma * t)\n",
        "\n",
        "        def psi_1(gamma, t):\n",
        "            return -torch.expm1(-gamma * t) / gamma\n",
        "\n",
        "        def psi_2(gamma, t):\n",
        "            return (torch.expm1(-gamma * t) + gamma * t) / gamma ** 2\n",
        "\n",
        "        def phi_2(gamma, t_):\n",
        "            t = t_.double()\n",
        "            out = (torch.exp(-gamma * t) * (torch.expm1(gamma * t) - gamma * t)) / gamma ** 2\n",
        "            return out.to(t_)\n",
        "\n",
        "        def phi_3(gamma, t_):\n",
        "            t = t_.double()\n",
        "            out = (torch.exp(-gamma * t) * (2 + gamma * t + torch.exp(gamma * t) * (gamma * t - 2))) / gamma ** 3\n",
        "            return out.to(t_)\n",
        "\n",
        "\n",
        "        # Compute model outputs and sample noise\n",
        "        x_trapz = torch.linspace(0, h, 1001, device=x.device)\n",
        "        y_trapz = [fun(gamma, x_trapz) for fun in (psi_0, psi_1, phi_2, phi_3)]\n",
        "        noise_cov = torch.tensor([[torch.trapz(y_trapz[i] * y_trapz[j], x=x_trapz) for j in range(4)] for i in range(4)], device=x.device)\n",
        "        noise_v, noise_x, noise_v2, noise_x2 = torch.distributions.MultivariateNormal(x.new_zeros([4]), noise_cov).sample(x.shape).unbind(-1)\n",
        "            \n",
        "        extra_args['g']=g\n",
        "\n",
        "        # loop over prompts and aggregate gradients for multicond\n",
        "        grad = torch.zeros_like(x)\n",
        "        h2_v = torch.zeros_like(x)\n",
        "        h2_noise_v2 = torch.zeros_like(x)\n",
        "        h2_noise_x2 = torch.zeros_like(x)\n",
        "        wt_norm = 0\n",
        "        for prompt in prompts:\n",
        "            wt = prompt.weight[i]\n",
        "            if wt == 0:\n",
        "                continue\n",
        "            wt_norm += wt\n",
        "            wt = torch.tensor(wt, device=x.device)\n",
        "            extra_args['cond'] = prompt.encoded\n",
        "\n",
        "            # Estimate gradient and hessian\n",
        "            grad_, (h2_v_, h2_noise_v2_, h2_noise_x2_) = hvp_fn(\n",
        "                x, sigma, torch.stack([v, noise_v2, noise_x2]),\n",
        "                **extra_args\n",
        "            )\n",
        "\n",
        "            grad = grad + grad_ * wt \n",
        "            h2_v = h2_v + h2_v_ * wt\n",
        "            h2_noise_v2 = h2_noise_v2 + h2_noise_v2_ * wt\n",
        "            h2_noise_x2 = h2_noise_x2 + h2_noise_x2_ * wt\n",
        "\n",
        "        # Normalize gradient to magnitude it'd have if just single prompt w/ wt=1.\n",
        "        # simplifies multicond w/o deep frying image or adding hyperparams\n",
        "        grad = grad / wt_norm \n",
        "        h2_v = h2_v / wt_norm\n",
        "        h2_noise_v2 = h2_noise_v2 / wt_norm\n",
        "        h2_noise_x2 = h2_noise_x2 / wt_norm\n",
        "        \n",
        "\n",
        "        # DPM-Solver++(2M) refinement steps\n",
        "        x_refine = x\n",
        "        use_dpm = True\n",
        "        old_denoised = None\n",
        "        for j in range(len(sigmas) - 1):\n",
        "            if j == 0:\n",
        "                denoised = x_refine - grad\n",
        "            else:\n",
        "                denoised = model(x_refine, sigmas[j] * s_in, **extra_args)\n",
        "            dt_ode = sigmas[j + 1] - sigmas[j]\n",
        "            if not use_dpm or old_denoised is None or sigmas[j + 1] == 0:\n",
        "                eps = K.sampling.to_d(x_refine, sigmas[j], denoised)\n",
        "                x_refine = x_refine + eps * dt_ode\n",
        "            else:\n",
        "                h_ode = sigmas[j].log() - sigmas[j + 1].log()\n",
        "                h_last = sigmas[j - 1].log() - sigmas[j].log()\n",
        "                fac = h_ode / (2 * h_last)\n",
        "                denoised_d = (1 + fac) * denoised - fac * old_denoised\n",
        "                eps = K.sampling.to_d(x_refine, sigmas[j], denoised_d)\n",
        "                x_refine = x_refine + eps * dt_ode\n",
        "            old_denoised = denoised\n",
        "        if callback is not None:\n",
        "            callback({'i': i, 'denoised': x_refine})\n",
        "\n",
        "        # Update the chain\n",
        "        noise_std = (2 * gamma * tau * sigma ** 2).sqrt()\n",
        "        v_next = 0 + psi_0(gamma, h) * v - psi_1(gamma, h) * grad - phi_2(gamma, h) * h2_v + noise_std * (noise_v - h2_noise_v2)\n",
        "        x_next = x + psi_1(gamma, h) * v - psi_2(gamma, h) * grad - phi_3(gamma, h) * h2_v + noise_std * (noise_x - h2_noise_x2)\n",
        "        v, x = v_next, x_next\n",
        "\n",
        "    x = x - grad\n",
        "    return x\n",
        "\n",
        "\n",
        "def show_video(video_path, video_width=512):\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return display.HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
        "\n",
        "\n",
        "def download_from_huggingface(repo, filename):\n",
        "  while True:\n",
        "    try:\n",
        "      return huggingface_hub.hf_hub_download(repo, filename)\n",
        "    except HTTPError as e:\n",
        "      if e.response.status_code == 401:\n",
        "        # Need to log into huggingface api\n",
        "        huggingface_hub.interpreter_login()\n",
        "        continue\n",
        "      elif e.response.status_code == 403:\n",
        "        # Need to do the click through license thing\n",
        "        print(f'Go here and agree to the click through license on your account: https://huggingface.co/{repo}')\n",
        "        input('Hit enter when ready:')\n",
        "        continue\n",
        "      else:\n",
        "        raise e\n",
        "\n",
        "\n",
        "def load_model_from_config(config, ckpt):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    config = OmegaConf.load(config)\n",
        "\n",
        "    try:\n",
        "        config['model']['params']['lossconfig']['target'] = \"torch.nn.Identity\"\n",
        "        print('Patched VAE config.')\n",
        "    except KeyError:\n",
        "        pass\n",
        "\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    model = model.to(cpu).eval().requires_grad_(False)\n",
        "    return model\n",
        "\n",
        "###############\n",
        "# Load Models #\n",
        "###############\n",
        "\n",
        "# @markdown ## Select Models { display-mode: \"form\" }\n",
        "\n",
        "load_from_hf = True                 # @param {type:'boolean'}\n",
        "\n",
        "sd_model_id = \"sd-v1-4\"             # @param [\"sd-v1-4\", \"sd-v1-5\"] \n",
        "\n",
        "# https://huggingface.co/stabilityai/sd-vae-ft-ema-original\n",
        "vae_model_id = \"vae-ft-mse-840k\"    # @param [\"vae-ft-mse-840k\", \"vae-ft-ema-560k\"]\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "custom_sdmodel_checkpoint_path = \"\" # @param {type:'string'}\n",
        "custom_vae_model_path = \"\"          # @param {type:'string'}\n",
        "custom_sdmodel_yaml_path = \"\"       # @param {type:'string'}\n",
        "custom_vaemodel_yaml_path = \"\"      # @param {type:'string'}\n",
        "custom_checkopint_loading_style = \"\" # @param [\"\", \"compvis\", \"diffusers\"] \n",
        "\n",
        "###################\n",
        "# Fetch artifacts #\n",
        "###################\n",
        "\n",
        "if load_from_hf:\n",
        "\n",
        "    ckpt_style = sdmodelid2ckptstyle[sd_model_id]\n",
        "\n",
        "    sd_model_repo = sdmodelid2hfrepo[sd_model_id]\n",
        "    sd_model_ckpt = sdmodelid2hfckpt[sd_model_id]\n",
        "\n",
        "    sd_model_path = download_from_huggingface(sd_model_repo, sd_model_ckpt)\n",
        "\n",
        "    sdmodel_yaml_url = sdmodelid2yamlurl[sd_model_id]\n",
        "    urlpath = urlparse(sdmodel_yaml_url).path\n",
        "    sdmodel_yaml_fname = Path(urlpath).name\n",
        "\n",
        "    if not Path(sdmodel_yaml_fname).exists():\n",
        "        !wget {sdmodel_yaml_url}\n",
        "    \n",
        "    ###########\n",
        "\n",
        "    vae_model_repo = vaemodelid2hfrepo[vae_model_id]\n",
        "    vae_model_ckpt = vaemodelid2hfckpt[vae_model_id]\n",
        "\n",
        "    vae_model_path = download_from_huggingface(vae_model_repo, vae_model_ckpt)\n",
        "\n",
        "    vaemodel_yaml_url = vaemodelid2yamlurl[vae_model_id]\n",
        "    urlpath = urlparse(vaemodel_yaml_url).path\n",
        "    vaemodel_yaml_fname = Path(urlpath).name\n",
        "\n",
        "    if not Path(vaemodel_yaml_fname).exists():\n",
        "        !wget {vaemodel_yaml_url}\n",
        "\n",
        "\n",
        "else:\n",
        "    #raise NotImplementedError\n",
        "    assert Path(custom_sdmodel_checkpoint_path).exists()\n",
        "    assert Path(custom_vae_model_path).exists()\n",
        "    sd_model_path = custom_sdmodel_checkpoint_path\n",
        "    vae_model_path = custom_vae_model_path\n",
        "    sdmodel_yaml_fname = custom_sdmodel_yaml_path\n",
        "    vaemodel_yaml_fname = custom_vaemodel_yaml_path\n",
        "\n",
        "###############\n",
        "# Load models #\n",
        "###############\n",
        "\n",
        "if ckpt_style == 'compvis':\n",
        "\n",
        "    sd_model = load_model_from_config(sdmodel_yaml_fname, sd_model_path).half().to(device)\n",
        "    vae_model = load_model_from_config(vaemodel_yaml_fname, vae_model_path).half().to(device)\n",
        "\n",
        "    # Disable checkpointing as it is not compatible with the method\n",
        "    for module in sd_model.modules():\n",
        "        if hasattr(module, 'checkpoint'):\n",
        "            module.checkpoint = False\n",
        "        if hasattr(module, 'use_checkpoint'):\n",
        "            module.use_checkpoint = False\n",
        "\n",
        "else:\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZljSF1ePnBl4"
      },
      "outputs": [],
      "source": [
        "#@title Parameters\n",
        "\n",
        "# just being doubly sure this line didn't get deleted. \n",
        "# TO DO: make this not necessary \n",
        "#prompt = prompts[0]\n",
        "\n",
        "#@markdown The strength of the conditioning on the prompt:\n",
        "g = 0.1 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown The number of frames to sample:\n",
        "n = 350 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown The noise level to sample at:\n",
        "sigma = 2.0 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Step size (range 0 to 1):\n",
        "h = 0.1 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Friction (2 is critically damped, lower -> smoother animation):\n",
        "gamma = 1.0 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Quadratic penalty (\"weight decay\") strength:\n",
        "alpha = 0.005 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Temperature (adjustment to the amount of noise added per step):\n",
        "tau = 1.0 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown The HVP method:\n",
        "#@markdown <br><small>`forward-functorch` and `reverse` provide real second derivatives. Compatibility, speed, and memory usage vary by model and xformers configuration.\n",
        "#@markdown `fake` is very fast and low memory but inaccurate. `zero` (fallback to first order KLMC) is not recommended.</small>\n",
        "hvp_method = 'fake' #@param [\"forward-functorch\", \"reverse\", \"fake\", \"zero\"]\n",
        "\n",
        "#@markdown If seed is negative, a random seed will be used\n",
        "seed = 1  #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pLTsdGBPXx6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "from keyframed import Curve, ParameterGroup, Keyframe #, Prompt\n",
        "import math\n",
        "\n",
        "#@title Prompts\n",
        "\n",
        "prompt_params = [\n",
        "    # FIRST PROMPT INITIALIZES IMAGE\n",
        "    [\"portrait of queen elizabeth at 10 years old\", {0:1, 30:0.5, 50:0}],\n",
        "    [\"portrait of queen elizabeth at 16 years old\", {20:0, 40:1, 70:0}],\n",
        "    [\"portrait of queen elizabeth at 22 years old\", {40:0, 60:1, 90:0}],\n",
        "    [\"portrait of queen elizabeth at 28 years old\", {60:0, 80:1, 110:0}],\n",
        "    [\"portrait of queen elizabeth at 34 years old\", {80:0, 100:1, 130:0}],\n",
        "    [\"portrait of queen elizabeth at 40 years old\", {100:0, 120:1, 150:0}],\n",
        "    [\"portrait of queen elizabeth at 46 years old\", {120:0, 140:1, 170:0}],\n",
        "    [\"portrait of queen elizabeth at 52 years old\", {140:0, 160:1, 190:0}],\n",
        "    [\"portrait of queen elizabeth at 58 years old\", {160:0, 180:1, 210:0}],\n",
        "    [\"portrait of queen elizabeth at 64 years old\", {180:0, 200:1, 230:0}],\n",
        "    [\"portrait of queen elizabeth at 70 years old\", {200:0, 220:1, 250:0}],\n",
        "    [\"portrait of queen elizabeth at 76 years old\", {220:0, 240:1, 270:0}],\n",
        "    [\"portrait of queen elizabeth at 82 years old\", {240:0, 260:1, 290:0}],\n",
        "    [\"portrait of queen elizabeth at 88 years old\", {260:0, 280:1, 310:0}],\n",
        "    [\"portrait of queen elizabeth at 94 years old\", {280:0, 300:1}],\n",
        "\n",
        "    # BACKGROUND PROMPTS LAST\n",
        "    #[\"portrait of queen elizabeth\", {0:0.1}],\n",
        "    #[\"a beautiful oil painting, masterpiece, realism, technique, detail\",{0:.2,300:0}],\n",
        "    [\"award winning portrait photography, color photo of queen elizabeth, extremely high resolution\", {0:0,300:0.2}],\n",
        "    [\"blurry, weird looking, drawn by an amateur\", {0:-.5, 300:-.5}],\n",
        "    ##########\n",
        "]\n",
        "\n",
        "def sin2(t):\n",
        "    return (math.sin(t * math.pi / 2)) ** 2\n",
        "\n",
        "class Prompt:\n",
        "    def __init__(self, text, weight_schedule):\n",
        "      c = sd_model.get_learned_conditioning([text])\n",
        "      self.text=text\n",
        "      self.encoded=c\n",
        "      self.weight=Curve(\n",
        "          weight_schedule, \n",
        "          default_interpolation='linear', \n",
        "          ease_in=sin2, \n",
        "          ease_out=sin2)\n",
        "\n",
        "\n",
        "prompts = [Prompt(text, weight_schedule) for (text, weight_schedule) in prompt_params]\n",
        "\n",
        "\n",
        "# ok this works\n",
        "settings = ParameterGroup({\n",
        "    'g':Curve(g),\n",
        "    'n':Curve(n), # this one doesn't need to be curved\n",
        "    'sigma':Curve(sigma),\n",
        "    'h':Curve(h),\n",
        "    'gamma':Curve(gamma),\n",
        "    'alpha':Curve(alpha),\n",
        "    'tau':Curve(tau),\n",
        "    #'hvp_method':hvp_method, # this could be Keyframed but not Curved\n",
        "    'seed':Curve(seed),\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_prompt_weight_curves = False # @param {type: 'boolean'}\n",
        "\n",
        "if plot_prompt_weight_curves:\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np \n",
        "\n",
        "    # how many frames out we should plot\n",
        "    #n = 100\n",
        "    n=settings[0]['n']\n",
        "\n",
        "    ytot=np.array([0 for _ in range(n)])\n",
        "    for prompt in prompts:#[:3]:\n",
        "      xs = np.array(range(n))\n",
        "      ys = np.array([prompt.weight[x] for x in xs])\n",
        "      ytot=ytot+ys\n",
        "      plt.plot(xs, ys)\n",
        "    plt.title(\"prompt weight schedules\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(xs, ytot)\n",
        "    plt.title(\"sum weight\\n(aka: why weights get normalized)\")\n",
        "    plt.show()\n",
        "\n",
        "    for prompt in prompts:#[:3]:\n",
        "      xs = np.array(range(n))\n",
        "      ys = np.array([prompt.weight[x] for x in xs])\n",
        "      plt.plot(xs, ys/ytot)\n",
        "    plt.title(\"normalized weights\\n(aka: why prompts might seem weighted differently than I asked)\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hZ0lh-WkdB19",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-_u1Q0wRqMb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Generate the animation\n",
        "\n",
        "# to do: if random seed, pick one for user and report chosen seed back\n",
        "if seed >= 0:\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "wrappers = {'eps': K.external.CompVisDenoiser, 'v': K.external.CompVisVDenoiser}\n",
        "model_wrap = wrappers[sd_model.parameterization](sd_model)\n",
        "model_wrap_cfg = NormalizingCFGDenoiser(model_wrap, g)\n",
        "sigma_min, sigma_max = model_wrap.sigmas[0].item(), model_wrap.sigmas[-1].item()\n",
        "\n",
        "uc = sd_model.get_learned_conditioning([''])\n",
        "c = prompts[0].encoded\n",
        "extra_args = {'cond': c, 'uncond': uc}\n",
        "\n",
        "def save_image_fn(image, name, i):\n",
        "    pil_image = K.utils.to_pil_image(image)\n",
        "    if i % 10 == 0 or i == n - 1:\n",
        "        print(f'\\nIteration {i}/{n}:')\n",
        "        display.display(pil_image)\n",
        "    if i == n - 1:\n",
        "        print('\\nDone!')\n",
        "    name = outdir / name\n",
        "    pil_image.save(name)\n",
        "\n",
        "# to do: add archival\n",
        "# Clean up old images and video - save them elsewhere before running this if you want to keep them!\n",
        "#for p in Path('.').glob('out_*.png'):\n",
        "for p in outdir.glob('out_*.png'):\n",
        "    p.unlink()\n",
        "Path('out.mp4').unlink(missing_ok=True)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "with torch.cuda.amp.autocast(), futures.ThreadPoolExecutor() as ex:\n",
        "    def callback(info):\n",
        "        i = info['i']\n",
        "        rgb = vae_model.decode(info['denoised'] / sd_model.scale_factor)\n",
        "        ex.submit(save_image_fn, rgb, f'out_{i:05}.png', i)\n",
        "\n",
        "    x = torch.randn([1, 4, 64, 64], device=device) * sigma_max\n",
        "    \n",
        "    # Initialize the chain\n",
        "    print('Initializing the chain...')\n",
        "    sigmas_pre = K.sampling.get_sigmas_karras(15, sigma, sigma_max, device=x.device)[:-1]\n",
        "\n",
        "    extra_args['g'] = settings[0]['g'] # yeesh...\n",
        "    x = K.sampling.sample_dpmpp_sde(model_wrap_cfg, x, sigmas_pre, extra_args=extra_args)\n",
        "\n",
        "    print('Actually doing the sampling...')\n",
        "    sample_mcmc_klmc2(\n",
        "        model=model_wrap_cfg,\n",
        "        x=x,\n",
        "        sigma_min=sigma_min,\n",
        "        sigma=sigma,\n",
        "        sigma_max=sigma_max,\n",
        "        n=n,\n",
        "        # h, gamma=gamma, alpha=alpha, tau=tau,\n",
        "        hvp_method=hvp_method,\n",
        "        #extra_args=extra_args,\n",
        "        callback=callback,\n",
        "        prompts=prompts,\n",
        "        settings=settings,\n",
        "                      )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DjwY7XrooLX_"
      },
      "outputs": [],
      "source": [
        "#@title Make the video\n",
        "\n",
        "outdir_str = str(outdir)\n",
        "\n",
        "out_fname = \"out.mp4\" # @param {type: \"string\"}\n",
        "\n",
        "print('\\nMaking the video...\\n')\n",
        "!cd {outdir_str}; ffmpeg -y -r 20 -i 'out_%*.png' -crf 15 -preset veryslow -pix_fmt yuv420p {out_fname}\n",
        "\n",
        "# @markdown If your video is larger than a few MB, attempting to embed it will probably crash\n",
        "# the session. If this happens, view the generated video after downloading it first.\n",
        "embed_video = True # @param {type:'boolean'}\n",
        "\n",
        "if embed_video:\n",
        "  print('\\nThe video:')\n",
        "  display.display(show_video(outdir / out_fname))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK_GlP_7WJiu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the MIT License { display-mode: \"form\" }\n",
        "\n",
        "# Copyright (c) 2022 Katherine Crowson <crowsonkb@gmail.com>\n",
        "# Copyright (c) 2023 David Marx <david.marx84@gmail.com>\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}