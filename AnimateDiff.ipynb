{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOx/kliQoJsQBKzuAGDLdf9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmarx/notebooks/blob/animate_diff/AnimateDiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5R8KaG34V7h"
      },
      "outputs": [],
      "source": [
        "!git clone -b dev https://github.com/camenduru/animatediff\n",
        "!pip install einops omegaconf safetensors diffusers[torch]==0.11.1 transformers\n",
        "!apt -y install -qq aria2\n",
        "!rm -rf /content/animatediff/models/StableDiffusion\n",
        "!git clone -b fp16 https://huggingface.co/runwayml/stable-diffusion-v1-5 /content/animatediff/models/StableDiffusion/\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AnimateDiff/resolve/main/mm_sd_v14.ckpt -d /content/animatediff/models/Motion_Module -o mm_sd_v14.ckpt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AnimateDiff/resolve/main/mm_sd_v15.ckpt -d /content/animatediff/models/Motion_Module -o mm_sd_v15.ckpt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AnimateDiff/resolve/main/toonyou_beta3.safetensors -d /content/animatediff/models/DreamBooth_LoRA -o toonyou_beta3.safetensors\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/animatediff; python -m scripts.animate --config /content/animatediff/configs/prompts/1-ToonYou.yaml --pretrained_model_path /content/animatediff/models/StableDiffusion --L 16 --W 256 --H 256"
      ],
      "metadata": {
        "id": "3Y7B8Wqq5_Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/animatediff; python -m scripts.animate --help"
      ],
      "metadata": {
        "id": "YzjAqBF57qNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install napm\n",
        "import napm\n",
        "napm.pseudoinstall_git_repo(\"https://github.com/guoyww/animatediff/\", package_name='animatediff', add_install_dir_to_path=True)"
      ],
      "metadata": {
        "id": "NPe_GLoE-t0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import animatediff.scripts.animate\n",
        "import napm\n",
        "import argparse\n",
        "import datetime\n",
        "import inspect\n",
        "import os\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "import torch\n",
        "\n",
        "import diffusers\n",
        "from diffusers import AutoencoderKL, DDIMScheduler\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from animatediff.models.unet import UNet3DConditionModel\n",
        "from animatediff.pipelines.pipeline_animation import AnimationPipeline\n",
        "from animatediff.utils.util import save_videos_grid\n",
        "from animatediff.utils.convert_from_ckpt import convert_ldm_unet_checkpoint, convert_ldm_clip_checkpoint, convert_ldm_vae_checkpoint\n",
        "from animatediff.utils.convert_lora_safetensor_to_diffusers import convert_lora\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "import csv, pdb, glob\n",
        "from safetensors import safe_open\n",
        "import math\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "btEIWXWO8Wc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "# --pretrained_model_path /content/animatediff/models/StableDiffusion --L 16 --W 256 --H 256\n",
        "args = SimpleNamespace()\n",
        "args.pretrained_model_path = \"/content/animatediff_github/models/StableDiffusion\"\n",
        "args.L = 16\n",
        "args.W = 256\n",
        "args.H = 256\n",
        "\n",
        "#args.noise_scheduler_kwargs = OmegaConf.to_container(OmegaConf.load(\"/content/animatediff/configs/noise\n",
        "args"
      ],
      "metadata": {
        "id": "CiqNXwq_Bfod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # base: \"\"\n",
        "  # path: \"models/DreamBooth_LoRA/toonyou_beta3.safetensors\"\n",
        "  # motion_module:\n",
        "  #   - \"models/Motion_Module/mm_sd_v14.ckpt\"\n",
        "  #   - \"models/Motion_Module/mm_sd_v15.ckpt\"\n",
        "\n",
        "  # seed:           [10788741199826055526, 6520604954829636163, 6519455744612555650, 16372571278361863751]\n",
        "  # steps:          25\n",
        "  # guidance_scale: 7.5\n",
        "\n",
        "  # prompt:\n",
        "  #   - \"best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress\"\n",
        "  #   - \"masterpiece, best quality, 1girl, solo, cherry blossoms, hanami, pink flower, white flower, spring season, wisteria, petals, flower, plum blossoms, outdoors, falling petals, white hair, black eyes,\"\n",
        "  #   - \"best quality, masterpiece, 1boy, formal, abstract, looking at viewer, masculine, marble pattern\"\n",
        "  #   - \"best quality, masterpiece, 1girl, cloudy sky, dandelion, contrapposto, alternate hairstyle,\"\n",
        "\n",
        "  # n_prompt:\n",
        "  #   - \"\"\n",
        "  #   - \"badhandv4,easynegative,ng_deepnegative_v1_75t,verybadimagenegative_v1.3, bad-artist, bad_prompt_version2-neg, teeth\"\n",
        "  #   - \"\"\n",
        "  #   - \"\""
      ],
      "metadata": {
        "id": "_YXSaA8CEAEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import napm\n",
        "dir(napm)\n",
        "cfg = napm.config.NapmConfig().load()\n",
        "PKG_ROOT = Path(cfg['packages']['animatediff']['install_dir'])"
      ],
      "metadata": {
        "id": "ZLHhC1qDF2JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = SimpleNamespace()\n",
        "model_config.path = ''\n",
        "model_config.motion_module = \"/content/animatediff_github/models/Motion_Module/mm_sd_v15.ckpt\"  #PKG_ROOT/\"models/Motion_Module/mm_sd_v15.ckpt\"\n",
        "model_config.seed = 10788741199826055526\n",
        "model_config.steps = 25\n",
        "model_config.guidance_scale = 7.5\n",
        "model_config.prompt = \"best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress\"\n",
        "model_config.n_prompt = \"\"\n",
        "#unet_additional_kwargs = {}\n",
        "\n",
        "#parser.add_argument(\"--inference_config\",      type=str, default=\"configs/inference/inference.yaml\")\n",
        "#inference_config = SimpleNamespace()\n",
        "inference_config = OmegaConf.load(PKG_ROOT/\"configs/inference/inference.yaml\")\n",
        "inference_config"
      ],
      "metadata": {
        "id": "0BE84AjbDdca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer    = CLIPTokenizer.from_pretrained(args.pretrained_model_path, subfolder=\"tokenizer\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_path, subfolder=\"text_encoder\")\n",
        "vae          = AutoencoderKL.from_pretrained(args.pretrained_model_path, subfolder=\"vae\")\n",
        "unet         = UNet3DConditionModel.from_pretrained_2d(args.pretrained_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.get(\"unet_additional_kwargs\", {})))\n",
        "\n",
        "pipeline = AnimationPipeline(\n",
        "  vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n",
        "  scheduler=DDIMScheduler(**OmegaConf.to_container(inference_config.noise_scheduler_kwargs)),\n",
        ").to(\"cuda\")"
      ],
      "metadata": {
        "id": "XMbZhSyvB-9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "func_args = SimpleNamespace\n",
        "motion_module_state_dict = torch.load(model_config.motion_module, map_location=\"cpu\")\n",
        "if \"global_step\" in motion_module_state_dict:\n",
        "  func_args.update({\"global_step\": motion_module_state_dict[\"global_step\"]})\n",
        "missing, unexpected = pipeline.unet.load_state_dict(motion_module_state_dict, strict=False)\n",
        "assert len(unexpected) == 0"
      ],
      "metadata": {
        "id": "2FAjg2fjIZJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 1.2 T2I\n",
        "if model_config.path != \"\":\n",
        "    if model_config.path.endswith(\".ckpt\"):\n",
        "        state_dict = torch.load(model_config.path)\n",
        "        pipeline.unet.load_state_dict(state_dict)\n",
        "\n",
        "    elif model_config.path.endswith(\".safetensors\"):\n",
        "        state_dict = {}\n",
        "        with safe_open(model_config.path, framework=\"pt\", device=\"cpu\") as f:\n",
        "            for key in f.keys():\n",
        "                state_dict[key] = f.get_tensor(key)\n",
        "\n",
        "        is_lora = all(\"lora\" in k for k in state_dict.keys())\n",
        "        if not is_lora:\n",
        "            base_state_dict = state_dict\n",
        "        else:\n",
        "            base_state_dict = {}\n",
        "            with safe_open(model_config.base, framework=\"pt\", device=\"cpu\") as f:\n",
        "                for key in f.keys():\n",
        "                    base_state_dict[key] = f.get_tensor(key)\n",
        "\n",
        "        # vae\n",
        "        converted_vae_checkpoint = convert_ldm_vae_checkpoint(base_state_dict, pipeline.vae.config)\n",
        "        pipeline.vae.load_state_dict(converted_vae_checkpoint)\n",
        "        # unet\n",
        "        converted_unet_checkpoint = convert_ldm_unet_checkpoint(base_state_dict, pipeline.unet.config)\n",
        "        pipeline.unet.load_state_dict(converted_unet_checkpoint, strict=False)\n",
        "        # text_model\n",
        "        pipeline.text_encoder = convert_ldm_clip_checkpoint(base_state_dict)\n",
        "\n",
        "        # import pdb\n",
        "        # pdb.set_trace()\n",
        "        if is_lora:\n",
        "            pipeline = convert_lora(pipeline, state_dict, alpha=model_config.lora_alpha)\n",
        "\n",
        "pipeline.to(\"cuda\")\n",
        "### <<< create validation pipeline <<< ###"
      ],
      "metadata": {
        "id": "9lfK9KBbKENt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = pipeline(\n",
        "    prompt=model_config.prompt,\n",
        "    negative_prompt     = model_config.n_prompt,\n",
        "    num_inference_steps = model_config.steps,\n",
        "    guidance_scale      = model_config.guidance_scale,\n",
        "    width               = args.W,\n",
        "    height              = args.H,\n",
        "    video_length        = args.L,\n",
        ").videos\n"
      ],
      "metadata": {
        "id": "g8gLhMqjGszy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = torch.concat([sample])\n",
        "savedir=\".\"\n",
        "save_videos_grid(samples, f\"{savedir}/sample.gif\", n_rows=1)"
      ],
      "metadata": {
        "id": "EfD97tv3HIc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}