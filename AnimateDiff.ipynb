{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM0rHJnbM0XRQo+pv/HUyF1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmarx/notebooks/blob/animate_diff/AnimateDiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unofficial AnimateDiff Demo\n",
        "\n",
        "> \"[AnimateDiff](https://animatediff.github.io/): Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning\"  \n",
        "> by [Yuwei Guo](guoyuwei@pjlab.org.cn), [Ceyuan Yang](https://ceyuan.me/) [Anyi Rao](https://anyirao.com/), [Yaohui Wang](https://wyhsirius.github.io/) [Yu Qiao](https://wyhsirius.github.io/) [Dahua Lin](http://dahua.site/) [Bo Dai](https://daibo.info/)\n",
        "\n",
        "\n",
        "```\n",
        "@misc{guo2023animatediff,\n",
        "  title={AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning},\n",
        "  author={Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai},\n",
        "  booktitle={arXiv preprint arxiv:2307.04725},\n",
        "  year={2023},\n",
        "  archivePrefix={arXiv},\n",
        "  primaryClass={cs.CV}\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "[Notebook](https://colab.research.google.com/github/dmarx/notebooks/blob/main/AnimateDiff.ipynb) by [David Marx](https://twitter.com/DigThatData), notebook brought to you by [Stability AI](https://stability.ai/)\n",
        "\n",
        "Setup adapted from Camenduru.  \n",
        "Rest of code mostly cannibalized from: https://github.com/guoyww/AnimateDiff/blob/main/scripts/animate.py\n",
        "\n",
        "Shared under MIT License.\n",
        "\n",
        "To report bugs or offer suggestions regarding the notebook, file an issue here: https://github.com/dmarx/notebooks/"
      ],
      "metadata": {
        "id": "v04sBovMvKBy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5R8KaG34V7h"
      },
      "outputs": [],
      "source": [
        "# @title setup\n",
        "\n",
        "fpath_sd_model = \"/content/models/StableDiffusion/\" # @param {type:\"string\"}\n",
        "fpath_motion_prior = \"/content/models/Motion_Module/\" # @param {type:\"string\"}\n",
        "fpath_dreambooth_lora = \"/content/models/DreamBooth_LoRA/\" # @param {type:\"string\"}\n",
        "\n",
        "#####################\n",
        "\n",
        "# Install stuff\n",
        "\n",
        "!pip install napm einops omegaconf safetensors diffusers[torch]==0.11.1 transformers\n",
        "\n",
        "# TODO: use huggingface hub rust downloader, backout aria dependency\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "\n",
        "# Handle uninstallable research dependencies\n",
        "import napm\n",
        "napm.pseudoinstall_git_repo(\"https://github.com/guoyww/animatediff/\", package_name='animatediff', add_install_dir_to_path=True)\n",
        "\n",
        "# Download models as needed\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "if not Path(fpath_sd_model).exists():\n",
        "    !mkdir -p {fpath_sd_model}\n",
        "    !git clone -b fp16 https://huggingface.co/runwayml/stable-diffusion-v1-5 {fpath_sd_model}\n",
        "\n",
        "if not Path(fpath_motion_prior).exists():\n",
        "    !mkdir -p {fpath_motion_prior}\n",
        "    #!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AnimateDiff/resolve/main/mm_sd_v14.ckpt -d {fpath_motion_prior} -o mm_sd_v14.ckpt\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AnimateDiff/resolve/main/mm_sd_v15.ckpt -d {fpath_motion_prior} -o mm_sd_v15.ckpt\n",
        "\n",
        "if not Path(fpath_dreambooth_lora).exists():\n",
        "    !mkdir -p {fpath_dreambooth_lora}\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AnimateDiff/resolve/main/toonyou_beta3.safetensors -d {fpath_dreambooth_lora} -o toonyou_beta3.safetensors\n",
        "\n",
        "############################################\n",
        "\n",
        "# import stuff\n",
        "\n",
        "#import argparse\n",
        "#import inspect\n",
        "#import csv #, pdb, glob\n",
        "#import pdb\n",
        "#import glob\n",
        "#import math\n",
        "#from einops import rearrange, repeat\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "from pathlib import Path\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import diffusers\n",
        "from diffusers import AutoencoderKL, DDIMScheduler\n",
        "from omegaconf import OmegaConf\n",
        "from safetensors import safe_open\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "# napm needs to be imported before animatediff\n",
        "import napm\n",
        "from animatediff.models.unet import UNet3DConditionModel\n",
        "from animatediff.pipelines.pipeline_animation import AnimationPipeline\n",
        "from animatediff.utils.util import save_videos_grid\n",
        "from animatediff.utils.convert_from_ckpt import convert_ldm_unet_checkpoint, convert_ldm_clip_checkpoint, convert_ldm_vae_checkpoint\n",
        "from animatediff.utils.convert_lora_safetensor_to_diffusers import convert_lora\n",
        "\n",
        "# dig up the config yaml from the napm dependency\n",
        "cfg = napm.config.NapmConfig().load()\n",
        "PKG_ROOT = Path(cfg['packages']['animatediff']['install_dir'])\n",
        "\n",
        "inference_config = OmegaConf.load(PKG_ROOT/\"configs/inference/inference.yaml\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SETTINGS (crystal says i should fix this name later)\n",
        "\n",
        "# --pretrained_model_path /content/animatediff/models/StableDiffusion --L 16 --W 256 --H 256\n",
        "args = SimpleNamespace()\n",
        "\n",
        "#args.pretrained_model_path = \"/content/models/StableDiffusion\" # @param {type:\"string\"}\n",
        "args.pretrained_model_path = fpath_sd_model\n",
        "args.L = 16 # @param {type:\"integer\"}\n",
        "args.W = 448 # @param {type:\"integer\"}\n",
        "args.H = 320 # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "model_config = SimpleNamespace()\n",
        "#model_config.path = '/content/models/DreamBooth_LoRA/toonyou_beta3.safetensors' # @param {type:\"string\"}\n",
        "model_config.path = str(list(Path(fpath_dreambooth_lora).glob('*.safetensors'))[0])\n",
        " #PKG_ROOT/\"models/Motion_Module/mm_sd_v15.ckpt\"\n",
        "#model_config.motion_module = \"/content/models/Motion_Module/mm_sd_v15.ckpt\" # @param {type:\"string\"}\n",
        "model_config.motion_module = str(list(Path(fpath_motion_prior).glob(\"*.ckpt\"))[0])\n",
        "model_config.seed = 10788741199826055526 # @param {type:\"integer\"}\n",
        "model_config.steps = 25 # @param {type:\"integer\"}\n",
        "model_config.guidance_scale = 7.5 # @param {type:\"number\"}\n",
        "model_config.prompt = \"apollo 13, 'houston we have a problem', tom hanks playing an astronaut\" # @param {type:\"string\"}\n",
        "model_config.n_prompt = \"stationary, motionless, boring, watermark, trademark, copyright, text, shutterstock\" # @param {type:\"string\"}\n",
        "\n",
        "#unet_additional_kwargs = {}\n"
      ],
      "metadata": {
        "id": "CiqNXwq_Bfod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load Models\n",
        "\n",
        "tokenizer    = CLIPTokenizer.from_pretrained(args.pretrained_model_path, subfolder=\"tokenizer\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_path, subfolder=\"text_encoder\")\n",
        "vae          = AutoencoderKL.from_pretrained(args.pretrained_model_path, subfolder=\"vae\")\n",
        "unet         = UNet3DConditionModel.from_pretrained_2d(args.pretrained_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.get(\"unet_additional_kwargs\", {})))\n",
        "\n",
        "pipeline = AnimationPipeline(\n",
        "  vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n",
        "  scheduler=DDIMScheduler(**OmegaConf.to_container(inference_config.noise_scheduler_kwargs)),\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "# probably wanna change this\n",
        "func_args = SimpleNamespace\n",
        "\n",
        "motion_module_state_dict = torch.load(model_config.motion_module, map_location=\"cpu\")\n",
        "if \"global_step\" in motion_module_state_dict:\n",
        "  func_args.update({\"global_step\": motion_module_state_dict[\"global_step\"]})\n",
        "missing, unexpected = pipeline.unet.load_state_dict(motion_module_state_dict, strict=False)\n",
        "assert len(unexpected) == 0\n",
        "\n",
        "\n",
        "\n",
        " # 1.2 T2I\n",
        "if model_config.path != \"\":\n",
        "    if model_config.path.endswith(\".ckpt\"):\n",
        "        state_dict = torch.load(model_config.path)\n",
        "        pipeline.unet.load_state_dict(state_dict)\n",
        "\n",
        "    elif model_config.path.endswith(\".safetensors\"):\n",
        "        state_dict = {}\n",
        "        with safe_open(model_config.path, framework=\"pt\", device=\"cpu\") as f:\n",
        "            for key in f.keys():\n",
        "                state_dict[key] = f.get_tensor(key)\n",
        "\n",
        "        is_lora = all(\"lora\" in k for k in state_dict.keys())\n",
        "        if not is_lora:\n",
        "            base_state_dict = state_dict\n",
        "        else:\n",
        "            base_state_dict = {}\n",
        "            with safe_open(model_config.base, framework=\"pt\", device=\"cpu\") as f:\n",
        "                for key in f.keys():\n",
        "                    base_state_dict[key] = f.get_tensor(key)\n",
        "\n",
        "        # vae\n",
        "        converted_vae_checkpoint = convert_ldm_vae_checkpoint(base_state_dict, pipeline.vae.config)\n",
        "        pipeline.vae.load_state_dict(converted_vae_checkpoint)\n",
        "        # unet\n",
        "        converted_unet_checkpoint = convert_ldm_unet_checkpoint(base_state_dict, pipeline.unet.config)\n",
        "        pipeline.unet.load_state_dict(converted_unet_checkpoint, strict=False)\n",
        "        # text_model\n",
        "        pipeline.text_encoder = convert_ldm_clip_checkpoint(base_state_dict)\n",
        "\n",
        "        # import pdb\n",
        "        # pdb.set_trace()\n",
        "        if is_lora:\n",
        "            pipeline = convert_lora(pipeline, state_dict, alpha=model_config.lora_alpha)\n",
        "\n",
        "pipeline.to(\"cuda\")\n",
        "### <<< create validation pipeline <<< ###"
      ],
      "metadata": {
        "id": "XMbZhSyvB-9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = pipeline(\n",
        "    prompt=model_config.prompt,\n",
        "    negative_prompt     = model_config.n_prompt,\n",
        "    num_inference_steps = model_config.steps,\n",
        "    guidance_scale      = model_config.guidance_scale,\n",
        "    width               = args.W,\n",
        "    height              = args.H,\n",
        "    video_length        = args.L,\n",
        ").videos\n",
        "\n",
        "samples = torch.concat([sample])\n",
        "savedir=\".\"\n",
        "outpath = f\"{savedir}/sample.gif\"\n",
        "save_videos_grid(samples, outpath , n_rows=1)"
      ],
      "metadata": {
        "id": "g8gLhMqjGszy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Show me the gif\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "Image(outpath)\n"
      ],
      "metadata": {
        "id": "EfD97tv3HIc4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}