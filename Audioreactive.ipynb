{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc75050-cbff-4cad-be82-191bfb2cb707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # @title # üõ†Ô∏è Install dependencies\n",
    "\n",
    "# try: \n",
    "#     import google.colab\n",
    "#     local=False\n",
    "# except:\n",
    "#     local=True\n",
    "\n",
    "# # TODO: pin versions\n",
    "    \n",
    "# # local only additional dependencies\n",
    "# if local:\n",
    "#     %pip install pandas torch pillow beautifulsoup4 scipy toolz numpy lxml librosa scikit-learn\n",
    "\n",
    "# # dependencies for both colab and local\n",
    "# %pip install yt-dlp python-tsp stability-sdk[anim_ui] diffusers transformers ftfy accelerate omegaconf\n",
    "# %pip install openai-whisper panel huggingface_hub ipywidgets safetensors keyframed demucs parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041ba01-101e-430c-a667-7a1f2110cd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime as dt\n",
    "import gc\n",
    "import io\n",
    "from itertools import chain, cycle\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display\n",
    "import keyframed\n",
    "import keyframed.dsl\n",
    "import librosa\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import parse\n",
    "\n",
    "from safetensors.numpy import save_file as save_safetensors\n",
    "from safetensors.numpy import load_file as load_safetensors\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import sklearn.cluster\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "########################\n",
    "\n",
    "# wrap some of the loading logic for portability\n",
    "\n",
    "def save_storyboard(storyboard):\n",
    "    root = Path(load_workspace().project_root)\n",
    "    storyboard_fname = root / 'storyboard.yaml'\n",
    "    with open(storyboard_fname, 'w') as fp: #TODO: specify writemode in the vktrs version\n",
    "        OmegaConf.save(config=storyboard, f=fp.name)\n",
    "\n",
    "def load_workspace():\n",
    "    return OmegaConf.load('config.yaml')\n",
    "        \n",
    "def load_storyboard():\n",
    "    workspace = load_workspace()\n",
    "    root = Path(workspace.project_root)\n",
    "    storyboard_fname = root / 'storyboard.yaml'\n",
    "    storyboard = OmegaConf.load(storyboard_fname)\n",
    "    return workspace, storyboard\n",
    "\n",
    "def load_audio_meta(workspace, storyboard):\n",
    "    assets_dir = Path(workspace.shared_assets_root)\n",
    "    audio_assets_meta_fname = assets_dir / 'audio_assets_meta.yaml'\n",
    "    audio_assets_meta = OmegaConf.load(audio_assets_meta_fname)\n",
    "    audio_meta=dict()\n",
    "    for idx, rec in enumerate(audio_assets_meta.content):\n",
    "        if rec.audio_fpath == storyboard.params.audio_fpath:\n",
    "            audio_meta = rec\n",
    "            break\n",
    "    return audio_meta\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "\n",
    "\n",
    "# audio processing\n",
    "\n",
    "\n",
    "def analyze_audio_structure(\n",
    "    audio_fpath,\n",
    "    BINS_PER_OCTAVE = 12 * 3, # should be a multiple of twelve: https://github.com/MTG/essentia/blob/master/src/examples/python/tutorial_spectral_constantq-nsg.ipynb\n",
    "    N_OCTAVES = 7,\n",
    "):\n",
    "    \"\"\"\n",
    "    via librosa docs\n",
    "    https://librosa.org/doc/latest/auto_examples/plot_segmentation.html#sphx-glr-auto-examples-plot-segmentation-py\n",
    "    cites: McFee and Ellis, 2014 - https://brianmcfee.net/papers/ismir2014_spectral.pdf\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(audio_fpath)\n",
    "\n",
    "    C = librosa.amplitude_to_db(np.abs(librosa.cqt(y=y, sr=sr,\n",
    "                                            bins_per_octave=BINS_PER_OCTAVE,\n",
    "                                            n_bins=N_OCTAVES * BINS_PER_OCTAVE)),\n",
    "                                ref=np.max)\n",
    "\n",
    "    # reduce dimensionality via beat-synchronization\n",
    "    tempo, beats = librosa.beat.beat_track(y=y, sr=sr, trim=False)\n",
    "    Csync = librosa.util.sync(C, beats, aggregate=np.median)\n",
    "    \n",
    "    # I have concerns about this frame fixing operation\n",
    "    beat_times = librosa.frames_to_time(librosa.util.fix_frames(beats, x_min=0), sr=sr)\n",
    "\n",
    "    # width=3 prevents links within the same bar \n",
    "    # mode=‚Äôaffinity‚Äô here implements S_rep (after Eq. 8)\n",
    "    R = librosa.segment.recurrence_matrix(Csync, width=3, mode='affinity', sym=True)\n",
    "    # Enhance diagonals with a median filter (Equation 2)\n",
    "    df = librosa.segment.timelag_filter(scipy.ndimage.median_filter)\n",
    "    Rf = df(R, size=(1, 7))\n",
    "    # build the sequence matrix (S_loc) using mfcc-similarity\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "    Msync = librosa.util.sync(mfcc, beats)\n",
    "    path_distance = np.sum(np.diff(Msync, axis=1)**2, axis=0)\n",
    "    sigma = np.median(path_distance)\n",
    "    path_sim = np.exp(-path_distance / sigma)\n",
    "    R_path = np.diag(path_sim, k=1) + np.diag(path_sim, k=-1)\n",
    "    # compute the balanced combination\n",
    "    deg_path = np.sum(R_path, axis=1)\n",
    "    deg_rec = np.sum(Rf, axis=1)\n",
    "    mu = deg_path.dot(deg_path + deg_rec) / np.sum((deg_path + deg_rec)**2)\n",
    "    A = mu * Rf + (1 - mu) * R_path\n",
    "\n",
    "    # compute normalized laplacian and its spectrum\n",
    "    L = scipy.sparse.csgraph.laplacian(A, normed=True)\n",
    "    evals, evecs = scipy.linalg.eigh(L)\n",
    "    # clean this up with a median filter. can help smooth over discontinuities\n",
    "    evecs = scipy.ndimage.median_filter(evecs, size=(9, 1))\n",
    "    return dict(\n",
    "        y=y, \n",
    "        sr=np.array(sr).astype(np.uint32),\n",
    "        tempo=tempo,\n",
    "        beats=beats,\n",
    "        beat_times=beat_times,\n",
    "        evecs=evecs,\n",
    "    )\n",
    "\n",
    "    \n",
    "def laplacian_segmentation(\n",
    "    audio_fpath=None,\n",
    "    evecs=None,\n",
    "    n_clusters = 5,\n",
    "    n_spectral_features = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    segment audio by clustering a self-similarity matrix.\n",
    "    via librosa docs\n",
    "    https://librosa.org/doc/latest/auto_examples/plot_segmentation.html#sphx-glr-auto-examples-plot-segmentation-py\n",
    "    cites: McFee and Ellis, 2014 - https://brianmcfee.net/papers/ismir2014_spectral.pdf\n",
    "    \"\"\"\n",
    "    if evecs is None:\n",
    "        if audio_fpath is None:\n",
    "            raise Exception(\"One of `audio_fpath` or `evecs` must be provided\")\n",
    "        features = analyze_audio_structure(audio_fpath)\n",
    "        evecs = features['evecs']\n",
    "    \n",
    "    if n_spectral_features is None:\n",
    "        n_spectral_features = n_clusters\n",
    "\n",
    "    # cumulative normalization is needed for symmetric normalize laplacian eigenvectors\n",
    "    Cnorm = np.cumsum(evecs**2, axis=1)**0.5\n",
    "    k = n_spectral_features\n",
    "    X = evecs[:, :k] / Cnorm[:, k-1:k]\n",
    "\n",
    "    # use these k components to cluster beats into segments\n",
    "    KM = sklearn.cluster.KMeans(n_clusters=n_clusters, n_init=\"auto\")\n",
    "    seg_ids = KM.fit_predict(X)\n",
    "\n",
    "    return seg_ids #, beat_times, tempo\n",
    "\n",
    "\n",
    "# for video duration\n",
    "def get_audio_duration_seconds(audio_fpath):\n",
    "    outv = subprocess.run([\n",
    "        'ffprobe'\n",
    "        ,'-i',audio_fpath\n",
    "        ,'-show_entries', 'format=duration'\n",
    "        ,'-v','quiet'\n",
    "        ,'-of','csv=p=0'\n",
    "        ],\n",
    "        stdout=subprocess.PIPE\n",
    "        ).stdout.decode('utf-8')\n",
    "    return float(outv.strip())\n",
    "\n",
    "\n",
    "##########################################\n",
    "\n",
    "\n",
    "# audioreactivity stuff\n",
    "\n",
    "\n",
    "def full_width_plot():\n",
    "    ax = plt.gca()\n",
    "    ax.figure.set_figwidth(20)\n",
    "    plt.show()\n",
    "\n",
    "def display_signal(y, sr, show_spec=True, title=None):\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    full_width_plot()\n",
    "    \n",
    "    if show_spec:\n",
    "        try:\n",
    "            times = librosa.times_like(y, sr=sr)\n",
    "            M = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "            librosa.display.specshow(librosa.power_to_db(M, ref=np.max),\n",
    "                             y_axis='mel', x_axis='time')\n",
    "            full_width_plot()\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "# https://github.com/pytti-tools/pytti-core/blob/9e8568365cfdc123d2d2fbc20d676ca0f8715341/src/pytti/AudioParse.py#L95\n",
    "from scipy.signal import butter, sosfilt, sosfreqz\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = butter(order, [low, high], analog=False, btype='bandpass', output='sos')\n",
    "    return sos\n",
    "\n",
    "def butter_bandpass_filter(data, sr, lowcut, highcut, order=10):\n",
    "    sos = butter_bandpass(lowcut, highcut, sr, order=order)\n",
    "    y = sosfilt(sos, data)\n",
    "    return y\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def show_storyboard(storyboard=None):\n",
    "    if storyboard is None:\n",
    "        workspace, storyboard = load_storyboard()\n",
    "    reactive_signal_map = storyboard.audioreactive.get('reactive_signal_map')\n",
    "    for idx, rec in enumerate(storyboard.prompt_starts):\n",
    "        report =(\n",
    "        f\"scene: {idx}\\t start: {rec['start']:.2f}\\t duration: {rec['duration_']}\\n\"\n",
    "        f\"spoken text: {rec.get('text')}\\n\"\n",
    "    )\n",
    "        # TODO: wrap prompt construction logic in a function\n",
    "        \n",
    "        #'_theme':'theme', 'structural_segmentation_label':\n",
    "        if rec.get('_theme'):\n",
    "            report += f\"theme prompt: {rec['_theme']}\\n\"\n",
    "        #f\"image prompt: {rec['_prompt']}\\n\"\n",
    "        prompt = rec.get('prompt')\n",
    "        #if not prompt:\n",
    "        #    prompt = ...\n",
    "        if prompt:\n",
    "            report += f\"image prompt: {rec['_prompt']}\\n\"\n",
    "        \n",
    "        if rec.get('animation_mode'):\n",
    "            report += f\"animation mode: {animation_mode}\"\n",
    "        print(report)\n",
    "        im_path = rec.get('frame0_fpath')\n",
    "        if im_path and Path(im_path).exists():\n",
    "            display(Image.open(rec['frame0_fpath']))\n",
    "\n",
    "        if reactive_signal_map:\n",
    "            n = rec['frames']\n",
    "            if n <1:\n",
    "                continue\n",
    "            for signal_name in reactive_signal_map.keys():\n",
    "                if signal_name in rec:\n",
    "                    curve = kf.dsl.curve_from_cn_string(rec[signal_name])\n",
    "                    xs = [i for i in range(n)]\n",
    "                    ys = [curve[i] for i in xs]\n",
    "                    plt.plot(xs, ys, label=signal_name)\n",
    "                plt.title(f\"scene {idx}\")\n",
    "                plt.xlabel(\"frame index within scene\")\n",
    "                plt.legend()\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c491860-32ed-4167-8ee1-df8398b71475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title # üîë Setup Workspace\n",
    "\n",
    "mount_gdrive = True # @param {type:'boolean'}\n",
    "\n",
    "try: \n",
    "    import google.colab\n",
    "    local=False\n",
    "except:\n",
    "    local=True\n",
    "\n",
    "if local:\n",
    "    mount_gdrive=False\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import string\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "os.environ['XDG_CACHE_HOME'] = os.environ.get(\n",
    "    'XDG_CACHE_HOME',\n",
    "    str(Path('~/.cache').expanduser())\n",
    ")\n",
    "if mount_gdrive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    Path('/content/drive/MyDrive/AI/models/.cache/').mkdir(parents=True, exist_ok=True) \n",
    "    os.environ['XDG_CACHE_HOME']='/content/drive/MyDrive/AI/models/.cache'\n",
    "\n",
    "model_dir_str=str(Path(os.environ['XDG_CACHE_HOME']))\n",
    "proj_root_str = '${active_project}'\n",
    "application_root = str(Path('.').absolute())\n",
    "if mount_gdrive:\n",
    "    application_root = '/content/drive/MyDrive/AI/VideoKilledTheRadioStar'\n",
    "\n",
    "\n",
    "# notebook config\n",
    "cfg = OmegaConf.create({\n",
    "    'active_project':str(time.time()),\n",
    "    'application_root':application_root,\n",
    "    'project_root':\"${application_root}/${active_project}\",\n",
    "    'shared_assets_root':\"${application_root}/shared_assets\",\n",
    "    'gdrive_mounted':mount_gdrive,\n",
    "    'model_dir':model_dir_str,\n",
    "    #'output_dir':'${project_root}/frames'\n",
    "})\n",
    "\n",
    "with open('config.yaml','w') as fp:\n",
    "    OmegaConf.save(config=cfg, f=fp.name)\n",
    "\n",
    "# TODO: fix name consistency\n",
    "workspace = cfg\n",
    "\n",
    "###################\n",
    "\n",
    "# add some tracking to reduce duplicated processing\n",
    "assets_dir = Path(cfg.shared_assets_root)\n",
    "assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# TODO: yaml -> jsonl ?\n",
    "video_assets_meta_fname = assets_dir / 'video_assets_meta.yaml'\n",
    "if not video_assets_meta_fname.exists():\n",
    "    video_assets_meta = OmegaConf.create()\n",
    "    video_assets_meta.videos = []\n",
    "    with video_assets_meta_fname.open('w') as fp:\n",
    "        OmegaConf.save(config=video_assets_meta, f=fp.name)\n",
    "else:\n",
    "    video_assets_meta = OmegaConf.load(video_assets_meta_fname)\n",
    "\n",
    "audio_assets_meta_fname = assets_dir / 'audio_assets_meta.yaml'\n",
    "if not audio_assets_meta_fname.exists():\n",
    "    audio_assets_meta = OmegaConf.create()\n",
    "    audio_assets_meta.content = []\n",
    "    with audio_assets_meta_fname.open('w') as fp:\n",
    "        OmegaConf.save(config=audio_assets_meta, f=fp.name)\n",
    "else:\n",
    "    audio_assets_meta = OmegaConf.load(audio_assets_meta_fname)\n",
    "\n",
    "###################\n",
    "\n",
    "# if use_stability_api:\n",
    "#     import os, getpass\n",
    "#     if not os.environ.get('STABILITY_KEY'):\n",
    "#         os.environ['STABILITY_KEY'] = getpass.getpass('Enter your Stability API Key, then press enter to continue')\n",
    "# else:\n",
    "#     # TODO: check for HF token in environment\n",
    "#     if not local:\n",
    "#         from google.colab import output\n",
    "#         output.enable_custom_widget_manager()\n",
    "        \n",
    "#     from huggingface_hub import notebook_login\n",
    "#     notebook_login()\n",
    "    \n",
    "##########################\n",
    "\n",
    "# more environment stuff\n",
    "# create/load remaining folders/files\n",
    "\n",
    "assets_dir = Path(workspace.shared_assets_root)\n",
    "video_assets_meta_fname = assets_dir / 'video_assets_meta.yaml'\n",
    "video_assets_meta = OmegaConf.load(video_assets_meta_fname)\n",
    "audio_assets_meta_fname = assets_dir / 'audio_assets_meta.yaml'\n",
    "audio_assets_meta = OmegaConf.load(audio_assets_meta_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d82894-d06e-4d37-a817-fc006844af8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: combine this w workspace setup cell\n",
    "\n",
    "def sanitize_folder_name(fp):\n",
    "    outv = ''\n",
    "    whitelist = string.ascii_letters + string.digits + '-_'\n",
    "    for token in str(fp):\n",
    "        if token not in whitelist:\n",
    "            token = '-'\n",
    "        outv += token\n",
    "    return outv\n",
    "\n",
    "project_name = 'audioreactive' # @param {type:'string'}\n",
    "if not project_name:\n",
    "    project_name = str(time.time())\n",
    "\n",
    "project_name = sanitize_folder_name(project_name)\n",
    "\n",
    "workspace.active_project = project_name\n",
    "with open('config.yaml','w') as fp:\n",
    "    OmegaConf.save(config=workspace, f=fp.name)\n",
    "\n",
    "root = Path(workspace.project_root)\n",
    "root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "resuming=False\n",
    "try:\n",
    "    workspace, storyboard = load_storyboard()\n",
    "    print(\"loading storyboard\")\n",
    "    resuming=True\n",
    "except:\n",
    "    print(\"creating new storyboard\")\n",
    "    storyboard = OmegaConf.create()\n",
    "    storyboard.params = {}\n",
    "    save_storyboard(storyboard) # needs to exist\n",
    "# @markdown To create a new project, enter a unique project name.\n",
    "# @markdown If you leave `project_name` blank, the current unix timestamp will be used\n",
    "# @markdown  (seconds since 1970-01-01 00:00).\n",
    "\n",
    "# @markdown If you use the name of an existing project, the workspace will switch to that project.\n",
    "\n",
    "# @markdown Non-alphanumeric characters (excluding '-' and '_') will be replaced with hyphens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07dcbb6-cc71-4720-9710-b8b2a6ad73db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# alternatively \n",
    "\n",
    "d_ = dict(\n",
    "    # all the underscore does is make it so each of the following lines can be preceded with a comma\n",
    "    # otw the first parameter would be offset from the other in the colab form\n",
    "    _=\"\"\n",
    "    , audio_fpath = '/home/dmarx/projects/video-killed-the-radio-star/shared_assets/DOWNLOADED__Burnout.m4a' # @param {type:'string'}\n",
    ")\n",
    "d_.pop('_')\n",
    "\n",
    "# @markdown `video_url` - URL of a youtube video to download as a source for audio and potentially for text transcription as well.\n",
    "\n",
    "# @markdown `audio_fpath` - Optionally provide an audio file instead of relying on a youtube download. Name it something other than 'audio.mp3', \n",
    "# @markdown                 otherwise it might get overwritten accidentally.\n",
    "\n",
    "\n",
    "storyboard.params = d_\n",
    "save_storyboard(storyboard)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201cd1e-f0f2-4daa-bad1-4271d8b3be5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Music Structure Analysis\n",
    "\n",
    "# Music Structure Analysis\n",
    "# - beat and tempo detection\n",
    "# - Self-similarity graph\n",
    "\n",
    "# TODO: detect previously processed audio\n",
    "\n",
    "audio_meta = load_audio_meta(*load_storyboard())\n",
    "\n",
    "audio_structure_features = analyze_audio_structure(audio_fpath=storyboard.params.audio_fpath)\n",
    "audio_features_fpath = Path(storyboard.params.audio_fpath).with_suffix('.audio_features.safetensors')\n",
    "save_safetensors(audio_structure_features, audio_features_fpath)\n",
    "\n",
    "assets_dir = Path(workspace.shared_assets_root)\n",
    "for idx, rec in enumerate(audio_assets_meta.content):\n",
    "    if rec.audio_fpath == storyboard.params.audio_fpath:\n",
    "        audio_meta = rec\n",
    "        break\n",
    "audio_meta['structural_features'] = audio_features_fpath\n",
    "\n",
    "# TODO: Make sure we're not creating a duplicate metadata record here \n",
    "if 'audio_fpath' not in audio_meta:\n",
    "    audio_meta['audio_fpath'] = storyboard.params.audio_fpath\n",
    "    audio_assets_meta.content.append(audio_meta)\n",
    "\n",
    "\n",
    "with open(audio_assets_meta_fname, 'wb') as fp:\n",
    "    OmegaConf.save(config=audio_assets_meta, f=fp.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8da9d-45c4-4a09-b12e-1c33cc7d1519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Theme -> Scene Assignment\n",
    "\n",
    "# @markdown `theme_prompt` - Text that will be appended to the end of each lyric, useful for e.g. applying a consistent aesthetic style. To provide multiple themes (only one will be used per scene), separate theme prompts with the `|` (pipe) symbol.\n",
    "\n",
    "# @markdown `infer_thematic_structure` - if False, themes will be rotated sequentially such that no two adjacent frames\n",
    "# @markdown will use the same theme prompt (if multiple theme prompts were provided). If True, song structure analysis will cluster related scenes into as many groups as there are theme prompts to attempt to associate a visual themes with respective musical themes.\n",
    "\n",
    "# @markdown The analysis runs quick. If you didn't understand that explanation, just try it both ways and you'll probably get the idea.\n",
    "\n",
    "#theme_prompt = 'Katsuhiro Otomo gundam mecha | 1980s transformers autobots | paperclips! paperclips! |  robotics for beginners | rusted industrial machinery' # @param {type:'string'}\n",
    "\n",
    "#infer_thematic_structure = True # @param {type:'boolean'}\n",
    "\n",
    "# theme_prompt = ( \n",
    "#     \", Katsuhiro Otomo gundam mecha |  \"\n",
    "#     \", 1980s transformers autobots | \"\n",
    "#     \", paperclips! paperclips! | \"\n",
    "#     \", robotics for beginners | \"\n",
    "#     \", rusted industrial machinery \"    \n",
    "# )\n",
    "#storyboard.params.theme_prompt = theme_prompt\n",
    "#themes = [prompt.strip() for prompt in theme_prompt.split('|') if prompt.strip()]\n",
    "\n",
    "n_clusters = 5\n",
    "\n",
    "if n_clusters > 1:\n",
    "    beat_times = audio_structure_features['beat_times']\n",
    "    evecs = audio_structure_features['evecs']\n",
    "    segment_labels = laplacian_segmentation(\n",
    "        evecs=evecs,\n",
    "        # TODO: publish these parameters to the user\n",
    "        n_clusters = n_clusters, # be sure to explain this is an upper bound, stochastic\n",
    "        n_spectral_features = n_clusters,\n",
    "    )\n",
    "\n",
    "    #TODO: persist beat-cluster mappings to audio_strucutre_features and/or storyboard\n",
    "\n",
    "#save_storyboard(storyboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccedf46f-bb51-4a47-82e4-f0f71e9b5000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Audioreactivity\n",
    "\n",
    "structural_features = audio_structure_features\n",
    "\n",
    "# @markdown ## 1. Choose a driving signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import keyframed as kf\n",
    "\n",
    "\n",
    "# TODO: turn this into a drop down \n",
    "\n",
    "driving_signal_name = \"default\" #@param ['default','user specified','vocals stem','bass stem','other stem','drum stem']\n",
    "\n",
    "\n",
    "# TODO: test this\n",
    "custom_signal_fpath = '' # @param {'type':'string'}\n",
    "\n",
    "\n",
    "#########################################\n",
    "\n",
    "stems_path = root / \"stems\"\n",
    "stems_outpath = stems_path / 'htdemucs_ft' / Path(storyboard.params.audio_fpath).stem\n",
    "    \n",
    "def ensure_stems_separated():    \n",
    "    if not stems_outpath.exists():    \n",
    "        !demucs -n htdemucs_ft -o {stems_path} \"{storyboard.params.audio_fpath}\"\n",
    "\n",
    "def get_stem(instrument_name):\n",
    "    ensure_stems_separated()\n",
    "    stem_fpaths  = list(stems_outpath.glob('*.wav'))\n",
    "\n",
    "    for stem_fpath in stem_fpaths:\n",
    "        if instrument_name in str(stem_fpath):\n",
    "            y, sr = librosa.load(stem_fpath)\n",
    "            return y, sr\n",
    "    raise ValueError(\n",
    "        f\"Unable to locate stem for instrument: {instrument_name}\\n\"\n",
    "        f\"in folder: {stems_outpath}\"\n",
    "    )\n",
    "\n",
    "def get_user_specified_signal():\n",
    "    y, sr = librosa.load(custom_signal_fpath)\n",
    "    return y, sr\n",
    "\n",
    "y = structural_features['y']\n",
    "sr = structural_features['sr']\n",
    "\n",
    "driving_signals = {\n",
    "    'default': lambda: (y, sr),\n",
    "    'user specified': get_user_specified_signal,\n",
    "    'vocals stem':lambda: get_stem('vocals'),\n",
    "    'bass stem':lambda: get_stem('bass'),\n",
    "    'other stem':lambda: get_stem('other'),\n",
    "    'drum stem':lambda: get_stem('drum'),\n",
    "}\n",
    "\n",
    "\n",
    "driving_signal, sr = driving_signals[driving_signal_name]()\n",
    "\n",
    "\n",
    "display_signal(driving_signal, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99fc24f-139a-4508-bec4-331762d1afb5",
   "metadata": {},
   "source": [
    "### Audio EQ cheat sheet\n",
    "\n",
    "![Audio EQ cheat sheet](https://i.pinimg.com/736x/4f/28/5e/4f285e3fbc5b6b6ea78638e58b2e3052.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0272b5e-3810-4cf7-90ea-a5d4be709647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy import signal\n",
    "# #sig = np.repeat([0., 1., 0.], 100)\n",
    "# # sig = normalized_signal\n",
    "\n",
    "# # k=500\n",
    "# # win_smooth = signal.windows.hann(int(k/4))#[k:]\n",
    "# # win_decay = signal.windows.hann(2*k)\n",
    "# # win_decay[:k]=0\n",
    "\n",
    "# # #filtered = signal.convolve(sig, win, mode='same') / sum(win)\n",
    "# # filtered = signal.convolve(sig, win_decay, mode='same') / sum(win_decay)\n",
    "# # filtered = signal.convolve(filtered, win_smooth, mode='same') / sum(win_smooth)\n",
    "# # filtered = signal.convolve(filtered, win_smooth, mode='same') / sum(win_smooth)\n",
    "# # filtered = signal.convolve(filtered, win_smooth, mode='same') / sum(win_smooth)\n",
    "# # filtered = signal.convolve(filtered, win_decay, mode='same') / sum(win_decay)\n",
    "\n",
    "# # scipy.signal.find_peaks\n",
    "\n",
    "# def smooth(y, k=150):\n",
    "#     win_smooth = signal.windows.hann(k)\n",
    "#     filtered = signal.convolve(y, win_smooth, mode='same') / sum(win_smooth)\n",
    "#     return filtered\n",
    "\n",
    "# def decay(y, k=500):\n",
    "#     win_decay = signal.windows.hann(2*k)\n",
    "#     win_decay[:k]=0\n",
    "#     filtered = signal.convolve(y, win_decay, mode='same') / sum(win_decay)\n",
    "#     return filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e4027-1a8d-4583-b4c6-caf04c87cb97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from inspect import signature\n",
    "from functools import partial\n",
    "\n",
    "#dir(signature(get_stem))\n",
    "#signature(bandpass).parameters\n",
    "\n",
    "#         filter_params = parse.parse(\"bandpass({low},{high})\", manipulation)\n",
    "#         low, high = float(filter_params['low']), float(filter_params['high'])\n",
    "#         f = lambda y, sr: butter_bandpass_filter(y, sr, low, high)\n",
    "\n",
    "\n",
    "\n",
    "signature(bandpass).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d4da3-a58b-4ecc-bbe0-8dd697d4690f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bandpass.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06d9ad-264a-45b2-86d8-3adcd26b7993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[p for p in signature(bandpass).parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7412a-b978-495a-81bd-706f938102c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15180aef-06fc-4dfe-90b6-81fe43d7f9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sig = signature(bandpass)\n",
    "n_args = len(sig.parameters)\n",
    "args = {k:v for k,v in zip(sig.parameters, (300,500))}\n",
    "#sig.bind(args)\n",
    "bandpass(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd061db1-c218-4fb0-b6d4-82fa30e702b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = parse.parse(\"bandpass({low},{high})\",\"bandpass(300,500)\")\n",
    "#dir(result)\n",
    "result.named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61419d49-b0ad-4aa4-a027-21b01d534b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 2. Manipulate the signal\n",
    "\n",
    "# @markdown To apply multiple operations, separate manipulation names with a '|' or just re-run this cell.\n",
    "# @markdown When you're satisfied, run the next cell to replace the loaded driving signal with the manipulated signal.\n",
    "\n",
    "# @markdown #### Available Signal Manipulations\n",
    "# @markdown * `raw` - no op\n",
    "# @markdown * `rms` - Root mean squared. Converts raw signal to signal power\n",
    "# @markdown * `novelty` - Estimates strength of sound event onsets.\n",
    "# @markdown * `predominant_pulse` - Combined estimate of beat timing and strength\n",
    "\n",
    "# @markdown * `pow2` - Square the signal. Increases gap between high and low amplitude signals\n",
    "# @markdown * `stretch` - Alias for `pow2`\n",
    "\n",
    "# @markdown * `sqrt` - Square root of signal. Reduces gap between high and low amplitude signals\n",
    "# @markdown * `smoosh` - Alias for `sqrt`\n",
    "\n",
    "# @markdown * `normalize` - Transform signal to `[0,1]` range. This will always be the last step, even if you don't specify it.\n",
    "# @markdown * `smooth(k)` - Smoothes out the waveworm, using a smoothing window of `k`. Bigger `k` = flatter signal\n",
    "# @markdown * `decay(k)` - Treats signal as a self-exciting process that decays over a window of `k`. \n",
    "\n",
    "# @markdown * `bandpass(low, high)` - Isolate signal to frequencies between `[low, high]`\n",
    "# @markdown * `threshold(low)` - Zero the signal where amplitude is less than `low`\n",
    "# @markdown * `clamp(high)` - Clamp the signal such that no values have amplitude greater than `high`\n",
    "\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "\n",
    "# TODO: specify which transforms are available in spectral space vs. not\n",
    "\n",
    "\n",
    "# trim/trim(q) -> y[y>quantile(y,.1)]\n",
    "# round -> quantize\n",
    "# that 4part way of parameterizing a wave... hit, sustain, decay,..?\n",
    "\n",
    "\n",
    "\n",
    "# all operations must either have signature (y, sr) or return a function which does\n",
    "\n",
    "def rms(y, sr):\n",
    "    return librosa.feature.rms(y=y)\n",
    "\n",
    "def novelty(y, sr):\n",
    "    return librosa.onset.onset_strength(y, sr)\n",
    "\n",
    "def predominant_pulse(y, sr):\n",
    "    return librosa.beat.plp(y, sr)\n",
    "\n",
    "def pow2(y, sr):\n",
    "    return y**2\n",
    "\n",
    "def sqrt(y, sr):\n",
    "    return y**-2\n",
    "\n",
    "\n",
    "# so... apparently `pow` is a python builtin. whoops. Meh, fuck it.\n",
    "def _pow(k):\n",
    "    def pow_(y, sr):\n",
    "        return y**k\n",
    "    return pow_\n",
    "\n",
    "def stretch(k=2):\n",
    "    return _pow(k)\n",
    "\n",
    "def smoosh(k=2):\n",
    "    return _pow(-k)\n",
    "\n",
    "def normalize(y, sr):\n",
    "    normalized_signal = np.abs(y).ravel()\n",
    "    normalized_signal /= max(normalized_signal)\n",
    "    return normalized_signal\n",
    "    \n",
    "######################################\n",
    "    \n",
    "def smooth(k=150):\n",
    "    k=int(k)\n",
    "    def smooth_(y, sr=None):\n",
    "        win_smooth = signal.windows.hann(k)\n",
    "        filtered = signal.convolve(y, win_smooth, mode='same') / sum(win_smooth)\n",
    "        return filtered\n",
    "    return smooth_\n",
    "\n",
    "def decay(k=500):\n",
    "    k=int(k)\n",
    "    def decay_(y, sr=None):\n",
    "        win_decay = signal.windows.hann(2*k)\n",
    "        win_decay[:k]=0\n",
    "        filtered = signal.convolve(y.ravel(), win_decay, mode='same') / sum(win_decay)\n",
    "        return filtered\n",
    "    return decay_\n",
    "\n",
    "\n",
    "#####################333\n",
    "    \n",
    "def bandpass(low: float, high:float):\n",
    "    # low = float(low)\n",
    "    # high=float(high)\n",
    "    return partial(butter_bandpass_filter, lowcut=low, highcut=high)\n",
    "    \n",
    "def threshold(low):\n",
    "    def f(y, sr):\n",
    "        y[y<low] = 0\n",
    "        return y\n",
    "    return f\n",
    "\n",
    "def clamp(high):\n",
    "    def f(y, sr):\n",
    "        y[y>high] = high\n",
    "        return y\n",
    "    return f\n",
    "    \n",
    "def just_harmonic(y, sr):\n",
    "    harmonic, percussive = librosa.effects.hpss(y)\n",
    "    return harmonic\n",
    "\n",
    "def just_percussive(y, sr):\n",
    "    harmonic, percussive = librosa.effects.hpss(y)\n",
    "    return percussive \n",
    "\n",
    "\n",
    "###############################3\n",
    "\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# def peak_detection(y, sr):\n",
    "#     peaks, _ = find_peaks(y)\n",
    "#     return peaks\n",
    "\n",
    "def modulo(k=2):\n",
    "    k=int(k)\n",
    "    def modulo_(y, sr=None):\n",
    "        #peaks = peak_detection(y, sr)\n",
    "        peaks, _ = find_peaks(y)\n",
    "        selected_peaks = peaks[::k]  # Select every kth peak\n",
    "        new_signal = np.zeros_like(y)\n",
    "        new_signal[selected_peaks] = y[selected_peaks]  # Build a new signal with only the selected peaks\n",
    "        return new_signal\n",
    "    return modulo_\n",
    "\n",
    "#################################\n",
    "\n",
    "# chatgpt wrote this, needs to be tested. also, i might want to use medoids rather than means\n",
    "from sklearn.cluster import KMeans\n",
    "#sklearn_extra.cluster.KMedoids\n",
    "\n",
    "def quantize(k=1):\n",
    "    k=int(k)\n",
    "    # why doesn't it respect `k` in the closure scope? Works fine for modulo(). weird.\n",
    "    #def quantize_(y, sr=None):\n",
    "    def quantize_(y, sr=None, K=k):\n",
    "        k=K\n",
    "        # Remove zero values\n",
    "        nonzero_values = y[y > 0].reshape(-1, 1)\n",
    "        \n",
    "        # If the number of nonzero values is less than k, reduce k\n",
    "        if nonzero_values.shape[0] < k:\n",
    "            k = nonzero_values.shape[0]\n",
    "        \n",
    "        # Perform k-means clustering\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(nonzero_values)\n",
    "\n",
    "        # Replace each value with the centroid of its cluster\n",
    "        quantized_values = kmeans.cluster_centers_[kmeans.labels_].flatten()\n",
    "\n",
    "        # Create a new signal\n",
    "        quantized_signal = np.zeros_like(y)\n",
    "        quantized_signal[y > 0] = quantized_values\n",
    "        return quantized_signal\n",
    "    return quantize_\n",
    "\n",
    "#####################################3\n",
    "\n",
    "\n",
    "# # TODO: add operations: threshold(val), clamp(val), quantize(n)\n",
    "simple_signal_operations = {\n",
    "    'raw': lambda y, _: y,\n",
    "    ##########\n",
    "    'rms': lambda y, _: librosa.feature.rms(y=y),\n",
    "    'novelty': librosa.onset.onset_strength,\n",
    "    'predominant_pulse': librosa.beat.plp,\n",
    "    'bandpass': bandpass,\n",
    "    'harmonic': lambda y, _: librosa.effects.harmonic(y=y),\n",
    "    'percussive': lambda y, _: librosa.effects.percussive(y=y),\n",
    "    ##########\n",
    "    'pow2': lambda y, _: y**2,\n",
    "    'stretch': stretch,\n",
    "    'sqrt': sqrt,\n",
    "    'smoosh': smoosh,\n",
    "    'pow':_pow,\n",
    "    #################\n",
    "    'smooth': smooth,\n",
    "    'decay': decay,\n",
    "    # #########\n",
    "    'normalize': normalize,\n",
    "    'abs': lambda y, _ : np.abs(np.abs(y)), \n",
    "    'threshold': threshold,\n",
    "    'clamp': clamp,\n",
    "    'modulo': modulo,\n",
    "    'quantize':quantize,\n",
    "}\n",
    "\n",
    "def prepare_operation(op_str, operations=simple_signal_operations):\n",
    "    op_str = op_str.replace(' ','') # make sure there are no spaces separating arguments\n",
    "    for op_name, op in operations.items():\n",
    "        if op_name == op_str:\n",
    "            return op\n",
    "        \n",
    "    # if we're here, that should mean we have arguments to parse.\n",
    "    for op_name, op in operations.items():\n",
    "        if op_str.startswith(f\"{op_name}(\"):\n",
    "            break\n",
    "    else: # if we're here, it means we never broke out of the `for` loop, i.e. couldn't find a matching op\n",
    "        raise ValueError(f\"{op_str} is not a supported operation. Supported operations: {[op_name for op in operations]}\")\n",
    "    \n",
    "    arg_names = [p for p in signature(op).parameters]\n",
    "    bracketed = [\"{\" + p + \"}\" for p in arg_names]\n",
    "    template = f\"{op_name}({','.join(bracketed)})\"\n",
    "    result = parse.parse(template, op_str)\n",
    "    kargs = result.named\n",
    "    kargs = {k:float(v) for k,v in kargs.items()} # coerce strings to floats\n",
    "    return op(**kargs)\n",
    "    \n",
    "\n",
    "# signal_manipulations = 'bandpass(300, 500) | pow2 | pow2 | rms | pow2 | threshold(5e-6) ' # @param {'type':'string'}\n",
    "signal_manipulations = 'percussive | bandpass(300, 500) | stretch | stretch | rms | pow2 | normalize | clamp(0.25) |  threshold(0.1) | modulo | quantize(2) | decay(80) '\n",
    "\n",
    "manipulations = [m.strip() for m in signal_manipulations.split('|') if m.strip()]\n",
    "\n",
    "# reset processing\n",
    "driving_signal_massaged = driving_signal\n",
    "\n",
    "\n",
    "show_spec=True\n",
    "for idx, manipulation in enumerate(manipulations):\n",
    "\n",
    "    f = prepare_operation(manipulation)\n",
    "    try:\n",
    "        driving_signal_massaged = f(driving_signal_massaged, sr)\n",
    "    except TypeError as e: # this is sort of gross but fuck it\n",
    "        print(e)\n",
    "        f=f()\n",
    "        driving_signal_massaged = f(driving_signal_massaged, sr)\n",
    "    \n",
    "    # visualize\n",
    "    if manipulation in ('rms'):\n",
    "        show_spec=False\n",
    "    display_signal(driving_signal_massaged, sr, title=f\"{idx}, {manipulation}\", show_spec=show_spec)\n",
    "else:\n",
    "    # finalize signal\n",
    "    driving_signal_massaged = np.abs(driving_signal_massaged).ravel()\n",
    "    driving_signal_massaged /= max(driving_signal_massaged)\n",
    "    display_signal(driving_signal_massaged, sr, title=f\"Final Driving Signal\", show_spec=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc71eaf-8400-4a7a-9faa-fe9cf3add9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_signal(smooth(100)(y=driving_signal_massaged, sr=sr), sr, show_spec=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e62a1-229f-4e9e-b50c-783e1318911d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M = 100\n",
    "s = 4.0\n",
    "w = 0.5\n",
    "wavelet = signal.morlet(M, s, w)\n",
    "filtered = signal.convolve(driving_signal_massaged, wavelet, mode='same') / sum(wavelet)\n",
    "display_signal(np.abs(filtered).ravel(), sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7026a0e-125d-4fe9-a6f3-a35e90a222bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 2b. Run this cell when you're satisfied with the transformed signal\n",
    "\n",
    "use_massaged_signal = True # @param {'type':'boolean'}\n",
    "if use_massaged_signal:\n",
    "    driving_signal = driving_signal_massaged\n",
    "\n",
    "\n",
    "### Uncomment this snippet to write your driving signal to an audio file\n",
    "#import soundfile\n",
    "#soundfile.write('driving_signal.wav', driving_signal, sr)\n",
    "\n",
    "normalized_signal = np.abs(driving_signal).ravel()\n",
    "normalized_signal /= max(normalized_signal)\n",
    "\n",
    "frame_time = librosa.frames_to_time(np.arange(len(normalized_signal)), sr=sr)\n",
    "\n",
    "driving_signal_kf = kf.Curve({t:v for t,v in zip(frame_time, normalized_signal)})\n",
    "\n",
    "plt.plot(frame_time, normalized_signal)\n",
    "plt.xlabel('seconds')\n",
    "plt.ylabel('intensity')\n",
    "plt.title(\"Processed Audioreactivity Signal\")\n",
    "full_width_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d495092-db2d-4184-b5e2-48046f505c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(normalized_signal[normalized_signal>0], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0366a-3160-484b-8c52-918f0691b5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7cdae-9794-4b0f-bc3f-d826615db51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "#sig = np.repeat([0., 1., 0.], 100)\n",
    "sig = normalized_signal\n",
    "\n",
    "k=500\n",
    "win_smooth = signal.windows.hann(int(k/4))#[k:]\n",
    "win_decay = signal.windows.hann(2*k)\n",
    "win_decay[:k]=0\n",
    "\n",
    "#filtered = signal.convolve(sig, win, mode='same') / sum(win)\n",
    "filtered = signal.convolve(sig, win_decay, mode='same') / sum(win_decay)\n",
    "filtered = signal.convolve(filtered, win_smooth, mode='same') / sum(win_smooth)\n",
    "filtered = signal.convolve(filtered, win_smooth, mode='same') / sum(win_smooth)\n",
    "filtered = signal.convolve(filtered, win_smooth, mode='same') / sum(win_smooth)\n",
    "filtered = signal.convolve(filtered, win_decay, mode='same') / sum(win_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b566923-ebd9-4994-a16f-8ce2f8e67d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#fig, (ax_orig, ax_win, ax_filt) = plt.subplots(3, 1, sharex=True)\n",
    "fig, (ax_orig,  ax_filt) = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "ax_orig.plot(sig)\n",
    "ax_orig.set_title('Original pulse')\n",
    "ax_orig.margins(0, 0.1)\n",
    "# ax_win.plot(win)\n",
    "# ax_win.set_title('Filter impulse response')\n",
    "# ax_win.margins(0, 0.1)\n",
    "ax_filt.plot(filtered)\n",
    "ax_filt.set_title('Filtered signal')\n",
    "ax_filt.margins(0, 0.1)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ddece-c76f-4adf-9287-3791dd13d45b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "driving_signal_kf.plot()\n",
    "full_width_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8882a-de26-4ad7-9ec0-6bdb89cf99e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from keyframed import Curve\n",
    "from keyframed.interpolation import get_context_left, get_context_right, register_interpolation_method\n",
    "\n",
    "def user_defined_quadratic_interp(k, curve, n=2):\n",
    "    xs = get_context_left(k, curve, n)\n",
    "    xs += get_context_right(k, curve, n)\n",
    "    ys = [curve[x] for x in xs]\n",
    "    f = interp1d(xs, ys, kind='quadratic')\n",
    "    return f(k)\n",
    "\n",
    "register_interpolation_method('quadratic', user_defined_quadratic_interp)\n",
    "\n",
    "# sample some random points that are linearly correlated with some jitter\n",
    "#d = {i:i+2*random.random() for i in range(10)}\n",
    "#curve = Curve(d, default_interpolation=user_defined_quadratic_interp)\n",
    "\n",
    "#xs = np.linspace(0,9,100)\n",
    "#curve.plot(xs=xs)\n",
    "\n",
    "def curve_from_signal(\n",
    "    y,\n",
    "    name = 'strength',\n",
    "    hi= 1,\n",
    "    low = 0,\n",
    "    inverse_relationship = False,\n",
    "    sr=22050,\n",
    "):\n",
    "    # normalize signal\n",
    "    y = np.abs(y).ravel()\n",
    "    y /= max(y)\n",
    "    if inverse_relationship:\n",
    "        y = 1 - y\n",
    "    y = y*(hi-low) + low\n",
    "    \n",
    "    frame_time = librosa.frames_to_time(np.arange(len(y)), sr=sr)\n",
    "    return Curve({t:v for t,v in zip(frame_time, y)}, label=name) #, default_interpolation='quadratic') # interp method makes basically zero difference given high SR\n",
    "\n",
    "curve = curve_from_signal(driving_signal)\n",
    "curve[0.25:2.25].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d8b19-df96-44ab-a39e-f77d1d476f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "curve[0.25:2.25].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f9c13-d7f6-4490-a0ce-3e56e376911a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to do:\n",
    "# * thresholding\n",
    "# * exp decay \n",
    "# * smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e202526-e49c-4310-8088-2f902285ad17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 3a. Map the driving signal to parameter ranges \n",
    "\n",
    "#TODO: Build these curves/signals agnostic of VKTRS's scene abstraction\n",
    "\n",
    "signals =[\n",
    "    dict(\n",
    "        signal_name = 'noise_reactive',\n",
    "        attr_hi= .08,\n",
    "        attr_low = .02,\n",
    "        inverse_relationship = False,\n",
    "    ),\n",
    "    dict(\n",
    "        attr_hi=.7,\n",
    "        attr_low = .3,\n",
    "        inverse_relationship = True,\n",
    "        signal_name =  'audio_reactive_curve',\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# 3b. Map the driving signal to parameter ranges\n",
    "reactive_signal_map = {\n",
    "    'audio_reactive_curve':'strength_curve',\n",
    "    'noise_reactive':'noise_add_curve',\n",
    "}\n",
    "\n",
    "# TODO: make the above part more user friendly\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "##################################################################################################################\n",
    "\n",
    "# Hi user. You've gone to far. Don't worry about this stuff. Just the bit above the line.\n",
    "\n",
    "for signal_params in signals:\n",
    "\n",
    "    attr_hi= signal_params['attr_hi']\n",
    "    attr_low = signal_params['attr_low']\n",
    "    inverse_relationship = signal_params['inverse_relationship']\n",
    "    signal_name = signal_params['signal_name']\n",
    "\n",
    "    # TODO: instead of attaching the string, let's just build a curve object and use\n",
    "    #       keyframed.Curve's native serialization. this was the whole point of doing it that way.\n",
    "\n",
    "    #print(signal_params)\n",
    "\n",
    "    \n",
    "#     for scene_idx, rec in enumerate(storyboard.prompt_starts):\n",
    "#         #print(scene_idx)\n",
    "#         if scene_idx == 0:\n",
    "#             prev_rec = rec\n",
    "#             continue\n",
    "#         start, end = prev_rec['start'], rec['start']\n",
    "#         if prev_rec['frames'] < 1:\n",
    "#             continue\n",
    "            \n",
    "#         curve_chunks = []        \n",
    "#         for frame_idx in range(prev_rec['frames']):\n",
    "#             curr_time = start + frame_idx * ifps\n",
    "#             signal_value = driving_signal_kf[curr_time]\n",
    "#             if inverse_relationship:\n",
    "#                 signal_value = 1-signal_value\n",
    "#             attr_value = signal_value*(attr_hi-attr_low)+attr_low\n",
    "#             curve_chunks.append(f\"{frame_idx}:({attr_value})\")\n",
    "#         curve_str = ','.join(curve_chunks)\n",
    "#         prev_rec[signal_name] = curve_str\n",
    "#         prev_rec = rec\n",
    "#     else: # hate this.\n",
    "#         start, end = rec['start'], rec['end']\n",
    "#         if rec['frames'] > 0:\n",
    "#             curve_chunks = []\n",
    "#             for frame_idx in range(rec['frames']):\n",
    "#                 curr_time = start + frame_idx * ifps\n",
    "#                 signal_value = driving_signal_kf[curr_time]\n",
    "#                 if inverse_relationship:\n",
    "#                     signal_value = 1-signal_value\n",
    "#                 attr_value = signal_value*(attr_hi-attr_low)+attr_low\n",
    "#                 curve_chunks.append(f\"{frame_idx}:({attr_value})\")\n",
    "#             curve_str = ','.join(curve_chunks)\n",
    "#             rec[signal_name] = curve_str\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "# TODO: save keyframed objects for signals\n",
    "\n",
    "if not storyboard.get('audioreactive'):\n",
    "    storyboard.audioreactive = {'signals': [], 'reactive_signal_map': {}}\n",
    "\n",
    "# TODO: check if these signals are already in the board\n",
    "#       ...actually, will probably be simpler to just reshape this so each signal is a dict whose key is the name\n",
    "storyboard.audioreactive.signals.append(signals) \n",
    "\n",
    "storyboard.audioreactive.reactive_signal_map.update(reactive_signal_map)\n",
    "save_storyboard(storyboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8f4e4-ab81-402f-af86-307a6c02559e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def kf_to_cn_string(curve):\n",
    "    return ','.join(\n",
    "        [f\"{kf['t']}:({kf['value']})\" \n",
    "         for kf in curve.to_dict()['curve'].values()\n",
    "        ])\n",
    "\n",
    "kf_to_cn_string(driving_signal_kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda1934-6abe-4036-9c3a-60ea67e67987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
