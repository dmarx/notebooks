{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15BNHICpOOXg"
      },
      "source": [
        "# Stable Diffusion KLMC2 Animation\n",
        "\n",
        "<div>\n",
        "<img src=\"https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/a432c21c-bb12-4f38-b5e2-1c12a3c403f6/Animated-Logo_1.gif\" width=\"150\"/>\n",
        "</div>\n",
        "\n",
        "\n",
        "Notebook by [Katherine Crowson](https://twitter.com/RiversHaveWings)\n",
        "\n",
        "Sponsored by [StabilityAI](https://twitter.com/stabilityai)\n",
        "\n",
        "Generate animations with [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release) 1.4, using the [KLMC2 discretization of underdamped Langevin dynamics](https://arxiv.org/abs/1807.09382). The notebook is largely inspired by [Ajay Jain](https://twitter.com/ajayj_) and [Ben Poole](https://twitter.com/poolio)'s paper [Journey to the BAOAB-limit](https://www.ajayjain.net/journey)&mdash;thank you so much for it!\n",
        "\n",
        "---\n",
        "\n",
        "## Modifications Provenance\n",
        "\n",
        "Original notebook URL - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1m8ovBpO2QilE2o4O-p2PONSwqGn4_x2G)\n",
        "\n",
        "Features and QOL Modifications by [David Marx](https://twitter.com/DigThatData) - [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dmarx/notebooks/blob/main/Stable_Diffusion_KLMC2_Animation.ipynb)\n",
        "\n",
        "* Keyframed prompts and settings\n",
        "* Multiprompt conditioning w independent prompt schedules\n",
        "* Set seed for deterministic output\n",
        "* Mount Google Drive\n",
        "* Faster Setup\n",
        "* Set output filename\n",
        "* Fancy GPU info\n",
        "* Video embed optional\n",
        "* Cheaper default runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ty3IOeXbLzvc"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU\n",
        "#!nvidia-smi\n",
        "\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "\n",
        "def gpu_info():\n",
        "    outv = subprocess.run([\n",
        "        'nvidia-smi',\n",
        "            # these lines concatenate into a single query string\n",
        "            '--query-gpu='\n",
        "            'timestamp,'\n",
        "            'name,'\n",
        "            'utilization.gpu,'\n",
        "            'utilization.memory,'\n",
        "            'memory.used,'\n",
        "            'memory.free,'\n",
        "            ,\n",
        "        '--format=csv'\n",
        "        ],\n",
        "        stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "    header, rec = outv.split('\\n')[:-1]\n",
        "    return pd.DataFrame({' '.join(k.strip().split('.')).capitalize():v for k,v in zip(header.split(','), rec.split(','))}, index=[0]).T\n",
        "\n",
        "gpu_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kelHR9VM1-hg"
      },
      "outputs": [],
      "source": [
        "#@title Install Dependencies\n",
        "\n",
        "# @markdown Your runtime will automatically restart after running this cell.\n",
        "# @markdown You should only need to run this cell when setting up a new runtime. After future runtime restarts,\n",
        "# @markdown you should be able to skip this cell.\n",
        "\n",
        "!pip install ftfy einops braceexpand requests transformers clip open_clip_torch omegaconf pytorch-lightning kornia k-diffusion ninja\n",
        "#!pip install -U torch torchvision\n",
        "!pip install -U git+https://github.com/huggingface/huggingface_hub\n",
        "!pip install napm keyframed\n",
        "\n",
        "# for deforum loading code\n",
        "!pip install omegaconf\n",
        "\n",
        "#####################\n",
        "# Install more Deps #\n",
        "#####################\n",
        "\n",
        "#!git clone https://github.com/Stability-AI/stablediffusion\n",
        "#!git clone https://github.com/CompVis/stable-diffusion\n",
        "#!git clone https://github.com/CompVis/taming-transformers\n",
        "#!git clone https://github.com/CompVis/latent-diffusion\n",
        "\n",
        "# !pip install -v -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers\n",
        "\n",
        "exit() # oh is this a way to restart the runtime? clever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJZtXShcPXx5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown # Setup Workspace { display-mode: \"form\" }\n",
        "\n",
        "###################\n",
        "# Setup Workspace #\n",
        "###################\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "mount_gdrive = True # @param {type:'boolean'}\n",
        "\n",
        "# defaults\n",
        "outdir = Path('./frames')\n",
        "if not os.environ.get('XDG_CACHE_HOME'):\n",
        "    os.environ['XDG_CACHE_HOME'] = str(Path('~/.cache').expanduser())\n",
        "\n",
        "if mount_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    Path('/content/drive/MyDrive/AI/models/.cache/').mkdir(parents=True, exist_ok=True) \n",
        "    os.environ['XDG_CACHE_HOME']='/content/drive/MyDrive/AI/models/.cache'\n",
        "    outdir = Path('/content/drive/MyDrive/AI/klmc2/frames/')\n",
        "\n",
        "# make sure the paths we need exist\n",
        "outdir.mkdir(parents=True, exist_ok=True)\n",
        "os.environ['NAPM_PATH'] = str( Path(os.environ['XDG_CACHE_HOME']) / 'napm' )\n",
        "Path(os.environ['NAPM_PATH']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "import napm\n",
        "\n",
        "url = 'https://github.com/Stability-AI/stablediffusion'\n",
        "napm.pseudoinstall_git_repo(url, add_install_dir_to_path=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2jXKIf2ZkT8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown # Imports and Definitions { display-mode: \"form\" }\n",
        "\n",
        "###########\n",
        "# imports #\n",
        "###########\n",
        "\n",
        "import napm\n",
        "\n",
        "from base64 import b64encode\n",
        "from collections import defaultdict\n",
        "from concurrent import futures\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import functorch\n",
        "from IPython import display\n",
        "import k_diffusion as K\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "#sys.path.extend(['./stablediffusion'])\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "from requests.exceptions import HTTPError\n",
        "import huggingface_hub\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "from keyframed import Curve, ParameterGroup, Keyframe\n",
        "import math\n",
        "\n",
        "\n",
        "#########################\n",
        "# Define useful globals #\n",
        "#########################\n",
        "\n",
        "cpu = torch.device(\"cpu\")\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "############################\n",
        "\n",
        "model_dir_str=str(Path(os.environ['XDG_CACHE_HOME']))\n",
        "\n",
        "sdmodelid2hfrepo = {\n",
        "    \"sd-v1-4\":\"CompVis/stable-diffusion-v-1-4-original\",\n",
        "    \"sd-v1-5\":\"runwayml/stable-diffusion-v1-5\",\n",
        "}\n",
        "\n",
        "sdmodelid2hfckpt = {\n",
        "    \"sd-v1-4\":\"sd-v1-4.ckpt\",\n",
        "    \"sd-v1-5\":\"v1-5-pruned-emaonly.ckpt\",\n",
        "}\n",
        "\n",
        "sdmodelid2yamlurl = {\n",
        "    \"sd-v1-4\":\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml\",\n",
        "    \"sd-v1-5\":\"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml\",\n",
        "}\n",
        "\n",
        "sdmodelid2ckptstyle = {\n",
        "    \"sd-v1-4\":\"compvis\",\n",
        "    \"sd-v1-5\":\"compvis\",\n",
        "}\n",
        "\n",
        "############################\n",
        "\n",
        "vaemodelid2hfrepo = {\n",
        "    \"vae-ft-mse-840k\":\"stabilityai/sd-vae-ft-mse-original\",\n",
        "    \"vae-ft-ema-560k\":\"stabilityai/sd-vae-ft-ema-original\",\n",
        "    #\"vae-orig\":,\n",
        "}\n",
        "\n",
        "vaemodelid2hfckpt = {\n",
        "    \"vae-ft-mse-840k\":\"vae-ft-mse-840000-ema-pruned.ckpt\",\n",
        "    \"vae-ft-ema-560k\":\"vae-ft-ema-560000-ema-pruned.ckpt\",\n",
        "    #\"vae-orig\":,\n",
        "}\n",
        "\n",
        "vaemodelid2yamlurl = {\n",
        "    \"vae-ft-mse-840k\":\"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/models/first_stage_models/kl-f8/config.yaml\",\n",
        "    \"vae-ft-ema-560k\":\"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/models/first_stage_models/kl-f8/config.yaml\",\n",
        "    #\"vae-orig\":\"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/models/first_stage_models/kl-f8/config.yaml\",\n",
        "}\n",
        "\n",
        "\n",
        "##############################\n",
        "# Define necessary functions #\n",
        "##############################\n",
        "\n",
        "class Prompt:\n",
        "    def __init__(\n",
        "        self,\n",
        "        text,\n",
        "        weight_schedule,\n",
        "        ease_in=None,\n",
        "        ease_out=None,\n",
        "        ):\n",
        "      c = sd_model.get_learned_conditioning([text])\n",
        "      self.text=text\n",
        "      self.encoded=c\n",
        "      self.weight=Curve(\n",
        "          weight_schedule, \n",
        "          default_interpolation='linear', \n",
        "          ease_in=ease_in, \n",
        "          ease_out=ease_out)\n",
        "\n",
        "##################\n",
        "\n",
        "class NormalizingCFGDenoiser(nn.Module):\n",
        "    def __init__(self, model, g):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "        self.g = g\n",
        "        self.eps_norms = defaultdict(lambda: (0, 0))\n",
        "\n",
        "    def mean_sq(self, x):\n",
        "        return x.pow(2).flatten(1).mean(1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_eps_norm(self, eps, sigma):\n",
        "        sigma = sigma[0].item()\n",
        "        eps_norm = self.mean_sq(eps).mean()\n",
        "        eps_norm_avg, count = self.eps_norms[sigma]\n",
        "        eps_norm_avg = eps_norm_avg * count / (count + 1) + eps_norm / (count + 1)\n",
        "        self.eps_norms[sigma] = (eps_norm_avg, count + 1)\n",
        "        return eps_norm_avg\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, g):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "\n",
        "        denoised = self.inner_model(x_in, sigma_in, cond=cond_in)\n",
        "        eps = K.sampling.to_d(x_in, sigma_in, denoised)\n",
        "        eps_uc, eps_c = eps.chunk(2)\n",
        "        eps_norm = self.update_eps_norm(eps, sigma).sqrt()\n",
        "        c = eps_c - eps_uc\n",
        "        cond_scale = g * eps_norm / self.mean_sq(c).sqrt()\n",
        "        eps_final = eps_uc + c * K.utils.append_dims(cond_scale, x.ndim)\n",
        "        return x - eps_final * K.utils.append_dims(sigma, eps.ndim)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_mcmc_klmc2(\n",
        "    model, x, \n",
        "    sigma_min, sigma, sigma_max, \n",
        "    n, \n",
        "    hvp_method='reverse', \n",
        "    callback=None, \n",
        "    disable=None, \n",
        "    prompts=None,\n",
        "    settings=None, # g, h, gamma, alpha, tau, prompt\n",
        "):\n",
        "    s_in = x.new_ones([x.shape[0]])\n",
        "    sigma = torch.tensor(sigma, device=x.device)\n",
        "    sigmas = K.sampling.get_sigmas_karras(6, sigma_min, sigma.item(), device=x.device)[:-1]\n",
        "\n",
        "    uc = sd_model.get_learned_conditioning([''])\n",
        "    extra_args = {'uncond': uc}\n",
        "    v = torch.randn_like(x) * sigma # ... I guess?\n",
        "\n",
        "    for i in trange(n, disable=disable):\n",
        "\n",
        "        h = settings[i]['h']\n",
        "        gamma = settings[i]['gamma']\n",
        "        alpha = settings[i]['alpha']\n",
        "        tau = settings[i]['tau']\n",
        "\n",
        "        h = torch.tensor(h, device=x.device)\n",
        "        gamma = torch.tensor(gamma, device=x.device)\n",
        "        alpha = torch.tensor(alpha, device=x.device)\n",
        "        tau = torch.tensor(tau, device=x.device)\n",
        "\n",
        "        # Model helper functions\n",
        "\n",
        "        def hvp_fn_forward_functorch(x, sigma, v, **extra_args):\n",
        "            def grad_fn(x, sigma):\n",
        "                denoised = model(x, sigma * s_in, **extra_args)\n",
        "                return (x - denoised) + alpha * x\n",
        "            jvp_fn = lambda v: functorch.jvp(grad_fn, (x, sigma), (v, torch.zeros_like(sigma)))\n",
        "            grad, jvp_out = functorch.vmap(jvp_fn)(v)\n",
        "            return grad[0], jvp_out\n",
        "\n",
        "        def hvp_fn_reverse(x, sigma, v, **extra_args):\n",
        "            def grad_fn(x, sigma):\n",
        "                denoised = model(x, sigma * s_in, **extra_args)\n",
        "                return (x - denoised) + alpha * x\n",
        "            vjps = []\n",
        "            with torch.enable_grad():\n",
        "                x_ = x.clone().requires_grad_()\n",
        "                grad = grad_fn(x_, sigma)\n",
        "                for k, item in enumerate(v):\n",
        "                    vjp_out = torch.autograd.grad(grad, x_, item, retain_graph=k < len(v) - 1)[0]\n",
        "                    vjps.append(vjp_out)\n",
        "            return grad, torch.stack(vjps)\n",
        "\n",
        "        def hvp_fn_zero(x, sigma, v, **extra_args):\n",
        "            def grad_fn(x, sigma):\n",
        "                denoised = model(x, sigma * s_in, **extra_args)\n",
        "                return (x - denoised) + alpha * x\n",
        "            return grad_fn(x, sigma), torch.zeros_like(v)\n",
        "\n",
        "        def hvp_fn_fake(x, sigma, v, **extra_args):\n",
        "            def grad_fn(x, sigma):\n",
        "                denoised = model(x, sigma * s_in, **extra_args)\n",
        "                return (x - denoised) + alpha * x\n",
        "            return grad_fn(x, sigma), (1 + alpha) * v\n",
        "\n",
        "        hvp_fns = {'forward-functorch': hvp_fn_forward_functorch,\n",
        "                  'reverse': hvp_fn_reverse,\n",
        "                  'zero': hvp_fn_zero,\n",
        "                  'fake': hvp_fn_fake}\n",
        "\n",
        "        hvp_fn = hvp_fns[hvp_method]\n",
        "\n",
        "        # KLMC2 helper functions\n",
        "        def psi_0(gamma, t):\n",
        "            return torch.exp(-gamma * t)\n",
        "\n",
        "        def psi_1(gamma, t):\n",
        "            return -torch.expm1(-gamma * t) / gamma\n",
        "\n",
        "        def psi_2(gamma, t):\n",
        "            return (torch.expm1(-gamma * t) + gamma * t) / gamma ** 2\n",
        "\n",
        "        def phi_2(gamma, t_):\n",
        "            t = t_.double()\n",
        "            out = (torch.exp(-gamma * t) * (torch.expm1(gamma * t) - gamma * t)) / gamma ** 2\n",
        "            return out.to(t_)\n",
        "\n",
        "        def phi_3(gamma, t_):\n",
        "            t = t_.double()\n",
        "            out = (torch.exp(-gamma * t) * (2 + gamma * t + torch.exp(gamma * t) * (gamma * t - 2))) / gamma ** 3\n",
        "            return out.to(t_)\n",
        "\n",
        "\n",
        "        # Compute model outputs and sample noise\n",
        "        x_trapz = torch.linspace(0, h, 1001, device=x.device)\n",
        "        y_trapz = [fun(gamma, x_trapz) for fun in (psi_0, psi_1, phi_2, phi_3)]\n",
        "        noise_cov = torch.tensor([[torch.trapz(y_trapz[i] * y_trapz[j], x=x_trapz) for j in range(4)] for i in range(4)], device=x.device)\n",
        "        noise_v, noise_x, noise_v2, noise_x2 = torch.distributions.MultivariateNormal(x.new_zeros([4]), noise_cov).sample(x.shape).unbind(-1)\n",
        "            \n",
        "        extra_args['g']=g\n",
        "\n",
        "        # loop over prompts and aggregate gradients for multicond\n",
        "        grad = torch.zeros_like(x)\n",
        "        h2_v = torch.zeros_like(x)\n",
        "        h2_noise_v2 = torch.zeros_like(x)\n",
        "        h2_noise_x2 = torch.zeros_like(x)\n",
        "        wt_norm = 0\n",
        "        for prompt in prompts:\n",
        "            wt = prompt.weight[i]\n",
        "            if wt == 0:\n",
        "                continue\n",
        "            wt_norm += wt\n",
        "            wt = torch.tensor(wt, device=x.device)\n",
        "            extra_args['cond'] = prompt.encoded\n",
        "\n",
        "            # Estimate gradient and hessian\n",
        "            grad_, (h2_v_, h2_noise_v2_, h2_noise_x2_) = hvp_fn(\n",
        "                x, sigma, torch.stack([v, noise_v2, noise_x2]),\n",
        "                **extra_args\n",
        "            )\n",
        "\n",
        "            grad = grad + grad_ * wt \n",
        "            h2_v = h2_v + h2_v_ * wt\n",
        "            h2_noise_v2 = h2_noise_v2 + h2_noise_v2_ * wt\n",
        "            h2_noise_x2 = h2_noise_x2 + h2_noise_x2_ * wt\n",
        "\n",
        "        # Normalize gradient to magnitude it'd have if just single prompt w/ wt=1.\n",
        "        # simplifies multicond w/o deep frying image or adding hyperparams\n",
        "        grad = grad / wt_norm \n",
        "        h2_v = h2_v / wt_norm\n",
        "        h2_noise_v2 = h2_noise_v2 / wt_norm\n",
        "        h2_noise_x2 = h2_noise_x2 / wt_norm\n",
        "        \n",
        "\n",
        "        # DPM-Solver++(2M) refinement steps\n",
        "        x_refine = x\n",
        "        use_dpm = True\n",
        "        old_denoised = None\n",
        "        for j in range(len(sigmas) - 1):\n",
        "            if j == 0:\n",
        "                denoised = x_refine - grad\n",
        "            else:\n",
        "                denoised = model(x_refine, sigmas[j] * s_in, **extra_args)\n",
        "            dt_ode = sigmas[j + 1] - sigmas[j]\n",
        "            if not use_dpm or old_denoised is None or sigmas[j + 1] == 0:\n",
        "                eps = K.sampling.to_d(x_refine, sigmas[j], denoised)\n",
        "                x_refine = x_refine + eps * dt_ode\n",
        "            else:\n",
        "                h_ode = sigmas[j].log() - sigmas[j + 1].log()\n",
        "                h_last = sigmas[j - 1].log() - sigmas[j].log()\n",
        "                fac = h_ode / (2 * h_last)\n",
        "                denoised_d = (1 + fac) * denoised - fac * old_denoised\n",
        "                eps = K.sampling.to_d(x_refine, sigmas[j], denoised_d)\n",
        "                x_refine = x_refine + eps * dt_ode\n",
        "            old_denoised = denoised\n",
        "        if callback is not None:\n",
        "            callback({'i': i, 'denoised': x_refine})\n",
        "\n",
        "        # Update the chain\n",
        "        noise_std = (2 * gamma * tau * sigma ** 2).sqrt()\n",
        "        v_next = 0 + psi_0(gamma, h) * v - psi_1(gamma, h) * grad - phi_2(gamma, h) * h2_v + noise_std * (noise_v - h2_noise_v2)\n",
        "        x_next = x + psi_1(gamma, h) * v - psi_2(gamma, h) * grad - phi_3(gamma, h) * h2_v + noise_std * (noise_x - h2_noise_x2)\n",
        "        v, x = v_next, x_next\n",
        "\n",
        "    x = x - grad\n",
        "    return x\n",
        "\n",
        "\n",
        "def show_video(video_path, video_width=512):\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return display.HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown **Select and Load Model**\n",
        "\n",
        "# scavenged from:\n",
        "#   https://github.com/deforum/stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "import napm\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "models_path = \"/content/models\" #@param {type:\"string\"}\n",
        "if mount_gdrive:\n",
        "  models_path_gdrive = \"/content/drive/MyDrive/AI/models\" #@param {type:\"string\"}\n",
        "  models_path = models_path_gdrive\n",
        "\n",
        "model_config = \"v1-inference.yaml\" #@param [\"custom\",\"v1-inference.yaml\"]\n",
        "model_checkpoint =  \"sd-v1-4.ckpt\" #@param [\"custom\",\"sd-v1-4-full-ema.ckpt\",\"sd-v1-4.ckpt\",\"sd-v1-3-full-ema.ckpt\",\"sd-v1-3.ckpt\",\"sd-v1-2-full-ema.ckpt\",\"sd-v1-2.ckpt\",\"sd-v1-1-full-ema.ckpt\",\"sd-v1-1.ckpt\", \"robo-diffusion-v1.ckpt\",\"waifu-diffusion-v1-3.ckpt\"]\n",
        "if model_checkpoint == \"waifu-diffusion-v1-3.ckpt\":\n",
        "    model_checkpoint = \"model-epoch05-float16.ckpt\"\n",
        "custom_config_path = \"\" #@param {type:\"string\"}\n",
        "custom_checkpoint_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "load_on_run_all = True #@param {type: 'boolean'}\n",
        "half_precision = True # check\n",
        "check_sha256 = True #@param {type:\"boolean\"}\n",
        "\n",
        "model_map = {\n",
        "    \"sd-v1-4-full-ema.ckpt\": {\n",
        "        'sha256': '14749efc0ae8ef0329391ad4436feb781b402f4fece4883c7ad8d10556d8a36a',\n",
        "        'url': 'https://huggingface.co/CompVis/stable-diffusion-v-1-2-original/blob/main/sd-v1-4-full-ema.ckpt',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"sd-v1-4.ckpt\": {\n",
        "        'sha256': 'fe4efff1e174c627256e44ec2991ba279b3816e364b49f9be2abc0b3ff3f8556',\n",
        "        'url': 'https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"sd-v1-3-full-ema.ckpt\": {\n",
        "        'sha256': '54632c6e8a36eecae65e36cb0595fab314e1a1545a65209f24fde221a8d4b2ca',\n",
        "        'url': 'https://huggingface.co/CompVis/stable-diffusion-v-1-3-original/blob/main/sd-v1-3-full-ema.ckpt',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"sd-v1-3.ckpt\": {\n",
        "        'sha256': '2cff93af4dcc07c3e03110205988ff98481e86539c51a8098d4f2236e41f7f2f',\n",
        "        'url': 'https://huggingface.co/CompVis/stable-diffusion-v-1-3-original/resolve/main/sd-v1-3.ckpt',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"sd-v1-2-full-ema.ckpt\": {\n",
        "        'sha256': 'bc5086a904d7b9d13d2a7bccf38f089824755be7261c7399d92e555e1e9ac69a',\n",
        "        'url': 'https://huggingface.co/CompVis/stable-diffusion-v-1-2-original/blob/main/sd-v1-2-full-ema.ckpt',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"sd-v1-2.ckpt\": {\n",
        "        'sha256': '3b87d30facd5bafca1cbed71cfb86648aad75d1c264663c0cc78c7aea8daec0d',\n",
        "        'url': 'https://huggingface.co/CompVis/stable-diffusion-v-1-2-original/resolve/main/sd-v1-2.ckpt',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"sd-v1-1-full-ema.ckpt\": {\n",
        "        'sha256': 'efdeb5dc418a025d9a8cc0a8617e106c69044bc2925abecc8a254b2910d69829',\n",
        "        'url':'https://huggingface.co/CompVis/stable-diffusion-v-1-1-original/resolve/main/sd-v1-1-full-ema.ckpt',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"sd-v1-1.ckpt\": {\n",
        "        'sha256': '86cd1d3ccb044d7ba8db743d717c9bac603c4043508ad2571383f954390f3cea',\n",
        "        'url': 'https://huggingface.co/CompVis/stable-diffusion-v-1-1-original/resolve/main/sd-v1-1.ckpt',\n",
        "        'requires_login': True,\n",
        "        },\n",
        "    \"robo-diffusion-v1.ckpt\": {\n",
        "        'sha256': '244dbe0dcb55c761bde9c2ac0e9b46cc9705ebfe5f1f3a7cc46251573ea14e16',\n",
        "        'url': 'https://huggingface.co/nousr/robo-diffusion/resolve/main/models/robo-diffusion-v1.ckpt',\n",
        "        'requires_login': False,\n",
        "        },\n",
        "    \"model-epoch05-float16.ckpt\": {\n",
        "        'sha256': '26cf2a2e30095926bb9fd9de0c83f47adc0b442dbfdc3d667d43778e8b70bece',\n",
        "        'url': 'https://huggingface.co/hakurei/waifu-diffusion-v1-3/resolve/main/model-epoch05-float16.ckpt',\n",
        "        'requires_login': False,\n",
        "        },\n",
        "}\n",
        "\n",
        "# config path\n",
        "ckpt_config_path = custom_config_path if model_config == \"custom\" else os.path.join(models_path, model_config)\n",
        "if os.path.exists(ckpt_config_path):\n",
        "    print(f\"{ckpt_config_path} exists\")\n",
        "else:\n",
        "    #ckpt_config_path = \"./stable-diffusion/configs/stable-diffusion/v1-inference.yaml\"\n",
        "    ckpt_config_path = \"./v1-inference.yaml\"\n",
        "    if not Path(ckpt_config_path).exists():\n",
        "        !wget https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml\n",
        "    \n",
        "print(f\"Using config: {ckpt_config_path}\")\n",
        "\n",
        "# checkpoint path or download\n",
        "ckpt_path = custom_checkpoint_path if model_checkpoint == \"custom\" else os.path.join(models_path, model_checkpoint)\n",
        "ckpt_valid = True\n",
        "if os.path.exists(ckpt_path):\n",
        "    print(f\"{ckpt_path} exists\")\n",
        "elif 'url' in model_map[model_checkpoint]:\n",
        "    url = model_map[model_checkpoint]['url']\n",
        "\n",
        "    # CLI dialogue to authenticate download\n",
        "    if model_map[model_checkpoint]['requires_login']:\n",
        "        print(\"This model requires an authentication token\")\n",
        "        print(\"Please ensure you have accepted its terms of service before continuing.\")\n",
        "\n",
        "        username = input(\"What is your huggingface username?:\")\n",
        "        token = input(\"What is your huggingface token?:\")\n",
        "\n",
        "        _, path = url.split(\"https://\")\n",
        "\n",
        "        url = f\"https://{username}:{token}@{path}\"\n",
        "\n",
        "    # contact server for model\n",
        "    print(f\"Attempting to download {model_checkpoint}...this may take a while\")\n",
        "    ckpt_request = requests.get(url)\n",
        "    request_status = ckpt_request.status_code\n",
        "\n",
        "    # inform user of errors\n",
        "    if request_status == 403:\n",
        "      raise ConnectionRefusedError(\"You have not accepted the license for this model.\")\n",
        "    elif request_status == 404:\n",
        "      raise ConnectionError(\"Could not make contact with server\")\n",
        "    elif request_status != 200:\n",
        "      raise ConnectionError(f\"Some other error has ocurred - response code: {request_status}\")\n",
        "\n",
        "    # write to model path\n",
        "    with open(os.path.join(models_path, model_checkpoint), 'wb') as model_file:\n",
        "        model_file.write(ckpt_request.content)\n",
        "else:\n",
        "    print(f\"Please download model checkpoint and place in {os.path.join(models_path, model_checkpoint)}\")\n",
        "    ckpt_valid = False\n",
        "\n",
        "if check_sha256 and model_checkpoint != \"custom\" and ckpt_valid:\n",
        "    import hashlib\n",
        "    print(\"\\n...checking sha256\")\n",
        "    with open(ckpt_path, \"rb\") as f:\n",
        "        bytes = f.read() \n",
        "        hash = hashlib.sha256(bytes).hexdigest()\n",
        "        del bytes\n",
        "    if model_map[model_checkpoint][\"sha256\"] == hash:\n",
        "        print(\"hash is correct\\n\")\n",
        "    else:\n",
        "        print(\"hash in not correct\\n\")\n",
        "        ckpt_valid = False\n",
        "\n",
        "if ckpt_valid:\n",
        "    print(f\"Using ckpt: {ckpt_path}\")\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False, device='cuda', half_precision=True):\n",
        "    map_location = \"cuda\" #@param [\"cpu\", \"cuda\"]\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=map_location)\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    if half_precision:\n",
        "        model = model.half().to(device)\n",
        "    else:\n",
        "        model = model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "if load_on_run_all and ckpt_valid:\n",
        "    local_config = OmegaConf.load(f\"{ckpt_config_path}\")\n",
        "    model = load_model_from_config(local_config, f\"{ckpt_path}\", half_precision=half_precision)\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model = model.to(device)\n",
        "    sd_model=model"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yt3d1hww17ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZljSF1ePnBl4"
      },
      "outputs": [],
      "source": [
        "# @title Settings\n",
        "\n",
        "# @markdown The number of frames to sample:\n",
        "n = 500 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown If seed is negative, a random seed will be used\n",
        "seed = 3126649841  # @param {type:\"number\"}\n",
        "\n",
        "init_image = \"profile_arcanegan-pinkfloyd.png\" # @param {type:'string'}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown The strength of the conditioning on the prompt:\n",
        "g = 0.09 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown The noise level to sample at:\n",
        "sigma = 0.25 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown Step size (range 0 to 1):\n",
        "h = 0.1 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown Friction (2 is critically damped, lower -> smoother animation):\n",
        "gamma = 1.0 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown Quadratic penalty (\"weight decay\") strength:\n",
        "alpha = 0.005 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown Temperature (adjustment to the amount of noise added per step):\n",
        "tau = 1.0 # @param {type:\"number\"}\n",
        "\n",
        "# @markdown The HVP method:\n",
        "# @markdown <br><small>`forward-functorch` and `reverse` provide real second derivatives. Compatibility, speed, and memory usage vary by model and xformers configuration.\n",
        "# @markdown `fake` is very fast and low memory but inaccurate. `zero` (fallback to first order KLMC) is not recommended.</small>\n",
        "hvp_method = 'fake' # @param [\"forward-functorch\", \"reverse\", \"fake\", \"zero\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "if Path(init_image).exists():\n",
        "  init_im_pil = Image.open(init_image)\n"
      ],
      "metadata": {
        "id": "DZ9f3t0UdWJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#init_im_pil.size\n",
        "import numpy as np\n",
        "\n",
        "x_pil = init_im_pil.resize([512,512])\n",
        "x_np = np.array(x_pil.convert('RGB')).astype(np.float16) / 255.0\n",
        "\n",
        "#image = np.array(image).astype(np.float16) / 255.0\n",
        "x = x_np[None].transpose(0, 3, 1, 2)\n",
        "x = 2.*x - 1.\n",
        "x = torch.from_numpy(x).to('cuda')\n",
        "x = sd_model.get_first_stage_encoding(sd_model.encode_first_stage(x))"
      ],
      "metadata": {
        "id": "MQVQAlVVfk2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pLTsdGBPXx6"
      },
      "outputs": [],
      "source": [
        "#@title Prompts\n",
        "\n",
        "#  [  \n",
        "#    [\"first prompt will be used to initialize the image\", {time:weight, time:weight...}], \n",
        "#    [\"more prompts if you want\", {...}], \n",
        "#  ...]\n",
        "\n",
        "# if a weight for time=0 isn't specified, the weight is assumed to be zero.\n",
        "\n",
        "\n",
        "prompt_params = [\n",
        "    # FIRST PROMPT INITIALIZES IMAGE\n",
        "    #[\"portrait of queen elizabeth at 20 years old\", {0:1, 50:1, 100:0}],\n",
        "    #[\"portrait of queen elizabeth at 82 years old\", {50:0, 100:1}],\n",
        "    [\"a man with glasses standing in front of a pink sunset, inspired by Android Jones, league of legends, computer art, drachenlord, lucy in the sky with diamonds, profile picture, recolored, thumbnail, swirling energy, in style of south park, infini - d - render, three handed god, stoner beanie hipster, fiberpunk\", {0:1}]\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Build prompt and settings objects\n",
        "\n",
        "plot_prompt_weight_curves = True # @param {type: 'boolean'}\n",
        "\n",
        "#################\n",
        "\n",
        "def sin2(t):\n",
        "    return (math.sin(t * math.pi / 2)) ** 2\n",
        "\n",
        "prompts = [\n",
        "    Prompt(text, weight_schedule, ease_in=sin2, ease_out=sin2) \n",
        "    for (text, weight_schedule) in prompt_params\n",
        "]\n",
        "\n",
        "\n",
        "curved_settings = ParameterGroup({\n",
        "    #'g':Curve(g),\n",
        "    #'sigma':Curve(sigma),\n",
        "    'g':Curve({0:0.08,50:1.1}), # warm up cfg\n",
        "    'sigma':Curve({0:.25,50:1, 125:1, 200:2}), # warm up noise w init image\n",
        "    'h':Curve(h),\n",
        "    'gamma':Curve(gamma),\n",
        "    'alpha':Curve(alpha),\n",
        "    'tau':Curve(tau),\n",
        "    'seed':Curve(seed),\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "if plot_prompt_weight_curves:\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np \n",
        "\n",
        "\n",
        "    ytot=np.array([0 for _ in range(n)])\n",
        "    for prompt in prompts:#[:3]:\n",
        "      xs = np.array(range(n))\n",
        "      ys = np.array([prompt.weight[x] for x in xs])\n",
        "      ytot=ytot+ys\n",
        "      plt.plot(xs, ys)\n",
        "    plt.title(\"prompt weight schedules\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(xs, ytot)\n",
        "    plt.title(\"sum weight\\n(aka: why weights get normalized)\")\n",
        "    plt.show()\n",
        "\n",
        "    for prompt in prompts:#[:3]:\n",
        "      xs = np.array(range(n))\n",
        "      ys = np.array([prompt.weight[x] for x in xs])\n",
        "      plt.plot(xs, ys/ytot)\n",
        "    plt.title(\"normalized weights\\n(aka: why prompts might seem weighted differently than I asked)\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hZ0lh-WkdB19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-_u1Q0wRqMb"
      },
      "outputs": [],
      "source": [
        "#@title Generate Animation Frames\n",
        "\n",
        "\n",
        "###################\n",
        "\n",
        "import random\n",
        "\n",
        "# to do: if random seed, pick one for user and report chosen seed back\n",
        "if seed >= 0:\n",
        "    torch.manual_seed(seed)\n",
        "else:\n",
        "    seed = random.randrange(0, 4294967295)\n",
        "print(f\"using seed: {seed}\")\n",
        "\n",
        "wrappers = {'eps': K.external.CompVisDenoiser, 'v': K.external.CompVisVDenoiser}\n",
        "model_wrap = wrappers[sd_model.parameterization](sd_model)\n",
        "model_wrap_cfg = NormalizingCFGDenoiser(model_wrap, g)\n",
        "sigma_min, sigma_max = model_wrap.sigmas[0].item(), model_wrap.sigmas[-1].item()\n",
        "\n",
        "uc = sd_model.get_learned_conditioning([''])\n",
        "c = prompts[0].encoded\n",
        "extra_args = {'cond': c, 'uncond': uc}\n",
        "\n",
        "def save_image_fn(image, name, i):\n",
        "    pil_image = K.utils.to_pil_image(image)\n",
        "    if i % 10 == 0 or i == n - 1:\n",
        "        print(f'\\nIteration {i}/{n}:')\n",
        "        display.display(pil_image)\n",
        "    if i == n - 1:\n",
        "        print('\\nDone!')\n",
        "    name = outdir / name\n",
        "    pil_image.save(name)\n",
        "\n",
        "# to do: add archival\n",
        "# Clean up old images and video - save them elsewhere before running this if you want to keep them!\n",
        "#for p in Path('.').glob('out_*.png'):\n",
        "for p in outdir.glob('out_*.png'):\n",
        "    p.unlink()\n",
        "Path('out.mp4').unlink(missing_ok=True)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "with torch.cuda.amp.autocast(), futures.ThreadPoolExecutor() as ex:\n",
        "    def callback(info):\n",
        "        i = info['i']\n",
        "        #rgb = vae_model.decode(info['denoised'] / sd_model.scale_factor)\n",
        "        rgb = sd_model.decode_first_stage(info['denoised'] )\n",
        "        ex.submit(save_image_fn, rgb, f'out_{i:05}.png', i)\n",
        "\n",
        "    #x = torch.randn([1, 4, 64, 64], device=device) * sigma_max\n",
        "    \n",
        "    # Initialize the chain\n",
        "    print('Initializing the chain...')\n",
        "    sigmas_pre = K.sampling.get_sigmas_karras(15, sigma, sigma_max, device=x.device)[:-1]\n",
        "\n",
        "    extra_args['g'] = curved_settings[0]['g']\n",
        "    #x = K.sampling.sample_dpmpp_sde(model_wrap_cfg, x, sigmas_pre, extra_args=extra_args)\n",
        "\n",
        "    print('Actually doing the sampling...')\n",
        "    sample_mcmc_klmc2(\n",
        "        model=model_wrap_cfg,\n",
        "        x=x,\n",
        "        sigma_min=sigma_min,\n",
        "        sigma=sigma,\n",
        "        sigma_max=sigma_max,\n",
        "        n=n,\n",
        "        hvp_method=hvp_method,\n",
        "        callback=callback,\n",
        "        prompts=prompts,\n",
        "        settings=curved_settings,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DjwY7XrooLX_"
      },
      "outputs": [],
      "source": [
        "#@title Make the video\n",
        "\n",
        "outdir_str = str(outdir)\n",
        "\n",
        "fps = 20 # @param {type:\"integer\"}\n",
        "out_fname = \"out.mp4\" # @param {type: \"string\"}\n",
        "\n",
        "print('\\nMaking the video...\\n')\n",
        "!cd {outdir_str}; ffmpeg -y -r {fps} -i 'out_%*.png' -crf 15 -preset veryslow -pix_fmt yuv420p {out_fname}\n",
        "\n",
        "# @markdown If your video is larger than a few MB, attempting to embed it will probably crash\n",
        "# @markdown the session. If this happens, view the generated video after downloading it first.\n",
        "embed_video = True # @param {type:'boolean'}\n",
        "\n",
        "if embed_video:\n",
        "  print('\\nThe video:')\n",
        "  display.display(show_video(outdir / out_fname))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK_GlP_7WJiu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the MIT License { display-mode: \"form\" }\n",
        "\n",
        "# Copyright (c) 2022 Katherine Crowson <crowsonkb@gmail.com>\n",
        "# Copyright (c) 2023 David Marx <david.marx84@gmail.com>\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}