{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e851a-4b84-458d-b138-f7e56d917212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install einops matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978c6e77-3841-49af-8098-86569a48b0e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sliding_tiling 138ce95] more flexible grid construction\n",
      " 1 file changed, 13 insertions(+), 7 deletions(-)\n",
      "Enumerating objects: 5, done.\n",
      "Counting objects: 100% (5/5), done.\n",
      "Delta compression using up to 20 threads\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 520 bytes | 520.00 KiB/s, done.\n",
      "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
      "To github.com:dmarx/notebooks.git\n",
      "   993a0c6..138ce95  sliding_tiling -> sliding_tiling\n"
     ]
    }
   ],
   "source": [
    "!git commit -am \"note on latent prompt\"; git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f1729c-a227-4541-a288-a55df38510b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['STABILITY_KEY']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c3b5816-1031-49b1-b018-30da4a2610fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from stability_sdk import client\n",
    "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
    "\n",
    "import torch\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from einops import rearrange\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from itertools import chain\n",
    "\n",
    "def multiplex(latents: torch.Tensor, K: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Rearrange the latents tensor into a KxK grid using the einops rearrange function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    latents: torch.Tensor\n",
    "        The latent tensors to be rearranged.\n",
    "    K: int\n",
    "        The grid size for rearrangement.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The rearranged latents in a KxK grid.\n",
    "    \"\"\"\n",
    "    return rearrange(latents, '(b k1 k2) h w c -> b (k1 h) (k2 w) c', k1=K, k2=K)\n",
    "\n",
    "\n",
    "def demultiplex(images: torch.Tensor, K: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Rearrange the images tensor back from a KxK grid using the einops rearrange function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images: torch.Tensor\n",
    "        The image tensors to be rearranged.\n",
    "    K: int\n",
    "        The grid size for rearrangement.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The rearranged images from a KxK grid back to their original shape.\n",
    "    \"\"\"\n",
    "    return rearrange(images, ' (k1 h) (k2 w) c -> (k1 k2) h w c', k1=K, k2=K)\n",
    "\n",
    "\n",
    "\n",
    "# Set up our connection to the API.\n",
    "stability_api = client.StabilityInference(\n",
    "    key=os.environ['STABILITY_KEY'], # API Key reference.\n",
    "    verbose=True, # Print debug messages.\n",
    "    engine=\"stable-diffusion-v1-5\", # Set the engine to use for generation.\n",
    "    # Available engines: stable-diffusion-v1 stable-diffusion-v1-5 stable-diffusion-512-v2-0 stable-diffusion-768-v2-0\n",
    "    # stable-diffusion-512-v2-1 stable-diffusion-768-v2-1 stable-diffusion-xl-beta-v2-2-2 stable-inpainting-v1-0 stable-inpainting-512-v2-0\n",
    "    upscale_engine=\"stable-diffusion-x4-latent-upscaler\",\n",
    ")\n",
    "\n",
    "\n",
    "def generate_image(**kargs):\n",
    "    # Set up our initial generation parameters.\n",
    "    if not 'seed' in kargs:\n",
    "        kargs['seed'] = random.randrange(0, 4294967295)\n",
    "    success=False\n",
    "    while not success:\n",
    "        answers = stability_api.generate(\n",
    "            **kargs\n",
    "        )\n",
    "\n",
    "        # Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "        # If adult content classifier is not tripped, save generated images.\n",
    "        for resp in answers:\n",
    "            for artifact in resp.artifacts:\n",
    "                if artifact.finish_reason == generation.FILTER:\n",
    "                    warnings.warn(\n",
    "                        \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                        #\"Please modify the prompt and try again.\"\n",
    "                        \"Trying again.\"\n",
    "                    )\n",
    "                    kargs['seed']+=1\n",
    "                    break\n",
    "                    \n",
    "                if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "                    success=True\n",
    "                    img = Image.open(io.BytesIO(artifact.binary))\n",
    "                    #img.save(str(artifact.seed)+ \".png\") # Save our generated images with their seed number as the filename.\n",
    "                    return img, kargs['seed']\n",
    "\n",
    "            \n",
    "def upscale_image(**kargs):\n",
    "    # Set up our initial generation parameters.\n",
    "    if not 'seed' in kargs:\n",
    "        kargs['seed'] = random.randrange(0, 4294967295)\n",
    "    success=False\n",
    "    while not success:\n",
    "        answers = stability_api.upscale(\n",
    "            **kargs\n",
    "        )\n",
    "        # Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "        # If adult content classifier is not tripped, save generated images.\n",
    "        for resp in answers:\n",
    "            for artifact in resp.artifacts:\n",
    "                if artifact.finish_reason == generation.FILTER:\n",
    "                    warnings.warn(\n",
    "                        \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                        #\"Please modify the prompt and try again.\"\n",
    "                        \"trying again...\"\n",
    "                    )\n",
    "                    kargs['seed']+=1\n",
    "                    break\n",
    "\n",
    "                if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "                    success=True\n",
    "                    img = Image.open(io.BytesIO(artifact.binary))\n",
    "                    #img.save(str(artifact.seed)+ \".png\") # Save our generated images with their seed number as the filename.\n",
    "                    return img, kargs['seed']\n",
    "\n",
    "            \n",
    "def image_to_grid(img: Image.Image, k: int = 4, downsample: bool = True) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Convert an image to a grid by rearranging it into a KxK grid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img: PIL.Image.Image\n",
    "        The input image to convert to a grid.\n",
    "    k: int, optional (default=4)\n",
    "        The size of the grid (KxK).\n",
    "    downsample: bool, optional (default=True)\n",
    "        Whether to downsample the image before creating the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PIL.Image.Image\n",
    "        The image arranged in a KxK grid.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The input image can be downscaled before creating the grid.\n",
    "    - The resulting grid will have dimensions (k, k) times smaller than the original image.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> image_grid = image_to_grid(image, k=4, downsample=True)\n",
    "    \"\"\"\n",
    "\n",
    "    if downsample:\n",
    "        h, w = img.size\n",
    "        img = img.resize((h // k, w // k))\n",
    "\n",
    "    imlist = [np.array(img)] * (k ** 2)\n",
    "    gridded = multiplex(imlist, K=k)\n",
    "    image_grid = Image.fromarray(np.squeeze(gridded.astype(np.uint8)))\n",
    "\n",
    "    return image_grid\n",
    "\n",
    "\n",
    "\n",
    "def partition_grid(images: list, n_frozen: int = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Partition the grid of images into frozen and dynamic partitions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images: list\n",
    "        List of images in the grid.\n",
    "    n_frozen: int or None, optional (default=None)\n",
    "        Number of images to keep fixed as frozen. If None, defaults to the square root of the total number of images.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing the mask image representing the frozen partition, the list of images in the frozen partition,\n",
    "        and the list of images in the dynamic partition.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The frozen partition contains the fixed images that will not change during processing.\n",
    "    - The dynamic partition contains the images that will be subject to modifications or transformations.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> mask, frozen_partition, dynamic_partition = partition_grid(images)\n",
    "    \"\"\"\n",
    "    K = int(np.sqrt(len(images)))\n",
    "\n",
    "    if n_frozen is None:\n",
    "        n_frozen = K\n",
    "\n",
    "    frozen_partition = []\n",
    "    dynamic_partition = []\n",
    "    mask_images = []\n",
    "\n",
    "    for i, im in enumerate(images):\n",
    "        if i < n_frozen:\n",
    "            mask = np.ones_like(im) * 255\n",
    "            frozen_partition.append(im)\n",
    "        else:\n",
    "            mask = np.zeros_like(im)\n",
    "            dynamic_partition.append(im)\n",
    "        mask_images.append(mask)\n",
    "\n",
    "    mask_image = multiplex(mask_images, K)\n",
    "    mask_image = Image.fromarray(np.squeeze(mask_image))\n",
    "\n",
    "    return mask_image, frozen_partition, dynamic_partition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a71b3e-002f-4156-b2e0-cf8fab41ea68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#here's how everything ties together. I feel like there's still more opportunity to make this more readable, maintainable, or useful.\n",
    "\n",
    "\n",
    "def generate_init_grid(\n",
    "    seed,\n",
    "    diffusion_params\n",
    "):\n",
    "    # generate anchor image for init grid\n",
    "    img, seed = generate_image(\n",
    "        prompt=diffusion_params['prompt'],\n",
    "        steps=diffusion_params['steps'],\n",
    "        cfg_scale=diffusion_params['cfg_scale'],\n",
    "        width=diffusion_params['width'],\n",
    "        height=diffusion_params['height'],\n",
    "        sampler=diffusion_params['sampler'],\n",
    "        seed=seed,\n",
    "        samples=1,\n",
    "    )\n",
    "    # build init grid\n",
    "    image_grid = image_to_grid(img)\n",
    "    return image_grid, seed\n",
    "\n",
    "\n",
    "#########################################################\n",
    "\n",
    "def step_image_grid(\n",
    "    image_grid,\n",
    "    k,\n",
    "    diffusion_params,\n",
    "    seed=None,\n",
    "    prev_img=None,\n",
    "):\n",
    "\n",
    "    # apply diffusion step to image grid\n",
    "    image_grid2, seed = generate_image(\n",
    "        init_image = image_grid,\n",
    "        start_schedule=diffusion_params['start_schedule'],\n",
    "        ###################################\n",
    "        prompt=diffusion_params['prompt'],\n",
    "        steps=diffusion_params['steps'],\n",
    "        cfg_scale=diffusion_params['cfg_scale'],\n",
    "        width=diffusion_params['width'],\n",
    "        height=diffusion_params['height'],\n",
    "        sampler=diffusion_params['sampler'],\n",
    "        seed=seed,\n",
    "        samples=1,\n",
    "    )\n",
    "\n",
    "    # move this to only upscaling the specific images we care about\n",
    "    # # upscale generation\n",
    "    # image_grid3, seed = upscale_image(\n",
    "    #     init_image = image_grid2,\n",
    "    #     seed=seed,\n",
    "    #     #########################\n",
    "    #     prompt=diffusion_params['prompt'],\n",
    "    #     steps=diffusion_params['steps'],\n",
    "    #     cfg_scale=diffusion_params['cfg_scale'],\n",
    "    #     width=diffusion_params['width']*4,\n",
    "    # )\n",
    "\n",
    "\n",
    "    #imlist3 = demultiplex(np.array(image_grid3), K=k) # (16, 512, 512, 3)\n",
    "    imlist3 = demultiplex(np.array(image_grid2), K=k) # (16, 128, 128, 3)\n",
    "    \n",
    "    num_fixed=0\n",
    "    if prev_img is not None:\n",
    "        num_fixed=1\n",
    "        imlist3 = list(imlist3)\n",
    "        #imlist3 = [np.array(prev_img)[None, :]] + imlist3\n",
    "        imlist3 = [np.array(prev_img)] + imlist3\n",
    "        imlist3 = np.array(imlist3)\n",
    "\n",
    "    # sort grid images\n",
    "\n",
    "    im_vectors = rearrange(imlist3, \"b h w c -> b (h w c)\") # (16, 786432)\n",
    "    dmat = pdist(im_vectors, metric='cosine') # 120\n",
    "    dmat2 = squareform(dmat) # 16 16\n",
    "    distance_matrix = dmat2[:]\n",
    "    np.fill_diagonal(distance_matrix, 1)\n",
    "\n",
    "    \n",
    "    row_indices, col_indices = linear_sum_assignment(distance_matrix[num_fixed:, num_fixed:])\n",
    "    sorted_order = torch.cat([torch.arange(num_fixed), torch.as_tensor(col_indices + num_fixed)])\n",
    "    images = imlist3[sorted_order] # images.shape\n",
    "    images = [Image.fromarray(v) for v in images]  \n",
    "    \n",
    "    if prev_img is not None:\n",
    "        images = images[1:]\n",
    "\n",
    "    return images, seed\n",
    "    \n",
    "from itertools import cycle\n",
    "    \n",
    "def process_images(images, k, frame_idx, k_partition=None):\n",
    "    if k_partition is None:\n",
    "        k_partition=k\n",
    "    frozen_partition, dynamic_partition = images[:-k_partition], images[-k_partition:]\n",
    "\n",
    "    for img in frozen_partition:\n",
    "        img_path = f\"frame_{frame_idx:05}.png\"\n",
    "        print(img_path)\n",
    "        display(img)\n",
    "        \n",
    "        # upscale generation\n",
    "        img_big, _ = upscale_image(\n",
    "            init_image = img,\n",
    "            seed=seed,\n",
    "            #########################\n",
    "            prompt=diffusion_params['prompt'],\n",
    "            steps=diffusion_params['steps'],\n",
    "            cfg_scale=diffusion_params['cfg_scale'],\n",
    "            #width=diffusion_params['width']*4,\n",
    "            width=diffusion_params['width'],\n",
    "        )\n",
    "        display(img_big)\n",
    "        img_big.save(img_path)\n",
    "        frame_idx+=1\n",
    "    prev_img = img\n",
    "\n",
    "    next_grid_images = []\n",
    "    #for _ in range(k):\n",
    "    #    next_grid_images.extend(dynamic_partition)\n",
    "    gen = cycle(dynamic_partition)\n",
    "    for _ in range(k*k):\n",
    "        next_grid_images.append(next(gen))\n",
    "    imlist = [np.array(im) for im in next_grid_images]\n",
    "    next_grid = multiplex(imlist, K=k)\n",
    "    image_grid = Image.fromarray(np.squeeze(next_grid.astype(np.uint8)))\n",
    "\n",
    "    # I think we need to shrink the new grid, since it's now at upscaled resolution\n",
    "    print(image_grid.size)\n",
    "\n",
    "    image_grid = image_grid.resize((512,512))\n",
    "    return image_grid, frame_idx, prev_img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "diffusion_params = {\n",
    "    \"start_schedule\": 0.4,\n",
    "    \"prompt\": \"expansive landscape rolling greens with blue daisies and weeping willow trees under a blue alien sky, artstation, masterful, ghibli\",\n",
    "    \"steps\": 30,\n",
    "    \"cfg_scale\": 8.0,\n",
    "    \"width\": 512,\n",
    "    \"height\": 512,\n",
    "    \"samples\": 1,\n",
    "    \"sampler\": generation.SAMPLER_K_DPMPP_2M\n",
    "}\n",
    "\n",
    "seed=992446758\n",
    "frame_idx=0\n",
    "k=4\n",
    "\n",
    "image_grid, seed = generate_init_grid(seed, diffusion_params)\n",
    "images, seed = step_image_grid(image_grid, k, seed=seed, diffusion_params=diffusion_params)\n",
    "image_grid, frame_idx, prev_img = process_images(images, k, frame_idx)\n",
    "\n",
    "grid_iterations = 10\n",
    "for j in range(grid_iterations):\n",
    "    seed+=1\n",
    "    images, seed = step_image_grid(image_grid=image_grid, k=k, prev_img=prev_img, seed=seed, diffusion_params=diffusion_params)\n",
    "    image_grid, frame_idx, prev_img = process_images(images, k, frame_idx)\n",
    "\n",
    "#for im in images:\n",
    "#    display(im)\n",
    "\n",
    "### to do: rinse and repeat to build up an animation from frozen frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcddb90d-2ed7-447d-a681-78e720958ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = generate_image(prompt=\"a delicious cheeseburger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f3fea4-bdb7-44b5-aa3d-fe35f6ca2802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0 1\n",
      "2 1 6\n",
      "2 2 8\n"
     ]
    }
   ],
   "source": [
    "# Set up our initial generation parameters.\n",
    "kargs = {'prompt':'a delicious cheeseburger'}\n",
    "if not 'seed' in kargs:\n",
    "    kargs['seed'] = random.randrange(0, 4294967295)\n",
    "\n",
    "answers = stability_api.generate(\n",
    "    **kargs\n",
    ")\n",
    "\n",
    "# Set up our warning to print to the console if the adult content classifier is tripped.\n",
    "# If adult content classifier is not tripped, save generated images.\n",
    "responses=[]\n",
    "for i,resp in enumerate(answers):\n",
    "    outv = {}\n",
    "    for j,artifact in enumerate(resp.artifacts):\n",
    "        print(i,j,artifact.type)#, type(artifact.type))\n",
    "        if artifact.finish_reason == generation.FILTER:\n",
    "            warnings.warn(\n",
    "                \"Your request activated the API's safety filters and could not be processed.\"\n",
    "                #\"Please modify the prompt and try again.\"\n",
    "                \"Trying again.\"\n",
    "            )\n",
    "            kargs['seed']+=1\n",
    "            break\n",
    "\n",
    "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
    "            #success=True\n",
    "            img = Image.open(io.BytesIO(artifact.binary))\n",
    "            #img.save(str(artifact.seed)+ \".png\") # Save our generated images with their seed number as the filename.\n",
    "            #return img, kargs['seed']\n",
    "            #display(img)\n",
    "            outv['img'] = img\n",
    "        if artifact.type == generation.ARTIFACT_LATENT:\n",
    "            outv['latent_binary'] = artifact#.binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "051485be-1e04-43ef-8c15-8f0bd42025b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorizer\n",
      "  Using cached tensorizer-1.1.0-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: torch>=1.9.0 in /home/dmarx/.local/lib/python3.10/site-packages (from tensorizer) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/dmarx/.local/lib/python3.10/site-packages (from tensorizer) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.19.5 in /home/dmarx/.local/lib/python3.10/site-packages (from tensorizer) (4.21.12)\n",
      "Requirement already satisfied: psutil>=5.9.4 in /home/dmarx/.local/lib/python3.10/site-packages (from tensorizer) (5.9.5)\n",
      "Collecting boto3>=1.26.0 (from tensorizer)\n",
      "  Using cached boto3-1.26.133-py3-none-any.whl (135 kB)\n",
      "Collecting botocore<1.30.0,>=1.29.133 (from boto3>=1.26.0->tensorizer)\n",
      "  Using cached botocore-1.29.133-py3-none-any.whl (10.7 MB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.26.0->tensorizer)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3>=1.26.0->tensorizer)\n",
      "  Using cached s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: filelock in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (1.12)\n",
      "Requirement already satisfied: networkx in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/dmarx/.local/lib/python3.10/site-packages (from torch>=1.9.0->tensorizer) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9.0->tensorizer) (59.6.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9.0->tensorizer) (0.37.1)\n",
      "Requirement already satisfied: cmake in /home/dmarx/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.9.0->tensorizer) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/dmarx/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.9.0->tensorizer) (16.0.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/dmarx/.local/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.133->boto3>=1.26.0->tensorizer) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/lib/python3/dist-packages (from botocore<1.30.0,>=1.29.133->boto3>=1.26.0->tensorizer) (1.26.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dmarx/.local/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->tensorizer) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/dmarx/.local/lib/python3.10/site-packages (from sympy->torch>=1.9.0->tensorizer) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.133->boto3>=1.26.0->tensorizer) (1.16.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, tensorizer\n",
      "Successfully installed boto3-1.26.133 botocore-1.29.133 jmespath-1.0.1 s3transfer-0.6.1 tensorizer-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e4a563b-426e-479b-8d70-cf83153ac05b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorizer\n",
    "import tensorizer.protobuf as tensorizer_pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eaa55ed-1bcf-41c7-8121-cbe23e27fa78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://github.com/Stability-AI/generator_server/blob/989aac26d40c81fdaa433bcaeb5aab15c133ba85/artifacts.py#L172\n",
    "import torch\n",
    "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n",
    "#interfaces.src.tensorizer.protobuf as tensorizer_pb\n",
    "#import stability_sdk.interfaces.src.tensorizer.protobuf as tensorizer_pb\n",
    "import tensorizer.protobuf as tensorizer_pb\n",
    "\n",
    "def get_latent(artifact: generation.Artifact, \n",
    "               device=torch.device(\"cuda\"),\n",
    "               half_precision=True,\n",
    "    ) -> torch.Tensor:\n",
    "     # latent tensor (B, C, H, W), C=4(RGB), values in [-1, 1]\n",
    "    assert artifact.type == generation.ARTIFACT_LATENT, f\"Expected artifact type {generation.ARTIFACT_LATENT}, got {artifact.type}\"\n",
    "    t = tensorizer_pb.deserialize_tensor(artifact.tensor)\n",
    "    if len(t.shape) == 3:\n",
    "        t = t[None, ...] # add batch dimension\n",
    "    assert len(t.shape) == 4, f\"Expected 4D tensor for artifact type {artifact.type}, got {t.shape}\"\n",
    "    assert t.shape[1] == 4, f\"Expected 4 channels for artifact type {artifact.type}, got {t.shape[1]}\"\n",
    "    if half_precision:\n",
    "        t = t.half()\n",
    "    t = t.to(device)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2498ccf-608e-4943-8726-c10f7ffd8086",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 64, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looks like we should be able to send a latent as a prompt to the\n",
    "# normal generation endpoint \n",
    "# https://github.com/Stability-AI/generator_server/blob/main/resolver.py#L177\n",
    "latent_tensor = get_latent(outv['latent_binary'])\n",
    "latent_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47f8cacf-0945-4e4a-8532-6b8c612aa9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25be9274-d8e4-4db3-b79c-3df4179a3844",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Exif at 0x7fd814295c30>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exif = img.getexif()\n",
    "exif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c02257c-f76b-46e9-8d99-b194b75d74dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Exif'>\n",
      "ImageWidth:512\n",
      "ImageLength:512\n",
      "ImageDescription:a delicious cheeseburger\n",
      "Model:stability-diffusion\n",
      "Software:stability.ai\n",
      "ImageHistory:b'3kES7vpB4:eO5Ncw{5fez/ddIf!Cl}cobD)v(38Owg.yMwmvbEvk{7}vp?INeJtW$h-S/PgH&vNixw&h0u$O-rse<u@^+dU0$-lH3iH>l9TS2Y[wbek1q7P:0$)KCB-RNRB8>+zvq{fsy?WuiB97&1y?l%:x>y$1B7]MgflqezfkUSrB.>SgflzbFwO#jdwPxmz2.i3?Fcl+]HX]Jb3)kNa0TG8[3>-)R00000eDt!ekMy=:'\n"
     ]
    }
   ],
   "source": [
    "#list(exif.keys())\n",
    "\n",
    "#https://stackoverflow.com/a/56571871/819544\n",
    "\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "#img = Image.open(\"sample.jpg\")\n",
    "img_exif = img.getexif()\n",
    "print(type(img_exif))\n",
    "# <class 'PIL.Image.Exif'>\n",
    "\n",
    "if img_exif is None:\n",
    "    print('Sorry, image has no exif data.')\n",
    "else:\n",
    "    for key, val in img_exif.items():\n",
    "        if key in ExifTags.TAGS:\n",
    "            print(f'{ExifTags.TAGS[key]}:{val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02134a9f-2ec1-479b-9699-ed4eb8e8835f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zmq\n",
    "import zmq.utils.z85 as z85\n",
    "#dir(zmq.utils)\n",
    "#dir(zmq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a1b0136-4b35-48d4-9985-f77b75080702",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PIL.Image.Exif at 0x7fd814295c30>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stability_sdk\n",
    "generation = stability_sdk.api.generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d1171428-b7db-46f9-a594-cdc1b6fbe1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\x15stable-diffusion-v1-5\\x12&8b6c38cd-f8db-4a84-af90-87a79763c7c9:0\\x18\\x01\"W\\x1aU\\n\\x04\\x10\\xfe\\x81\\x03\\n\\x03\\x10\\xc0\\x02\\n\\x03\\x10\\xf7\\x1e\\n\\x04\\x10\\xf0\\xc1\\x02\\n\\x04\\x10\\xff\\x81\\x03\\x127/usr/src/app/lib/stable_diffusion/src/modules/tokenizer*!\\x08\\x80\\x04\\x10\\x80\\x04\\x1a\\x05\\x87\\xfb\\xd2\\xb4\\x0c \\x012\\x02\\x08\\t:\\x0c\\x12\\n\\r\\x00\\x00\\x00\\x00-\\x00\\x00\\xe0@\\x00\\x00\\x00'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exif = img.getexif()\n",
    "encoded_request = exif[TagsExif[\"ImageHistory\"]]\n",
    "padded_request = z85.decode(encoded_request)\n",
    "#request = generation.Request()\n",
    "#request.ParseFromString(padded_request)\n",
    "padded_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c91a85c4-258c-4813-aee3-375b12296a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#padded_request[:-1]\n",
    "#request.ParseFromString(padded_request[:-1])\n",
    "request.ParseFromString(padded_request[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5870a82-19e0-47b2-b2c6-44960778a5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "engine_id: \"stable-diffusion-v1-5\"\n",
       "request_id: \"8b6c38cd-f8db-4a84-af90-87a79763c7c9:0\"\n",
       "requested_type: ARTIFACT_IMAGE\n",
       "prompt {\n",
       "  tokens {\n",
       "    tokens {\n",
       "      id: 49406\n",
       "    }\n",
       "    tokens {\n",
       "      id: 320\n",
       "    }\n",
       "    tokens {\n",
       "      id: 3959\n",
       "    }\n",
       "    tokens {\n",
       "      id: 41200\n",
       "    }\n",
       "    tokens {\n",
       "      id: 49407\n",
       "    }\n",
       "    tokenizer_id: \"/usr/src/app/lib/stable_diffusion/src/modules/tokenizer\"\n",
       "  }\n",
       "}\n",
       "image {\n",
       "  height: 512\n",
       "  width: 512\n",
       "  seed: 3331636615\n",
       "  samples: 1\n",
       "  transform {\n",
       "    diffusion: SAMPLER_K_DPMPP_2M\n",
       "  }\n",
       "  parameters {\n",
       "    sampler {\n",
       "      eta: 0\n",
       "      cfg_scale: 7\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "137a49b6-f6f0-4fbc-98f3-43623db54d79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "DecodeError",
     "evalue": "Error parsing message",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDecodeError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 28\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# for prompt in request.prompt:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#     if prompt.HasField(\"tokens\"):\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#         prompt.text = decode_tokens_pb(prompt.tokens)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#         prompt.ClearField(\"tokens\")\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request\n\u001b[0;32m---> 28\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[43mload_exif_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 21\u001b[0m, in \u001b[0;36mload_exif_metadata\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     19\u001b[0m padded_request \u001b[38;5;241m=\u001b[39m z85\u001b[38;5;241m.\u001b[39mdecode(encoded_request)\n\u001b[1;32m     20\u001b[0m request \u001b[38;5;241m=\u001b[39m generation\u001b[38;5;241m.\u001b[39mRequest()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParseFromString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# for prompt in request.prompt:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     if prompt.HasField(\"tokens\"):\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#         prompt.text = decode_tokens_pb(prompt.tokens)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#         prompt.ClearField(\"tokens\")\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m request\n",
      "\u001b[0;31mDecodeError\u001b[0m: Error parsing message"
     ]
    }
   ],
   "source": [
    "# https://github.com/Stability-AI/generator_server/blob/main/metadata.py#L87-L105\n",
    "\n",
    "# lol even better, right below that\n",
    "\n",
    "from PIL import Image, ExifTags\n",
    "from PIL.Image import Exif\n",
    "#import torch\n",
    "#import pathlib\n",
    "\n",
    "TagsExif = {v: k for k, v in ExifTags.TAGS.items()}\n",
    "\n",
    "def load_exif_metadata(image: Image.Image) -> generation.Request:\n",
    "    exif = image.getexif()\n",
    "    if not exif:\n",
    "        return None\n",
    "    encoded_request = exif[TagsExif[\"ImageHistory\"]]\n",
    "    if not encoded_request:\n",
    "        return None\n",
    "    padded_request = z85.decode(encoded_request)\n",
    "    request = generation.Request()\n",
    "    request.ParseFromString(padded_request)\n",
    "    # for prompt in request.prompt:\n",
    "    #     if prompt.HasField(\"tokens\"):\n",
    "    #         prompt.text = decode_tokens_pb(prompt.tokens)\n",
    "    #         prompt.ClearField(\"tokens\")\n",
    "    return request\n",
    "\n",
    "meta = load_exif_metadata(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d565b-c93f-4f5a-97dc-2128b38507ea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47784114-49a1-44b3-beb5-130d2d508162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from stability_sdk.utils import create_video_from_frames\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def create_video_from_frames(frames_path: str, mp4_path: str, fps: int=24, reverse: bool=False):\n",
    "    \"\"\"\n",
    "    Convert a series of image frames to a video file using ffmpeg.\n",
    "\n",
    "    :param frames_path: The path to the directory containing the image frames named frame_00000.png, frame_00001.png, etc.\n",
    "    :param mp4_path: The path to save the output video file.\n",
    "    :param fps: The frames per second for the output video. Default is 24.\n",
    "    :param reverse: A flag to reverse the order of the frames in the output video. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    cmd = [\n",
    "        'ffmpeg',\n",
    "        '-y',\n",
    "        '-vcodec', 'png',\n",
    "        '-r', str(fps),\n",
    "        '-start_number', str(0),\n",
    "        #'-i', os.path.join(frames_path, \"frame_%05d.png\"),\n",
    "        '-i', os.path.join(frames_path, \"frame_%04d.png\"),\n",
    "        '-c:v', 'libx264',\n",
    "        '-vf',\n",
    "        f'fps={fps}',\n",
    "        '-pix_fmt', 'yuv420p',\n",
    "        '-crf', '17',\n",
    "        '-preset', 'veryslow',\n",
    "        mp4_path\n",
    "    ]\n",
    "    if reverse:\n",
    "        cmd.insert(-1, '-vf')\n",
    "        cmd.insert(-1, 'reverse')    \n",
    "\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    _, stderr = process.communicate()\n",
    "    if process.returncode != 0:\n",
    "        raise RuntimeError(stderr)\n",
    "\n",
    "create_video_from_frames('.', 'testout.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87168f51-7e2f-446e-9a63-28ff1b059093",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fca80e-6797-4033-8016-04fd57624fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up our connection to the API.\n",
    "stability_api = client.StabilityInference(\n",
    "    key=os.environ['STABILITY_KEY'], # API Key reference.\n",
    "    verbose=True, # Print debug messages.\n",
    "    engine=\"stable-diffusion-v1-5\", # Set the engine to use for generation.\n",
    "    # Available engines: stable-diffusion-v1 stable-diffusion-v1-5 stable-diffusion-512-v2-0 stable-diffusion-768-v2-0\n",
    "    # stable-diffusion-512-v2-1 stable-diffusion-768-v2-1 stable-diffusion-xl-beta-v2-2-2 stable-inpainting-v1-0 stable-inpainting-512-v2-0\n",
    "    upscale_engine=\"stable-diffusion-x4-latent-upscaler\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d15ff4-e770-4f1d-8716-716f93ddef49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = generate_image(\n",
    "    prompt=\"expansive landscape rolling greens with blue daisies and weeping willow trees under a blue alien sky, artstation, masterful, ghibli\",\n",
    "    seed=992446758, # If a seed is provided, the resulting generated image will be deterministic.\n",
    "                    # What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
    "                    # Note: This isn't quite the case for CLIP Guided generations, which we tackle in the CLIP Guidance documentation.\n",
    "    steps=30, # Amount of inference steps performed on image generation. Defaults to 30.\n",
    "    cfg_scale=8.0, # Influences how strongly your generation is guided to match your prompt.\n",
    "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
    "                   # Defaults to 7.0 if not specified.\n",
    "    width=512, # Generation width, defaults to 512 if not included.\n",
    "    height=512, # Generation height, defaults to 512 if not included.\n",
    "    samples=1, # Number of images to generate, defaults to 1 if not included.\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2M # Choose which sampler we want to denoise our generation with.\n",
    "                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n",
    "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m, k_dpmpp_sde)\n",
    ")\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3dedc-e90d-43de-932f-361a981d4b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_grid = image_to_grid(img)\n",
    "image_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876790c-87a5-4f28-abc7-f77f87b38a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed=12345678\n",
    "\n",
    "# apply diffusion step to image grid\n",
    "image_grid2 = generate_image(\n",
    "    init_image = image_grid,\n",
    "    start_schedule=0.2,\n",
    "    #########################\n",
    "    prompt=\"expansive landscape rolling greens with blue daisies and weeping willow trees under a blue alien sky, artstation, masterful, ghibli\",\n",
    "    steps=30,\n",
    "    cfg_scale=8.0,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    samples=1,\n",
    "    seed=seed,\n",
    "    sampler=generation.SAMPLER_K_DPMPP_2M\n",
    ")\n",
    "\n",
    "# upscale generation\n",
    "image_grid3 = upscale_image(\n",
    "    init_image = image_grid2,\n",
    "    #start_schedule=0.2,\n",
    "    #########################\n",
    "    prompt=\"expansive landscape rolling greens with blue daisies and weeping willow trees under a blue alien sky, artstation, masterful, ghibli\",\n",
    "    steps=30,\n",
    "    cfg_scale=8.0,\n",
    "    width=512*4,\n",
    "    #height=512,\n",
    "    #samples=1,\n",
    "    seed=seed,\n",
    "    #sampler=generation.SAMPLER_K_DPMPP_2M\n",
    ")\n",
    "\n",
    "image_grid3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f4713-246b-429c-ac3f-f8faecad2f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "imlist3 = demultiplex(np.array(image_grid3), K=4)\n",
    "\n",
    "#imlist3.shape # (16, 512, 512, 3)\n",
    "\n",
    "im_vectors = rearrange(imlist3, \"b h w c -> b (h w c)\")\n",
    "\n",
    "#im_vectors.shape # (16, 786432)\n",
    "\n",
    "dmat = pdist(im_vectors, metric='cosine')\n",
    "dmat.shape # 120\n",
    "dmat2 = squareform(dmat)\n",
    "dmat2.shape # 16 16\n",
    "\n",
    "distance_matrix = dmat2[:]\n",
    "np.fill_diagonal(distance_matrix, 1)\n",
    "\n",
    "\n",
    "num_fixed=0\n",
    "\n",
    "row_indices, col_indices = linear_sum_assignment(distance_matrix[num_fixed:, num_fixed:])\n",
    "\n",
    "# Add the fixed indices back to get the final sorted order\n",
    "sorted_order = torch.cat([torch.arange(num_fixed), torch.as_tensor(col_indices + num_fixed)])\n",
    "\n",
    "images = imlist3[sorted_order] # images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d5f19-60b1-48c7-ad5f-c18208462102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad35cd-c304-4e91-a047-99a81b35cc10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = [Image.fromarray(v) for v in images]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb260e3-605b-4d19-8c49-d683bd2eeea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "mask, frozen_partition, dynamic_partition = partition_grid(images)\n",
    "\n",
    "#mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84957674-66a4-4514-a4bf-ce61f8f3f37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for im in images:\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a636dbc5-424e-44eb-a65c-16455235e41e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76ebe8-627e-4b89-8bda-2acf40ffbb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: let's add transforms i guess? do it without transforms first. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c602815a-4806-470e-bf93-da65d5fb3db0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df224d-2252-4429-9d6f-44f503326e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chatgpt guessing at how to loop this.\n",
    "\n",
    "num_rounds = 5  # Number of rounds of diffusion and animation generation\n",
    "num_frozen_frames = 6  # Number of frozen frames per round\n",
    "\n",
    "frozen_frames = deepcopy(frozen_partition)  # Initialize the frozen frames with initial frozen partition\n",
    "\n",
    "for round in range(num_rounds):\n",
    "    print(f\"Generating Animation - Round {round + 1}\")\n",
    "\n",
    "    # Set the initial image grid as the current frozen frames\n",
    "    initial_image_grid = image_to_grid(frozen_frames, k=4, downsample=False)\n",
    "\n",
    "    # Apply diffusion step to the image grid\n",
    "    diffusion_params[\"init_image\"] = initial_image_grid\n",
    "    diffused_image_grid = generate_image(diffusion_params)\n",
    "\n",
    "    # Upscale the diffused image\n",
    "    upscale_params[\"init_image\"] = diffused_image_grid\n",
    "    upscaled_image_grid = upscale_image(upscale_params)\n",
    "\n",
    "    # Demultiplex the upscaled image grid\n",
    "    demultiplexed_images = demultiplex(np.array(upscaled_image_grid), K=4)\n",
    "\n",
    "    # Calculate pairwise cosine distances\n",
    "    image_vectors = rearrange(demultiplexed_images, \"b h w c -> b (h w c)\")\n",
    "    distance_matrix = pdist(image_vectors, metric='cosine')\n",
    "    distance_matrix = squareform(distance_matrix)\n",
    "    np.fill_diagonal(distance_matrix, 1)\n",
    "\n",
    "    # Perform TSP sort with the appropriate number of fixed frames\n",
    "    row_indices, col_indices = linear_sum_assignment(distance_matrix[num_frozen_frames:, num_frozen_frames:])\n",
    "    sorted_order = torch.cat([torch.arange(num_frozen_frames), torch.as_tensor(col_indices + num_frozen_frames)])\n",
    "    sorted_images = demultiplexed_images[sorted_order]\n",
    "\n",
    "    # Update the frozen frames with the desired number of frames\n",
    "    frozen_frames = sorted_images[:num_frozen_frames]\n",
    "\n",
    "    # Display or save the generated frames as needed\n",
    "    for frame in sorted_images:\n",
    "        display(Image.fromarray(frame))\n",
    "\n",
    "    print(\"Round Complete\\n\")\n",
    "\n",
    "print(\"Animation Generation Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d439e0fb-38ca-4272-a353-4e5de590d4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's try that again\n",
    "\n",
    "# def image_to_grid(images: list, k: int = 4, downsample: bool = False) -> Image.Image:\n",
    "#     \"\"\"\n",
    "#     Convert a list of images to a grid by rearranging them into a KxK grid.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     images: list\n",
    "#         The input list of images to convert to a grid.\n",
    "#     k: int, optional (default=4)\n",
    "#         The size of the grid (KxK).\n",
    "#     downsample: bool, optional (default=False)\n",
    "#         Whether to downsample the images before creating the grid.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     PIL.Image.Image\n",
    "#         The images arranged in a KxK grid.\n",
    "\n",
    "#     Notes\n",
    "#     -----\n",
    "#     - The input images can be downsampled before creating the grid.\n",
    "#     - The resulting grid will have dimensions (k, k) times smaller than the original images.\n",
    "#     \"\"\"\n",
    "#     if downsample:\n",
    "#         resized_images = [Image.fromarray(image).resize((image.shape[0] // k, image.shape[1] // k)) for image in images]\n",
    "#     else:\n",
    "#         resized_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "#     gridded = multiplex(resized_images, K=k)\n",
    "#     image_grid = Image.fromarray(np.squeeze(gridded.astype(np.uint8)))\n",
    "\n",
    "#     return image_grid\n",
    "\n",
    "def image_to_grid(images: list, k: int = 4, downsample: bool = False) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Convert a list of images to a grid by rearranging them into a KxK grid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images: list\n",
    "        The input list of images to convert to a grid.\n",
    "    k: int, optional (default=4)\n",
    "        The size of the grid (KxK).\n",
    "    downsample: bool, optional (default=False)\n",
    "        Whether to downsample the images before creating the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PIL.Image.Image\n",
    "        The images arranged in a KxK grid.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The input images can be downsampled before creating the grid.\n",
    "    - The resulting grid will have dimensions (k, k) times smaller than the original images.\n",
    "    \"\"\"\n",
    "    if downsample:\n",
    "        resized_images = [np.array(image.resize((image.size[0] // k, image.size[1] // k))) for image in images]\n",
    "    else:\n",
    "        resized_images = [np.array(image) for image in images]\n",
    "\n",
    "    gridded = multiplex(resized_images, K=k)\n",
    "    image_grid = Image.fromarray(np.squeeze(gridded.astype(np.uint8)))\n",
    "\n",
    "    return image_grid\n",
    "\n",
    "\n",
    "# Set up our connection to the API.\n",
    "stability_api = client.StabilityInference(\n",
    "    key=os.environ['STABILITY_KEY'],\n",
    "    verbose=True,\n",
    "    engine=\"stable-diffusion-v1-5\",\n",
    "    upscale_engine=\"stable-diffusion-x4-latent-upscaler\"\n",
    ")\n",
    "\n",
    "# Generate initial image\n",
    "initial_img_params = {\n",
    "    \"prompt\": \"expansive landscape rolling greens with blue daisies and weeping willow trees under a blue alien sky, artstation, masterful, ghibli\",\n",
    "    \"seed\": 992446758,\n",
    "    \"steps\": 30,\n",
    "    \"cfg_scale\": 8.0,\n",
    "    \"width\": 512,\n",
    "    \"height\": 512,\n",
    "    \"samples\": 1,\n",
    "    \"sampler\": generation.SAMPLER_K_DPMPP_2M\n",
    "}\n",
    "\n",
    "initial_image = generate_image(**initial_img_params)\n",
    "\n",
    "# Convert initial image to grid\n",
    "initial_image_grid = image_to_grid([initial_image], k=4, downsample=False)\n",
    "\n",
    "seed = 12345678\n",
    "\n",
    "# Apply diffusion step to the image grid\n",
    "diffusion_params = {\n",
    "    \"init_image\": initial_image_grid,\n",
    "    \"start_schedule\": 0.2,\n",
    "    \"prompt\": \"expansive landscape rolling greens with blue daisies and weeping willow trees under a blue alien sky, artstation, masterful, ghibli\",\n",
    "    \"steps\": 30,\n",
    "    \"cfg_scale\": 8.0,\n",
    "    \"width\": 512,\n",
    "    \"height\": 512,\n",
    "    \"samples\": 1,\n",
    "    \"seed\": seed,\n",
    "    \"sampler\": generation.SAMPLER_K_DPMPP_2M\n",
    "}\n",
    "\n",
    "diffused_image_grid = generate_image(diffusion_params)\n",
    "\n",
    "# Upscale the diffused image\n",
    "# Upscale the diffused image\n",
    "upscale_params = {\n",
    "    \"init_image\": diffused_image_grid,\n",
    "    \"prompt\": \"expansive landscape rolling greens with blue daisies and weeping willow trees under a blue alien sky, artstation, masterful, ghibli\",\n",
    "    \"steps\": 30,\n",
    "    \"cfg_scale\": 8.0,\n",
    "    \"width\": 512 * 4,\n",
    "    \"seed\": seed,\n",
    "}\n",
    "\n",
    "upscaled_image_grid = upscale_image(upscale_params)\n",
    "\n",
    "# Demultiplex the upscaled image grid\n",
    "demultiplexed_images = demultiplex(np.array(upscaled_image_grid), K=4)\n",
    "\n",
    "# Calculate pairwise cosine distances\n",
    "image_vectors = rearrange(demultiplexed_images, \"b h w c -> b (h w c)\")\n",
    "distance_matrix = pdist(image_vectors, metric='cosine')\n",
    "distance_matrix = squareform(distance_matrix)\n",
    "np.fill_diagonal(distance_matrix, 1)\n",
    "\n",
    "num_fixed = 0\n",
    "\n",
    "for round in range(num_rounds):\n",
    "    print(f\"Generating Animation - Round {round + 1}\")\n",
    "\n",
    "    # Set the initial image grid as the current frozen frames\n",
    "    initial_image_grid = image_to_grid(frozen_frames, k=4, downsample=False)\n",
    "\n",
    "    # Apply diffusion step to the image grid\n",
    "    diffusion_params[\"init_image\"] = initial_image_grid\n",
    "    diffused_image_grid = generate_image(diffusion_params)\n",
    "\n",
    "    # Upscale the diffused image\n",
    "    upscale_params[\"init_image\"] = diffused_image_grid\n",
    "    upscaled_image_grid = upscale_image(upscale_params)\n",
    "\n",
    "    # Demultiplex the upscaled image grid\n",
    "    demultiplexed_images = demultiplex(np.array(upscaled_image_grid), K=4)\n",
    "\n",
    "    # Calculate pairwise cosine distances\n",
    "    image_vectors = rearrange(demultiplexed_images, \"b h w c -> b (h w c)\")\n",
    "    distance_matrix = pdist(image_vectors, metric='cosine')\n",
    "    distance_matrix = squareform(distance_matrix)\n",
    "    np.fill_diagonal(distance_matrix, 1)\n",
    "\n",
    "    # Perform TSP sort with the appropriate number of fixed frames\n",
    "    row_indices, col_indices = linear_sum_assignment(distance_matrix[num_fixed:, num_fixed:])\n",
    "    sorted_order = torch.cat([torch.arange(num_fixed), torch.as_tensor(col_indices + num_fixed)])\n",
    "    sorted_images = demultiplexed_images[sorted_order]\n",
    "\n",
    "    # Update the frozen frames with the desired number of frames\n",
    "    frozen_frames = sorted_images[:num_frozen_frames]\n",
    "\n",
    "    # Display or save the generated frames as needed\n",
    "    for frame in sorted_images:\n",
    "        display(Image.fromarray(frame))\n",
    "\n",
    "    print(\"Round Complete\\n\")\n",
    "\n",
    "print(\"Animation Generation Complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ecb832-f7cd-456a-9f1e-ac47513f0547",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def image_to_grid(images: list, k: int = 4, downsample: bool = False) -> Image.Image:\n",
    "#     \"\"\"\n",
    "#     Convert a list of images to a grid by rearranging them into a KxK grid.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     images: list\n",
    "#         The input list of images to convert to a grid.\n",
    "#     k: int, optional (default=4)\n",
    "#         The size of the grid (KxK).\n",
    "#     downsample: bool, optional (default=False)\n",
    "#         Whether to downsample the images before creating the grid.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     PIL.Image.Image\n",
    "#         The images arranged in a KxK grid.\n",
    "\n",
    "#     Notes\n",
    "#     -----\n",
    "#     - The input images can be downsampled before creating the grid.\n",
    "#     - The resulting grid will have dimensions (k, k) times smaller than the original images.\n",
    "#     \"\"\"\n",
    "#     if downsample:\n",
    "#         resized_images = [np.array(image.resize((image.size[0] // k, image.size[1] // k))) for image in images]\n",
    "#     else:\n",
    "#         resized_images = [np.array(image) for image in images]\n",
    "\n",
    "#     gridded = multiplex(resized_images, K=k)\n",
    "#     image_grid = Image.fromarray(np.squeeze(gridded.astype(np.uint8)))\n",
    "\n",
    "#     return image_grid\n",
    "\n",
    "def image_to_grid(images: list, k: int = 4, downsample: bool = False) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Convert a list of images to a grid by rearranging them into a KxK grid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images: list\n",
    "        The input list of images to convert to a grid.\n",
    "    k: int, optional (default=4)\n",
    "        The size of the grid (KxK).\n",
    "    downsample: bool, optional (default=False)\n",
    "        Whether to downsample the images before creating the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PIL.Image.Image\n",
    "        The images arranged in a KxK grid.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The input images can be downsampled before creating the grid.\n",
    "    - The resulting grid will have dimensions (k, k) times smaller than the original images.\n",
    "    \"\"\"\n",
    "    if downsample:\n",
    "        resized_images = [np.array(image.resize((image.size[0] // k, image.size[1] // k))) for image in images]\n",
    "    else:\n",
    "        resized_images = [np.array(image) for image in images]\n",
    "\n",
    "    if len(resized_images) == 1:\n",
    "        return Image.fromarray(resized_images[0])\n",
    "\n",
    "    gridded = multiplex(resized_images, K=k)\n",
    "    image_grid = Image.fromarray(np.squeeze(gridded.astype(np.uint8)))\n",
    "\n",
    "    return image_grid\n",
    "\n",
    "\n",
    "# Set up our connection to the API.\n",
    "stability_api = client.StabilityInference(\n",
    "    key=os.environ['STABILITY_KEY'],\n",
    "    verbose=True,\n",
    "    engine=\"stable-diffusion-v1-5\",\n",
    "    upscale_engine=\"stable-diffusion-x4-latent-upscaler\"\n",
    ")\n",
    "\n",
    "# Generate initial image\n",
    "initial_img_params = {\n",
    "    \"prompt\": \"expansive landscape rolling greens with blue daisies and weeping willow trees under a blue alien sky, artstation, masterful, ghibli\",\n",
    "    \"seed\": 992446758,\n",
    "    \"steps\": 30,\n",
    "    \"cfg_scale\": 8.0,\n",
    "    \"width\": 512,\n",
    "    \"height\": 512,\n",
    "    \"samples\": 1,\n",
    "    \"sampler\": generation.SAMPLER_K_DPMPP_2M\n",
    "}\n",
    "\n",
    "initial_image = generate_image(**initial_img_params)\n",
    "\n",
    "# Convert initial image to grid\n",
    "initial_image_grid = image_to_grid([initial_image], k=4, downsample=False)\n",
    "\n",
    "seed = 12345678\n",
    "\n",
    "# Apply diffusion step to the image grid\n",
    "diffusion_params = {\n",
    "    \"init_image\": initial_image_grid,\n",
    "    \"start_schedule\": 0.2,\n",
    "    \"prompt\": \"expansive landscape rolling greens with blue daisies and weeping willow trees under a blue alien sky, artstation, masterful, ghibli\",\n",
    "    \"steps\": 30,\n",
    "    \"cfg_scale\": 8.0,\n",
    "    \"width\": 512,\n",
    "    \"height\": 512,\n",
    "    \"samples\": 1,\n",
    "    \"seed\": seed,\n",
    "    \"sampler\": generation.SAMPLER_K_DPMPP_2M\n",
    "}\n",
    "\n",
    "diffused_image_grid = generate_image(**diffusion_params)\n",
    "\n",
    "# Upscale the diffused image\n",
    "upscale_params = {\n",
    "    \"init_image\": diffused_image_grid,\n",
    "    \"prompt\": \"expansive landscape rolling greens with blue daisies and weeping willow trees under a blue alien sky, artstation, masterful, ghibli\",\n",
    "    \"steps\": 30,\n",
    "    \"cfg_scale\": 8.0,\n",
    "    \"width\": 512 * 4,\n",
    "    \"seed\": seed,\n",
    "}\n",
    "\n",
    "upscaled_image_grid = upscale_image(**upscale_params)\n",
    "\n",
    "# Demultiplex the upscaled image grid\n",
    "demultiplexed_images = demultiplex(np.array(upscaled_image_grid), K=4)\n",
    "\n",
    "# Calculate pairwise cosine distances\n",
    "image_vectors = rearrange(demultiplexed_images, \"b h w c -> b (h w c)\")\n",
    "distance_matrix = pdist(image_vectors, metric='cosine')\n",
    "distance_matrix = squareform(distance_matrix)\n",
    "np.fill_diagonal(distance_matrix, 1)\n",
    "\n",
    "num_fixed = 0\n",
    "\n",
    "all_frames = []\n",
    "\n",
    "for round in range(num_rounds):\n",
    "    print(f\"Generating Animation - Round {round + 1}\")\n",
    "\n",
    "    # Set the initial image grid as the current frozen frames\n",
    "    #initial_image_grid = image_to_grid(frozen_frames, k=4, downsample=False)\n",
    "    im_seq = []\n",
    "    for _ in range(4): # range(len(frozen_frames)): ?\n",
    "        im_seq.extend(frozen_frames)\n",
    "    im_seq = [Image.fromarray(f) for f in im_seq]\n",
    "    #initial_image_grid = image_to_grid(im_seq, k=4, downsample=False)\n",
    "    #initial_image_grid = image_to_grid(im_seq, k=4, downsample=(round>0))\n",
    "    initial_image_grid = image_to_grid(im_seq, k=4, downsample=True)\n",
    "    print(initial_image_grid.size)\n",
    "\n",
    "    # Apply diffusion step to the image grid\n",
    "    diffusion_params[\"init_image\"] = initial_image_grid\n",
    "    diffused_image_grid = generate_image(**diffusion_params)\n",
    "\n",
    "    # Upscale the diffused image\n",
    "    upscale_params[\"init_image\"] = diffused_image_grid\n",
    "    upscaled_image_grid = upscale_image(**upscale_params)\n",
    "\n",
    "    # Demultiplex the upscaled image grid\n",
    "    demultiplexed_images = demultiplex(np.array(upscaled_image_grid), K=4)\n",
    "\n",
    "    # Calculate pairwise cosine distances\n",
    "    image_vectors = rearrange(demultiplexed_images, \"b h w c -> b (h w c)\")\n",
    "    distance_matrix = pdist(image_vectors, metric='cosine')\n",
    "    distance_matrix = squareform(distance_matrix)\n",
    "    np.fill_diagonal(distance_matrix, 1)\n",
    "\n",
    "    # Perform TSP sort with the appropriate number of fixed frames\n",
    "    row_indices, col_indices = linear_sum_assignment(distance_matrix[num_fixed:, num_fixed:])\n",
    "    sorted_order = torch.cat([torch.arange(num_fixed), torch.as_tensor(col_indices + num_fixed)])\n",
    "    sorted_images = demultiplexed_images[sorted_order]\n",
    "\n",
    "    # Update the frozen frames with the desired number of frames\n",
    "    frozen_frames = sorted_images[:num_frozen_frames]\n",
    "\n",
    "    # Display or save the generated frames as needed\n",
    "    #for frame in sorted_images:\n",
    "    for frame in frozen_frames:\n",
    "        frame = Image.fromarray(frame)\n",
    "        display(frame)\n",
    "        all_frames.append(frame)\n",
    "\n",
    "    print(\"Round Complete\\n\")\n",
    "\n",
    "print(\"Animation Generation Complete\")\n",
    "# ```\n",
    "\n",
    "# In this updated code, the `image_to_grid()` function has been modified to handle resizing and grid creation outside the loop, ensuring consistent image shapes for each iteration. Additionally, the code for generating animation frames has been provided, including the freezing of frames, diffusion steps, and TSP-based sorting of images.\n",
    "\n",
    "# Please note that the code assumes the availability of the required functions and libraries, and it may need further adjustments based on the specifics of your environment and requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fd475-242b-41d3-8243-9410faf63dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im_seq[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
